{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-flock/","text":"Submit Node Flocking to OSG \u00b6 This page has moved to https://opensciencegrid.org/docs/submit/osg-flock/","title":"Osg flock"},{"location":"hpc_administration/osg_for_hpc_administrators/osg-flock/#submit-node-flocking-to-osg","text":"This page has moved to https://opensciencegrid.org/docs/submit/osg-flock/","title":"Submit Node Flocking to OSG"},{"location":"managing_htc_workloads_on_osg_connect/automated_workflows/dagman-workflows/","text":"Submit Workflows with HTCondor's DAGMan \u00b6 Overview \u00b6 If your work requires jobs that run in a particular sequence, you may benefit from a workflow tool that submits and monitors jobs for you in the correct order. A simple workflow manager that integrates with HTCondor is DAGMan, or \"DAG Manager\" where DAG stands for the typical picture of a workflow, a directed acyclic graph. Learning Resources \u00b6 This talk (originally presented at HTCondor Week 2020) gives a good overview of when to use DAGMan and its most useful features: For full details on various DAGMan features, see the HTCondor manual page: DAGMan Manual Page","title":"Submit Workflows with HTCondor's DAGMan "},{"location":"managing_htc_workloads_on_osg_connect/automated_workflows/dagman-workflows/#submit-workflows-with-htcondors-dagman","text":"","title":"Submit Workflows with HTCondor's DAGMan"},{"location":"managing_htc_workloads_on_osg_connect/automated_workflows/dagman-workflows/#overview","text":"If your work requires jobs that run in a particular sequence, you may benefit from a workflow tool that submits and monitors jobs for you in the correct order. A simple workflow manager that integrates with HTCondor is DAGMan, or \"DAG Manager\" where DAG stands for the typical picture of a workflow, a directed acyclic graph.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/automated_workflows/dagman-workflows/#learning-resources","text":"This talk (originally presented at HTCondor Week 2020) gives a good overview of when to use DAGMan and its most useful features: For full details on various DAGMan features, see the HTCondor manual page: DAGMan Manual Page","title":"Learning Resources"},{"location":"managing_htc_workloads_on_osg_connect/automated_workflows/tutorial-pegasus/","text":"Introduction wordfreq workflow Getting Help Introduction \u00b6 The Pegasus project encompasses a set of technologies that help workflow-based applications execute in a number of different environments including desktops, campus clusters, grids, and clouds. Pegasus bridges the scientific domain and the execution environment by automatically mapping high-level workflow descriptions onto distributed resources. It automatically locates the necessary input data and computational resources necessary for workflow execution. Pegasus enables scientists to construct workflows in abstract terms without worrying about the details of the underlying execution environment or the particulars of the low-level specifications required by the middleware. Some of the advantages of using Pegasus include: Portability / Reuse - User created workflows can easily be run in different environments without alteration. Pegasus currently runs workflows on compute systems scheduled via HTCondor, including the OSPool, and other other systems or via other schedulers (e.g. XSEDE resources, Amazon EC2, Google Cloud, and many campus clusters). The same workflow can run on a single system or across a heterogeneous set of resources. Performance - The Pegasus mapper can reorder, group, and prioritize tasks in order to increase the overall workflow performance. Scalability - Pegasus can easily scale both the size of the workflow, and the resources that the workflow is distributed over. Pegasus runs workflows ranging from just a few computational tasks up to 1 million tasks. The number of resources involved in executing a workflow can scale as needed without any impediments to performance. Provenance - By default, all jobs in Pegasus are launched via the kickstart process that captures runtime provenance of the job and helps in debugging. The provenance data is collected in a database, and the data can be summarized with tools such as pegasus-statistics or directly with SQL queries. Data Management - Pegasus handles replica selection, data transfers and output registrations in data catalogs. These tasks are added to a workflow as auxiliary jobs by the Pegasus planner. Reliability - Jobs and data transfers are automatically retried in case of failures. Debugging tools such as pegasus-analyzer help the user to debug the workflow in case of non-recoverable failures. Error Recovery - When errors occur, Pegasus tries to recover when possible by retrying tasks, retrying the entire workflow, providing workflow-level checkpointing, re-mapping portions of the workflow, trying alternative data sources for staging data, and, when all else fails, providing a rescue workflow containing a description of only the work that remains to be done. Pegasus keeps track of what has been done (provenance) including the locations of data used and produced, and which software was used with which parameters. As mentioned earlier in this book, OSG has no read/write enabled shared file system across the resources. Jobs are required to either bring inputs along with the job, or as part of the job stage the inputs from a remote location. The following examples highlight how Pegasus can be used to manage workloads in such an environment by providing an abstraction layer around things like data movements and job retries, enabling the users to run larger workloads, spending less time developing job management tools and babysitting their computations. Pegasus workflows have 4 components: Site Catalog - Describes the execution environment in which the workflow will be executed. Transformation Catalog - Specifies locations of the executables used by the workflow. Replica Catalog - Specifies locations of the input data to the workflow. Workflow Description - An abstract workflow description containing compute steps and dependencies between the steps. We refer to this workflow as abstract because it does not contain data locations and available software. When developing a Pegasus Workflow using the Python API , all four components may be defined in the same file. For details, please refer to the Pegasus documentation . wordfreq workflow \u00b6 wordfreq is an example application and workflow that can be used to introduce Pegasus tools and concepts. The application is available on the OSG Connect login host. This example is using OSG StashCache for data transfers. Credentials are transparant to the end users, so all the workflow has to do is use the stashcp command to copy data to and from the OSG Connect Stash instance. Additionally, this example uses a custom container to run jobs. The container capability is provided by OSG ( Docker and Singularity Containers ) and is used by setting HTCondor properties when defining your workflow. Exercise 1 : create a copy of the Pegasus tutorial and change the working directory to the wordfreq workflow by running the following commands: $ tutorial pegasus $ cd tutorial-pegasus/wordfreq In the wordfreq directory, you will find: wordfreq/ \u251c\u2500\u2500 bin | \u251c\u2500\u2500 summarize | \u2514\u2500\u2500 wordfreq \u251c\u2500\u2500 inputs | \u251c\u2500\u2500 Alices_Adventures_in_Wonderland_by_Lewis_Carroll.txt | \u251c\u2500\u2500 Dracula_by_Bram_Stoker.txt | \u251c\u2500\u2500 Pride_and_Prejudice_by_Jane_Austen.txt | \u251c\u2500\u2500 The_Adventures_of_Huckleberry_Finn_by_Mark_Twain.txt | \u251c\u2500\u2500 Ulysses_by_James_Joyce.txt | \u2514\u2500\u2500 Visual_Signaling_By_Signal_Corps_United_States_Army.txt \u251c\u2500\u2500 many-more-inputs | \u2514\u2500\u2500 ... \u2514\u2500\u2500 workflow.py The inputs/ directory contains 6 public domain ebooks. The wordreq workflow uses the two executables in the bin/ directory. bin/wordfreq takes a text file as input and produces a summary output file containting the counts and names of the top five most frequently used words from the input file. A wordfreq job is created for each file in inputs/ . bin/summarize concatenates the the output of each wordfreq job into a single output file called summary.txt . This workflow structure, which is a set of independent tasks joining into a single summary or analysis type of task, is a common use case on OSG and therefore this workflow can be thought of as a template for such problems. For example, instead of using wordfreq on ebooks, the application could be protein folding on a set of input structures. When invoked, the workflow script ( workflow.py ) does the following: Writes the file pegasus.properties . This file contains configuration settings used by Pegasus and HTCondor. # --- Properties --------------------------------------------------------------- props = Properties() props[\"pegasus.data.configuration\"] = \"nonsharedfs\" # Provide a full kickstart record, including the environment, even for successful jobs props[\"pegasus.gridstart.arguments\"] = \"-f\" #Limit the number of idle jobs for large workflows props[\"dagman.maxidle\"] = \"1000\" # Help Pegasus developers by sharing performance data (optional) props[\"pegasus.monitord.encoding\"] = \"json\" props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\" # write properties file to ./pegasus.properties props.write() Writes the file sites.yml . This file describes the execution environment in which the workflow will be run. # --- Sites -------------------------------------------------------------------- sc = SiteCatalog() # local site (submit machine) local_site = Site(name=\"local\", arch=Arch.X86_64) local_shared_scratch = Directory(directory_type=Directory.SHARED_SCRATCH, path=WORK_DIR / \"scratch\") local_shared_scratch.add_file_servers(FileServer(url=\"file://\" + str(WORK_DIR / \"scratch\"), operation_type=Operation.ALL)) local_site.add_directories(local_shared_scratch) local_storage = Directory(directory_type=Directory.LOCAL_STORAGE, path=WORK_DIR / \"outputs\") local_storage.add_file_servers(FileServer(url=\"file://\" + str(WORK_DIR / \"outputs\"), operation_type=Operation.ALL)) local_site.add_directories(local_storage) local_site.add_env(PATH=os.environ[\"PATH\"]) sc.add_sites(local_site) # stash site (staging site, where intermediate data will be stored) stash_site = Site(name=\"stash\", arch=Arch.X86_64, os_type=OS.LINUX) stash_staging_path = \"/public/{USER}/staging\".format(USER=getpass.getuser()) stash_shared_scratch = Directory(directory_type=Directory.SHARED_SCRATCH, path=stash_staging_path) stash_shared_scratch.add_file_servers( FileServer( url=\"stash:///osgconnect{STASH_STAGING_PATH}\".format(STASH_STAGING_PATH=stash_staging_path), operation_type=Operation.ALL) ) stash_site.add_directories(stash_shared_scratch) sc.add_sites(stash_site) # condorpool (execution site) condorpool_site = Site(name=\"condorpool\", arch=Arch.X86_64, os_type=OS.LINUX) condorpool_site.add_pegasus_profile(style=\"condor\") condorpool_site.add_condor_profile( universe=\"vanilla\", requirements=\"HAS_SINGULARITY == True\", request_cpus=1, request_memory=\"1 GB\", request_disk=\"1 GB\", ) condorpool_site.add_profiles( Namespace.CONDOR, key=\"+SingularityImage\", value='\"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest\"' ) sc.add_sites(condorpool_site) # write SiteCatalog to ./sites.yml sc.write() In order for the workflow to use the container capability provided by OSG, ( Docker and Singularity Containers ) the following HTCondor profiles must be added to the condorpool execution site: requirements=\"HAS_SINGULARITY == True\" , and +SingularityImage='\"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest\"' . The requirements expression indicates that the host on which the jobs run must have Singularity installed. +SingularityImage specifies the container to use. If you want to use stashcp, make sure it is accessible in the image. A symlink to /cvmfs/ from a standard location in the PATH is often enough for the tool to be found and used ( example Dockerfile ). Writes the file transformations.yml . This file specifies the executables used in the workflow and contains the locations where they are physically located. In this example, we have two entries: wordfreq and summarize . # --- Transformations ---------------------------------------------------------- wordfreq = Transformation( name=\"wordfreq\", site=\"local\", pfn=TOP_DIR / \"bin/wordfreq\", is_stageable=True, arch=Arch.X86_64 ).add_pegasus_profile(clusters_size=1) summarize = Transformation( name=\"summarize\", site=\"local\", pfn=TOP_DIR / \"bin/summarize\", is_stageable=True, arch=Arch.X86_64 ) tc = TransformationCatalog() tc.add_transformations(wordfreq, summarize) # write TransformationCatalog to ./transformations.yml tc.write() Writes the file replicas.yml . This file specifies the physical locations of any input files used by the workflow. In this example, there is an entry for each file in the inputs/ directory. # --- Replicas ----------------------------------------------------------------- input_files = [File(f.name) for f in (TOP_DIR / \"inputs\").iterdir() if f.name.endswith(\".txt\")] rc = ReplicaCatalog() for f in input_files: rc.add_replica(site=\"local\", lfn=f, pfn=TOP_DIR / \"inputs\" / f.lfn) # write ReplicaCatalog to replicas.yml rc.write() Builds the wordfreq workflow and submits it for execution. When wf.plan is invoked, pegasus.properties , sites.yml , transformations.yml , and replicas.yml will be consumed as part of the workflow planning process. Note that in this step there is no mention of data movement and job details as these are added by Pegasus when the workflow is planned into an executable workflow. As part of the planning process, additional jobs which handle scratch directory creation, data staging, and data cleanup are added to the workflow. # --- Workflow ----------------------------------------------------------------- wf = Workflow(name=\"wordfreq-workflow\") summarize_job = Job(summarize).add_outputs(File(\"summary.txt\")) wf.add_jobs(summarize_job) for f in input_files: out_file = File(f.lfn + \".out\") wordfreq_job = Job(wordfreq)\\ .add_args(f, out_file)\\ .add_inputs(f)\\ .add_outputs(out_file) wf.add_jobs(wordfreq_job) summarize_job.add_inputs(out_file) # plan and run the workflow wf.plan( dir=WORK_DIR / \"runs\", sites=[\"condorpool\"], staging_sites={\"condorpool\": \"stash\"}, output_sites=[\"local\"], cluster=[\"horizontal\"], submit=True ) Exercise 2: Submit the workflow by executing workflow.py . $ ./workflow.py Note that when Pegasus plans/submits a workflow, a workflow directory is created and presented in the output. In the following example output, the workflow directory is /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 . 2020.12.18 14:33:07.059 CST: ----------------------------------------------------------------------- 2020.12.18 14:33:07.064 CST: File for submitting this DAG to HTCondor : wordfreq-workflow-0.dag.condor.sub 2020.12.18 14:33:07.070 CST: Log of DAGMan debugging messages : wordfreq-workflow-0.dag.dagman.out 2020.12.18 14:33:07.075 CST: Log of HTCondor library output : wordfreq-workflow-0.dag.lib.out 2020.12.18 14:33:07.080 CST: Log of HTCondor library error messages : wordfreq-workflow-0.dag.lib.err 2020.12.18 14:33:07.086 CST: Log of the life of condor_dagman itself : wordfreq-workflow-0.dag.dagman.log 2020.12.18 14:33:07.091 CST: 2020.12.18 14:33:07.096 CST: -no_submit given, not submitting DAG to HTCondor. You can do this with: 2020.12.18 14:33:07.107 CST: ----------------------------------------------------------------------- 2020.12.18 14:33:10.381 CST: Your database is compatible with Pegasus version: 5.1.0dev 2020.12.18 14:33:11.347 CST: Created Pegasus database in: sqlite:////home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014/wordfreq-workflow-0.replicas.db 2020.12.18 14:33:11.352 CST: Your database is compatible with Pegasus version: 5.1.0dev 2020.12.18 14:33:11.404 CST: Output replica catalog set to jdbc:sqlite:/home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014/wordfreq-workflow-0.replicas.db [WARNING] Submitting to condor wordfreq-workflow-0.dag.condor.sub 2020.12.18 14:33:12.060 CST: Time taken to execute is 5.818 seconds Your workflow has been started and is running in the base directory: /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 *** To monitor the workflow you can run *** pegasus-status -l /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 *** To remove your workflow run *** pegasus-remove /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 This directory is the handle to the workflow instance and is used by Pegasus command line tools. Some useful tools to know about: pegasus-status -v [wfdir] Provides status on a currently running workflow. ( more ) pegasus-analyzer [wfdir] Provides debugging clues why a workflow failed. Run this after a workflow has failed. ( more ) pegasus-statistics [wfdir] Provides statistics, such as walltimes, on a workflow after it has completed. ( more ) pegasus-remove [wfdir] Removes a workflow from the system. ( more ) Exercise 3: Check the status of the workflow: $ pegasus-status [wfdir] You can keep checking the status periodically to see that the workflow is making progress. Exercise 4: Examine a submit file and the *.dag.dagman.out files. Do these look familiar to you from previous modules in the book? Pegasus is based on HTCondor and DAGMan. $ cd [wfdir] $ cat 00/00/summarize_ID0000001.sub ... $ cat *.dag.dagman.out ... Exercise 5: Keep checking progress with pegasus-status . Once the workflow is done, display statistics with pegasus-statistics : $ pegasus-status [wfdir] $ pegasus-statistics [wfdir] ... Exercise 6: cd to the output directory and look at the outputs. Which is the most common word used in the 6 books? Hint: $ cd $HOME/workflows/outputs $ head -n 5 *.out Exercise 7: Want to try something larger? Copy the additional 994 ebooks from \\ the many-more-inputs/ directory to the inputs/ directory: $ cp many-more-inputs/* inputs/ As these tasks are really short, let's tell Pegasus to cluster multiple tasks together into jobs. If you do not do this step, the jobs will still run, but not very efficiently. This is because every job has a small scheduling overhead. For short jobs, the overhead is obvious. If we make the jobs longer, the scheduling overhead becomes negligible. To enable the clustering feature, edit the workflow.py script. Find the section under Transformations : wordfreq = Transformation( name=\"wordfreq\", site=\"local\", pfn=TOP_DIR / \"bin/wordfreq\", is_stageable=True, arch=Arch.X86_64 ).add_pegasus_profile(clusters_size=1) Change clusters_size=1 to clusters_size=50 . This informs Pegasus that it is ok to cluster up to 50 of the jobs which use the wordfreq executable. Save the file and re-run the script: $ ./workflow.py Use pegasus-status and pegasus-statistics to monitor your workflow. Using pegasus-statistics , determine how many jobs ended up in your workflow and see how this compares with our initial workflow run. Getting Help \u00b6 For assistance or questions, please email the OSG User Support team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Use Pegasus to Manage Workflows on OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/automated_workflows/tutorial-pegasus/#introduction","text":"The Pegasus project encompasses a set of technologies that help workflow-based applications execute in a number of different environments including desktops, campus clusters, grids, and clouds. Pegasus bridges the scientific domain and the execution environment by automatically mapping high-level workflow descriptions onto distributed resources. It automatically locates the necessary input data and computational resources necessary for workflow execution. Pegasus enables scientists to construct workflows in abstract terms without worrying about the details of the underlying execution environment or the particulars of the low-level specifications required by the middleware. Some of the advantages of using Pegasus include: Portability / Reuse - User created workflows can easily be run in different environments without alteration. Pegasus currently runs workflows on compute systems scheduled via HTCondor, including the OSPool, and other other systems or via other schedulers (e.g. XSEDE resources, Amazon EC2, Google Cloud, and many campus clusters). The same workflow can run on a single system or across a heterogeneous set of resources. Performance - The Pegasus mapper can reorder, group, and prioritize tasks in order to increase the overall workflow performance. Scalability - Pegasus can easily scale both the size of the workflow, and the resources that the workflow is distributed over. Pegasus runs workflows ranging from just a few computational tasks up to 1 million tasks. The number of resources involved in executing a workflow can scale as needed without any impediments to performance. Provenance - By default, all jobs in Pegasus are launched via the kickstart process that captures runtime provenance of the job and helps in debugging. The provenance data is collected in a database, and the data can be summarized with tools such as pegasus-statistics or directly with SQL queries. Data Management - Pegasus handles replica selection, data transfers and output registrations in data catalogs. These tasks are added to a workflow as auxiliary jobs by the Pegasus planner. Reliability - Jobs and data transfers are automatically retried in case of failures. Debugging tools such as pegasus-analyzer help the user to debug the workflow in case of non-recoverable failures. Error Recovery - When errors occur, Pegasus tries to recover when possible by retrying tasks, retrying the entire workflow, providing workflow-level checkpointing, re-mapping portions of the workflow, trying alternative data sources for staging data, and, when all else fails, providing a rescue workflow containing a description of only the work that remains to be done. Pegasus keeps track of what has been done (provenance) including the locations of data used and produced, and which software was used with which parameters. As mentioned earlier in this book, OSG has no read/write enabled shared file system across the resources. Jobs are required to either bring inputs along with the job, or as part of the job stage the inputs from a remote location. The following examples highlight how Pegasus can be used to manage workloads in such an environment by providing an abstraction layer around things like data movements and job retries, enabling the users to run larger workloads, spending less time developing job management tools and babysitting their computations. Pegasus workflows have 4 components: Site Catalog - Describes the execution environment in which the workflow will be executed. Transformation Catalog - Specifies locations of the executables used by the workflow. Replica Catalog - Specifies locations of the input data to the workflow. Workflow Description - An abstract workflow description containing compute steps and dependencies between the steps. We refer to this workflow as abstract because it does not contain data locations and available software. When developing a Pegasus Workflow using the Python API , all four components may be defined in the same file. For details, please refer to the Pegasus documentation .","title":"Introduction"},{"location":"managing_htc_workloads_on_osg_connect/automated_workflows/tutorial-pegasus/#wordfreq-workflow","text":"wordfreq is an example application and workflow that can be used to introduce Pegasus tools and concepts. The application is available on the OSG Connect login host. This example is using OSG StashCache for data transfers. Credentials are transparant to the end users, so all the workflow has to do is use the stashcp command to copy data to and from the OSG Connect Stash instance. Additionally, this example uses a custom container to run jobs. The container capability is provided by OSG ( Docker and Singularity Containers ) and is used by setting HTCondor properties when defining your workflow. Exercise 1 : create a copy of the Pegasus tutorial and change the working directory to the wordfreq workflow by running the following commands: $ tutorial pegasus $ cd tutorial-pegasus/wordfreq In the wordfreq directory, you will find: wordfreq/ \u251c\u2500\u2500 bin | \u251c\u2500\u2500 summarize | \u2514\u2500\u2500 wordfreq \u251c\u2500\u2500 inputs | \u251c\u2500\u2500 Alices_Adventures_in_Wonderland_by_Lewis_Carroll.txt | \u251c\u2500\u2500 Dracula_by_Bram_Stoker.txt | \u251c\u2500\u2500 Pride_and_Prejudice_by_Jane_Austen.txt | \u251c\u2500\u2500 The_Adventures_of_Huckleberry_Finn_by_Mark_Twain.txt | \u251c\u2500\u2500 Ulysses_by_James_Joyce.txt | \u2514\u2500\u2500 Visual_Signaling_By_Signal_Corps_United_States_Army.txt \u251c\u2500\u2500 many-more-inputs | \u2514\u2500\u2500 ... \u2514\u2500\u2500 workflow.py The inputs/ directory contains 6 public domain ebooks. The wordreq workflow uses the two executables in the bin/ directory. bin/wordfreq takes a text file as input and produces a summary output file containting the counts and names of the top five most frequently used words from the input file. A wordfreq job is created for each file in inputs/ . bin/summarize concatenates the the output of each wordfreq job into a single output file called summary.txt . This workflow structure, which is a set of independent tasks joining into a single summary or analysis type of task, is a common use case on OSG and therefore this workflow can be thought of as a template for such problems. For example, instead of using wordfreq on ebooks, the application could be protein folding on a set of input structures. When invoked, the workflow script ( workflow.py ) does the following: Writes the file pegasus.properties . This file contains configuration settings used by Pegasus and HTCondor. # --- Properties --------------------------------------------------------------- props = Properties() props[\"pegasus.data.configuration\"] = \"nonsharedfs\" # Provide a full kickstart record, including the environment, even for successful jobs props[\"pegasus.gridstart.arguments\"] = \"-f\" #Limit the number of idle jobs for large workflows props[\"dagman.maxidle\"] = \"1000\" # Help Pegasus developers by sharing performance data (optional) props[\"pegasus.monitord.encoding\"] = \"json\" props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\" # write properties file to ./pegasus.properties props.write() Writes the file sites.yml . This file describes the execution environment in which the workflow will be run. # --- Sites -------------------------------------------------------------------- sc = SiteCatalog() # local site (submit machine) local_site = Site(name=\"local\", arch=Arch.X86_64) local_shared_scratch = Directory(directory_type=Directory.SHARED_SCRATCH, path=WORK_DIR / \"scratch\") local_shared_scratch.add_file_servers(FileServer(url=\"file://\" + str(WORK_DIR / \"scratch\"), operation_type=Operation.ALL)) local_site.add_directories(local_shared_scratch) local_storage = Directory(directory_type=Directory.LOCAL_STORAGE, path=WORK_DIR / \"outputs\") local_storage.add_file_servers(FileServer(url=\"file://\" + str(WORK_DIR / \"outputs\"), operation_type=Operation.ALL)) local_site.add_directories(local_storage) local_site.add_env(PATH=os.environ[\"PATH\"]) sc.add_sites(local_site) # stash site (staging site, where intermediate data will be stored) stash_site = Site(name=\"stash\", arch=Arch.X86_64, os_type=OS.LINUX) stash_staging_path = \"/public/{USER}/staging\".format(USER=getpass.getuser()) stash_shared_scratch = Directory(directory_type=Directory.SHARED_SCRATCH, path=stash_staging_path) stash_shared_scratch.add_file_servers( FileServer( url=\"stash:///osgconnect{STASH_STAGING_PATH}\".format(STASH_STAGING_PATH=stash_staging_path), operation_type=Operation.ALL) ) stash_site.add_directories(stash_shared_scratch) sc.add_sites(stash_site) # condorpool (execution site) condorpool_site = Site(name=\"condorpool\", arch=Arch.X86_64, os_type=OS.LINUX) condorpool_site.add_pegasus_profile(style=\"condor\") condorpool_site.add_condor_profile( universe=\"vanilla\", requirements=\"HAS_SINGULARITY == True\", request_cpus=1, request_memory=\"1 GB\", request_disk=\"1 GB\", ) condorpool_site.add_profiles( Namespace.CONDOR, key=\"+SingularityImage\", value='\"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest\"' ) sc.add_sites(condorpool_site) # write SiteCatalog to ./sites.yml sc.write() In order for the workflow to use the container capability provided by OSG, ( Docker and Singularity Containers ) the following HTCondor profiles must be added to the condorpool execution site: requirements=\"HAS_SINGULARITY == True\" , and +SingularityImage='\"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest\"' . The requirements expression indicates that the host on which the jobs run must have Singularity installed. +SingularityImage specifies the container to use. If you want to use stashcp, make sure it is accessible in the image. A symlink to /cvmfs/ from a standard location in the PATH is often enough for the tool to be found and used ( example Dockerfile ). Writes the file transformations.yml . This file specifies the executables used in the workflow and contains the locations where they are physically located. In this example, we have two entries: wordfreq and summarize . # --- Transformations ---------------------------------------------------------- wordfreq = Transformation( name=\"wordfreq\", site=\"local\", pfn=TOP_DIR / \"bin/wordfreq\", is_stageable=True, arch=Arch.X86_64 ).add_pegasus_profile(clusters_size=1) summarize = Transformation( name=\"summarize\", site=\"local\", pfn=TOP_DIR / \"bin/summarize\", is_stageable=True, arch=Arch.X86_64 ) tc = TransformationCatalog() tc.add_transformations(wordfreq, summarize) # write TransformationCatalog to ./transformations.yml tc.write() Writes the file replicas.yml . This file specifies the physical locations of any input files used by the workflow. In this example, there is an entry for each file in the inputs/ directory. # --- Replicas ----------------------------------------------------------------- input_files = [File(f.name) for f in (TOP_DIR / \"inputs\").iterdir() if f.name.endswith(\".txt\")] rc = ReplicaCatalog() for f in input_files: rc.add_replica(site=\"local\", lfn=f, pfn=TOP_DIR / \"inputs\" / f.lfn) # write ReplicaCatalog to replicas.yml rc.write() Builds the wordfreq workflow and submits it for execution. When wf.plan is invoked, pegasus.properties , sites.yml , transformations.yml , and replicas.yml will be consumed as part of the workflow planning process. Note that in this step there is no mention of data movement and job details as these are added by Pegasus when the workflow is planned into an executable workflow. As part of the planning process, additional jobs which handle scratch directory creation, data staging, and data cleanup are added to the workflow. # --- Workflow ----------------------------------------------------------------- wf = Workflow(name=\"wordfreq-workflow\") summarize_job = Job(summarize).add_outputs(File(\"summary.txt\")) wf.add_jobs(summarize_job) for f in input_files: out_file = File(f.lfn + \".out\") wordfreq_job = Job(wordfreq)\\ .add_args(f, out_file)\\ .add_inputs(f)\\ .add_outputs(out_file) wf.add_jobs(wordfreq_job) summarize_job.add_inputs(out_file) # plan and run the workflow wf.plan( dir=WORK_DIR / \"runs\", sites=[\"condorpool\"], staging_sites={\"condorpool\": \"stash\"}, output_sites=[\"local\"], cluster=[\"horizontal\"], submit=True ) Exercise 2: Submit the workflow by executing workflow.py . $ ./workflow.py Note that when Pegasus plans/submits a workflow, a workflow directory is created and presented in the output. In the following example output, the workflow directory is /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 . 2020.12.18 14:33:07.059 CST: ----------------------------------------------------------------------- 2020.12.18 14:33:07.064 CST: File for submitting this DAG to HTCondor : wordfreq-workflow-0.dag.condor.sub 2020.12.18 14:33:07.070 CST: Log of DAGMan debugging messages : wordfreq-workflow-0.dag.dagman.out 2020.12.18 14:33:07.075 CST: Log of HTCondor library output : wordfreq-workflow-0.dag.lib.out 2020.12.18 14:33:07.080 CST: Log of HTCondor library error messages : wordfreq-workflow-0.dag.lib.err 2020.12.18 14:33:07.086 CST: Log of the life of condor_dagman itself : wordfreq-workflow-0.dag.dagman.log 2020.12.18 14:33:07.091 CST: 2020.12.18 14:33:07.096 CST: -no_submit given, not submitting DAG to HTCondor. You can do this with: 2020.12.18 14:33:07.107 CST: ----------------------------------------------------------------------- 2020.12.18 14:33:10.381 CST: Your database is compatible with Pegasus version: 5.1.0dev 2020.12.18 14:33:11.347 CST: Created Pegasus database in: sqlite:////home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014/wordfreq-workflow-0.replicas.db 2020.12.18 14:33:11.352 CST: Your database is compatible with Pegasus version: 5.1.0dev 2020.12.18 14:33:11.404 CST: Output replica catalog set to jdbc:sqlite:/home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014/wordfreq-workflow-0.replicas.db [WARNING] Submitting to condor wordfreq-workflow-0.dag.condor.sub 2020.12.18 14:33:12.060 CST: Time taken to execute is 5.818 seconds Your workflow has been started and is running in the base directory: /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 *** To monitor the workflow you can run *** pegasus-status -l /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 *** To remove your workflow run *** pegasus-remove /home/ryantanaka/workflows/runs/ryantanaka/pegasus/wordfreq-workflow/run0014 This directory is the handle to the workflow instance and is used by Pegasus command line tools. Some useful tools to know about: pegasus-status -v [wfdir] Provides status on a currently running workflow. ( more ) pegasus-analyzer [wfdir] Provides debugging clues why a workflow failed. Run this after a workflow has failed. ( more ) pegasus-statistics [wfdir] Provides statistics, such as walltimes, on a workflow after it has completed. ( more ) pegasus-remove [wfdir] Removes a workflow from the system. ( more ) Exercise 3: Check the status of the workflow: $ pegasus-status [wfdir] You can keep checking the status periodically to see that the workflow is making progress. Exercise 4: Examine a submit file and the *.dag.dagman.out files. Do these look familiar to you from previous modules in the book? Pegasus is based on HTCondor and DAGMan. $ cd [wfdir] $ cat 00/00/summarize_ID0000001.sub ... $ cat *.dag.dagman.out ... Exercise 5: Keep checking progress with pegasus-status . Once the workflow is done, display statistics with pegasus-statistics : $ pegasus-status [wfdir] $ pegasus-statistics [wfdir] ... Exercise 6: cd to the output directory and look at the outputs. Which is the most common word used in the 6 books? Hint: $ cd $HOME/workflows/outputs $ head -n 5 *.out Exercise 7: Want to try something larger? Copy the additional 994 ebooks from \\ the many-more-inputs/ directory to the inputs/ directory: $ cp many-more-inputs/* inputs/ As these tasks are really short, let's tell Pegasus to cluster multiple tasks together into jobs. If you do not do this step, the jobs will still run, but not very efficiently. This is because every job has a small scheduling overhead. For short jobs, the overhead is obvious. If we make the jobs longer, the scheduling overhead becomes negligible. To enable the clustering feature, edit the workflow.py script. Find the section under Transformations : wordfreq = Transformation( name=\"wordfreq\", site=\"local\", pfn=TOP_DIR / \"bin/wordfreq\", is_stageable=True, arch=Arch.X86_64 ).add_pegasus_profile(clusters_size=1) Change clusters_size=1 to clusters_size=50 . This informs Pegasus that it is ok to cluster up to 50 of the jobs which use the wordfreq executable. Save the file and re-run the script: $ ./workflow.py Use pegasus-status and pegasus-statistics to monitor your workflow. Using pegasus-statistics , determine how many jobs ended up in your workflow and see how this compares with our initial workflow run.","title":"wordfreq workflow"},{"location":"managing_htc_workloads_on_osg_connect/automated_workflows/tutorial-pegasus/#getting-help","text":"For assistance or questions, please email the OSG User Support team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Getting Help"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/","text":"Using GPUs on the OSPool \u00b6 The Open Science Pool has an increasing number of GPUs available to run jobs. Requesting GPUs \u00b6 To request a GPU for your HTCondor job, you can use the HTCondor request_gpus attribute in your submit file (along with the usual request_cpus , request_memory , and request_disk attributes). For example: request_gpus = 1 request_cpus = 1 request_memory = 4 GB request_disk = 2 GB Currently, a job can only use 1 GPU at the time. You should only request a GPU if your software has been written to use a GPU. It is also worth running test jobs on a GPU versus CPUs-only, to observe the amount of speed up. Specific GPU Requests \u00b6 HTCondor records different GPU attributes that can be used to select specific types of GPU devices. A few attributes that may be useful: CUDACapability : this is NOT the CUDA library, but rather a measure of the GPU's \"Compute Capability\" CUDADriverVersion : maximum version of the CUDA libraries that can be supported on the GPU CUDAGlobalMemoryMb : amount of GPU memory available on the GPU device Any of the attributes above can be used in the submit file's requirements line to select a specific kind of GPU. For example, to request a GPU with more than 8GB of GPU memory, one could use: requirements = (CUDAGlobalMemoryMb >= 8192) If you want a certain type or family of GPUs, we usually recommend using the GPU's 'Compute Capability', known as the CUDACapability by HTCondor. An A100 GPU has a Compute Capability of 8.0, so if you wanted to run on an A100 GPU specifically, the submit file requirement would be: requirements = (CUDACapability == 8.0) Note that the more requirements you include, the fewer resources will be available to you! It's always better to set the minimal possible requirements (ideally, none!) in order to access the greatest amount of computing capacity. Available GPUs \u00b6 Just like CPUs, GPUs are shared with the OSG community only when the resource is idle. Therefore, we do not know exactly what resources are available at what time. When requesting a GPU job, you might land on one of the following types of GPUs: Tesla K20m, K40m (CUDACapability: 3.5) Quadro M5000 (CUDACapability: 5.2) GeForce GTX 1080 Ti (CUDACapability: 6.1) V100 (CUDACapability: 7.0) Quadro RTX 6000 (CUDACapability: 7.5) A100 (CUDACapability: 8.0) A40 (CUDACapability: 8.6) Software and Data Considerations \u00b6 Software for GPUs \u00b6 For GPU-enabled machine learning libraries, we recommend using containers to set up your software for jobs: Using Containers on the OSPool Sample TensorFlow GPU Container Image Definition TensorFlow Example Job Data Needs for GPU Jobs \u00b6 As with any kind of job submission, check your data sizes (per job) before submitting jobs and choose the appropriate file transfer method for your data. See our Data Staging and Transfer guide for details and contact the Research Computing Facilitation team with questions.","title":"Using GPUs on the OSPool"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#using-gpus-on-the-ospool","text":"The Open Science Pool has an increasing number of GPUs available to run jobs.","title":"Using GPUs on the OSPool"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#requesting-gpus","text":"To request a GPU for your HTCondor job, you can use the HTCondor request_gpus attribute in your submit file (along with the usual request_cpus , request_memory , and request_disk attributes). For example: request_gpus = 1 request_cpus = 1 request_memory = 4 GB request_disk = 2 GB Currently, a job can only use 1 GPU at the time. You should only request a GPU if your software has been written to use a GPU. It is also worth running test jobs on a GPU versus CPUs-only, to observe the amount of speed up.","title":"Requesting GPUs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#specific-gpu-requests","text":"HTCondor records different GPU attributes that can be used to select specific types of GPU devices. A few attributes that may be useful: CUDACapability : this is NOT the CUDA library, but rather a measure of the GPU's \"Compute Capability\" CUDADriverVersion : maximum version of the CUDA libraries that can be supported on the GPU CUDAGlobalMemoryMb : amount of GPU memory available on the GPU device Any of the attributes above can be used in the submit file's requirements line to select a specific kind of GPU. For example, to request a GPU with more than 8GB of GPU memory, one could use: requirements = (CUDAGlobalMemoryMb >= 8192) If you want a certain type or family of GPUs, we usually recommend using the GPU's 'Compute Capability', known as the CUDACapability by HTCondor. An A100 GPU has a Compute Capability of 8.0, so if you wanted to run on an A100 GPU specifically, the submit file requirement would be: requirements = (CUDACapability == 8.0) Note that the more requirements you include, the fewer resources will be available to you! It's always better to set the minimal possible requirements (ideally, none!) in order to access the greatest amount of computing capacity.","title":"Specific GPU Requests"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#available-gpus","text":"Just like CPUs, GPUs are shared with the OSG community only when the resource is idle. Therefore, we do not know exactly what resources are available at what time. When requesting a GPU job, you might land on one of the following types of GPUs: Tesla K20m, K40m (CUDACapability: 3.5) Quadro M5000 (CUDACapability: 5.2) GeForce GTX 1080 Ti (CUDACapability: 6.1) V100 (CUDACapability: 7.0) Quadro RTX 6000 (CUDACapability: 7.5) A100 (CUDACapability: 8.0) A40 (CUDACapability: 8.6)","title":"Available GPUs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#software-and-data-considerations","text":"","title":"Software and Data Considerations"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#software-for-gpus","text":"For GPU-enabled machine learning libraries, we recommend using containers to set up your software for jobs: Using Containers on the OSPool Sample TensorFlow GPU Container Image Definition TensorFlow Example Job","title":"Software for GPUs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/gpu-jobs/#data-needs-for-gpu-jobs","text":"As with any kind of job submission, check your data sizes (per job) before submitting jobs and choose the appropriate file transfer method for your data. See our Data Staging and Transfer guide for details and contact the Research Computing Facilitation team with questions.","title":"Data Needs for GPU Jobs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/large-memory-jobs/","text":"Large Memory Jobs \u00b6 By default, 2 GB of RAM (aka memory) will be assigned to your jobs. However, some jobs will require additional memory to complete successfully. To request more memory, use the HTCondor request_memory attribute in your submit file. The default unit is MB. For example, the following will request 12 GB: request_memory = 12228 You might be wondering why the above is requesting 12228 MB for 12 GB. That's because byte units don't actually scale by 1000 (10^10) like the metric system, but instead scale by 1024 (2^10) due to the binary nature of bytes. Alternatively, you can define a memory request using standard units request_memory = 12GB We recommend always explictly defining the byte units in your request_memory statement. Please note that the OSG has limited resources available for large memory jobs. Requesting jobs with higher memory needs will results in longer than average queue times for these jobs.","title":"Large Memory Jobs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/large-memory-jobs/#large-memory-jobs","text":"By default, 2 GB of RAM (aka memory) will be assigned to your jobs. However, some jobs will require additional memory to complete successfully. To request more memory, use the HTCondor request_memory attribute in your submit file. The default unit is MB. For example, the following will request 12 GB: request_memory = 12228 You might be wondering why the above is requesting 12228 MB for 12 GB. That's because byte units don't actually scale by 1000 (10^10) like the metric system, but instead scale by 1024 (2^10) due to the binary nature of bytes. Alternatively, you can define a memory request using standard units request_memory = 12GB We recommend always explictly defining the byte units in your request_memory statement. Please note that the OSG has limited resources available for large memory jobs. Requesting jobs with higher memory needs will results in longer than average queue times for these jobs.","title":"Large Memory Jobs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/multicore-jobs/","text":"Multicore Jobs \u00b6 Please note, the OSG has limited support for multicore jobs. Multicore jobs can be submitted for threaded or OpenMP applications. To request multiple cores (aka cpus) use the HTCondor request_cpus attribute in your submit file. Example: request_cpus = 8 We recommend requesting a maximum of 8 cpus. Important considerations When submitting multicore jobs please note that you will also have to tell your code or application to use the number of cpus requested in your submit file. Do not use core auto-detection as it might detect more cores than what were actually assigned to your job. MPI Jobs For jobs that require MPI, see our OpenMPI Jobs guide.","title":"Multicore Jobs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/multicore-jobs/#multicore-jobs","text":"Please note, the OSG has limited support for multicore jobs. Multicore jobs can be submitted for threaded or OpenMP applications. To request multiple cores (aka cpus) use the HTCondor request_cpus attribute in your submit file. Example: request_cpus = 8 We recommend requesting a maximum of 8 cpus. Important considerations When submitting multicore jobs please note that you will also have to tell your code or application to use the number of cpus requested in your submit file. Do not use core auto-detection as it might detect more cores than what were actually assigned to your job. MPI Jobs For jobs that require MPI, see our OpenMPI Jobs guide.","title":"Multicore Jobs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/openmpi-jobs/","text":"OpenMPI Jobs \u00b6 Even though the Open Science Pool is a high throughput computing system, sometimes there is a need to run small OpenMPI based jobs. OSG has limited support for this, as long as the core count is small (4 is known to work well, 8 and 16 becomes more difficult due to the limited number of resources). To get started, first compile your code using the OpenMPI in the modules system. For example: $ module load openmpi $ mpicc -o hello hello.c You can test the executable locally using mpiexec : $ mpiexec -n 4 hello Hello world from process 1 of 4 Hello world from process 3 of 4 Hello world from process 0 of 4 Hello world from process 2 of 4 To run your code as a job on the Open Science Pool, first create a wrapper.sh . Example: 1 2 3 4 5 6 7 #!/bin/bash set -e module load openmpi mpiexec -n 4 hello Then, a job submit file: Requirements = OSGVO_OS_STRING == \"RHEL 7\" && TARGET.Arch == \"X86_64\" && HAS_MODULES == True +has_mpi = true request_cpus = 4 request_memory = 4 GB executable = wrapper.sh transfer_input_files = hello output = job.out.$(Cluster).$(Process) error = job.error.$(Cluster).$(Process) log = job.log.$(Cluster).$(Process) queue 1 Note how the executable is the wrapper.sh script, and that the real executable hello is transferred using the transfer_input_files mechanism. Additionally, the submit file line +has_mpi = true should be added to all MPI jobs so that HTCondor matches these jobs to the correct execute machines. Please make sure that the number of cores specified in the submit file via request_cpus match the -n argument in the wrapper.sh file.","title":"OpenMPI Jobs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/openmpi-jobs/#openmpi-jobs","text":"Even though the Open Science Pool is a high throughput computing system, sometimes there is a need to run small OpenMPI based jobs. OSG has limited support for this, as long as the core count is small (4 is known to work well, 8 and 16 becomes more difficult due to the limited number of resources). To get started, first compile your code using the OpenMPI in the modules system. For example: $ module load openmpi $ mpicc -o hello hello.c You can test the executable locally using mpiexec : $ mpiexec -n 4 hello Hello world from process 1 of 4 Hello world from process 3 of 4 Hello world from process 0 of 4 Hello world from process 2 of 4 To run your code as a job on the Open Science Pool, first create a wrapper.sh . Example: 1 2 3 4 5 6 7 #!/bin/bash set -e module load openmpi mpiexec -n 4 hello Then, a job submit file: Requirements = OSGVO_OS_STRING == \"RHEL 7\" && TARGET.Arch == \"X86_64\" && HAS_MODULES == True +has_mpi = true request_cpus = 4 request_memory = 4 GB executable = wrapper.sh transfer_input_files = hello output = job.out.$(Cluster).$(Process) error = job.error.$(Cluster).$(Process) log = job.log.$(Cluster).$(Process) queue 1 Note how the executable is the wrapper.sh script, and that the real executable hello is transferred using the transfer_input_files mechanism. Additionally, the submit file line +has_mpi = true should be added to all MPI jobs so that HTCondor matches these jobs to the correct execute machines. Please make sure that the number of cores specified in the submit file via request_cpus match the -n argument in the wrapper.sh file.","title":"OpenMPI Jobs"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/","text":"Running OSG jobs on XSEDE \u00b6 Overview \u00b6 The OSG promotes science by: enabling a framework of distributed computing and storage resources making available a set of services and methods that enable better access to ever increasing computing resources for researchers and communities providing resource sharing principles and software that enable distributed High Throughput Computing (dHTC) for users and communities at all scales. The OSG facilitates access to dHTC for scientific research in the US. The resources accessible through the OSG are contributed by the community, organized by the OSG, and governed by the OSG Consortium ; an overview is available at An Introduction to OSG . In 2017, OSG is comprised of about 126 institutions with ~120 active sites that collectively support usage of ~4,000,000 core hours per day. Up-to-date usage metrics are available at the OSG Usage Display . Cores that are not being used at any point in time by the owning communities are made available for shared use by other researchers; this usage mode is called opportunistic access. OSG supports XSEDE users by providing a Virtual Cluster that forms an abstraction layer to access the opportunistic cores in the distributed OSG infrastructure. This interface allows XSEDE users to view the OSG as a single cluster to manage their jobs, provide the inputs and retrieve the outputs. XSEDE users access the OSG via the OSG-XSEDE login host that appears as a resource in the XSEDE infrastructure. Computation that is a good match for OSG \u00b6 High throughput workflows with simple system and data dependencies are a good fit for OSG. The Condor manual has an overview of high throughput computing . Jobs submitted into the OSG Virtual Cluster will be executed on machines at several remote physical clusters. These machines may differ in terms of computing environment from the submit node. Therefore it is important that the jobs are as self-contained as possible by generic binaries and data that can be either carried with the job, or staged on demand. Please consider the following guidelines: Software should preferably be single threaded , using less than 2 GB memory and each invocation should run for 1-12 hours . Please contact the support listed below for more information about these capabilities. System level check pointing, such as the HTCondor standard universe, is not available. Application level check pointing, for example applications writing out state and restart files, can be made to work on the system. Compute sites in the OSG can be configured to use pre-emption, which means jobs can be automatically killed if higher priority jobs enter the system. Pre-empted jobs will restart on another site, but it is important that the jobs can handle multiple restarts. Binaries should preferably be statically linked. However, dynamically linked binaries with standard library dependencies, built for a 64-bit Red Hat Enterprise Linux (RHEL) 6 machines will also work. Also, interpreted languages such as Python or Perl will work as long as there are no special module requirements. Input and output data for each job should be < 10 GB to allow them to be pulled in by the jobs, processed and pushed back to the submit node. Note that the OSG Virtual Cluster does not currently have a global shared file system, so jobs with such dependencies will not work. Software dependencies can be difficult to accommodate unless the software can be staged with the job. The following are examples of computations that are not good matches for OSG: Tightly coupled computations, for example MPI based communication, will not work well on OSG due to the distributed nature of the infrastructure. Computations requiring a shared file system will not work, as there is no shared filesystem between the different clusters on OSG. Computations requiring complex software deployments are not a good fit. There is limited support for distributing software to the compute clusters, but for complex software, or licensed software, deployment can be a major task. System Configuration \u00b6 The OSG Virtual Cluster is a Condor pool overlay on top of OSG resources. The pool is dynamically sized based on the demand, the number of jobs in the queue, and supply, resource availability at the OSG resources. It is expected that the average number of resources, on average, available to XSEDE users will be in the order of 1,000 cores. One important difference between the OSG Virtual Cluster and most of the other XSEDE resources is that the OSG Virtual Cluster does not have a shared file system. This means that your jobs will have to bring executables and input data. Condor can transfer the files for you, but you will have to identify and list the files in your Condor job description file. Local storage space at the submission site is controlled by quota. Your home directory has a quota of 10 GBs and your work directory /local-scratch/$USER has a quota of 1 TB. There are no global quotas on the remote compute nodes, but expect that about 10 GBs are available as scratch space for each job. System Access \u00b6 The preferred method to access the system is via the XSEDE Single Sign On (SSO) Hub. Please see the sso documentation for details. A secondary access methor is to use SSH public key authentication. Secure shell users should feel free to append their public RSA key to their ~/.ssh/authorized_keys file to enable access from their own computer. Please login once via the SSO Hub to install your key. Please make sure that the permissions on the .ssh directory and the authorized_keys file have appropiate permissions. For example $ chmod 755 ~/.ssh $ chmod 644 ~/.ssh/authorized_keys Application Development< \u00b6 Most of the clusters in OSG are running Red Hat Enterprise Linux (RHEL) 6 or 7, or some derivative thereof, on an x86_64 architecture. For your application to work well in this environment, it is recommend that the application is compiled on a similar system, for example on the OSG Virtual Cluster login system: submit-1.osg.xsede.org . It is also recommended that the application be statically linked, or alternatively dynamically linked against just a few standard libraries. What libraries a binary depends on can be checked using the Unix ldd command-line utility: $ ldd a.out a.out is a static executable In the case of interpreted languages like Python and Perl, applications have to either use only standard modules, or be able to ship the modules with the jobs. Please note that different compute nodes might have different versions of these tools installed. A good solution to complex software stack is Singularity containers which are described below. Running Your Application \u00b6 The OSG Virtual Cluster is based on Condor and the Condor manual provides a reference for command line tools. The commonly used tools are: **condor_submit** - Takes a Condor submit file and adds the job to the queue **condor_q** - Lists the jobs in the queue. Can be invoked with your username to limit the list of jobs to your jobs: condor_q $USER **condor_status** - Lists the available slots in the system. Note that this is a dynamic list and if there are no jobs in the system, condor_status may return an empty list **condor_rm** - Remove a job from the queue. If you are running a DAG, please condor_rm the id of the DAG to remove the whole workflow. Submitting a Simple Job \u00b6 Below is a basic job description for the Virtual Cluster. universe = vanilla # specifies the XSEDE project to charge the job usage to - this is a # required attribute for all jobs submitted on the OSG-XSEDE resource +ProjectName = \"TG-NNNNNN\" # requirements is an expression to specify machines that can run jobs requirements = OSGVO_OS_STRING == \"RHEL 6\" && Arch == \"X86_64\" && HAS_MODULES == True request_cpus = 1 request_memory = 2 GB request_disk = 10 GB executable = /bin/hostname arguments = -f should_transfer_files = YES WhenToTransferOutput = ON_EXIT output = job.out error = job.err log = job.log notification = NEVER queue Create a file named job.condor containing the above text and then run: $ condor_submit job.condor You can check the status of the job using the condor_q command. Note: The Open Science Pool is a distributed resource, and there will be minor differences in the compute nodes, for example in what system libraries and tools are installed. Therefore, when running a large number of jobs, it is important to detect and handle job failures correctly in an automatic fashion. It is recommended that your application uses non-zero exit code convention to indicate failures, and that you enable retries in your Condor submit files. For example: # stay in queue on failures on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # retry job 3 times, pause 1 hour between retries periodic_release = (NumJobStarts &lt; 3) &amp;&amp; ((CurrentTime - EnteredCurrentStatus) &gt; (60*60)) Job Example: Java with a job wrapper \u00b6 The following is an example on how to run Java code on Open Science Pool. The job requirements specifies that the job requires Java, and a wrapper script is used to invoke Java. File: condor.sub universe = vanilla # specifies the XSEDE project to charge the job usage to - this is a # required attribute for all jobs submitted on the OSG-XSEDE resource +ProjectName = \"TG-NNNNNN\" # requirements is an expression to specify machines that can run jobs requirements = HAS_JAVA == True # stay in queue on failures on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # retry job 3 times, pause 1 hour between retries periodic_release = (NumJobStarts < 3) && ((CurrentTime - EnteredCurrentStatus) > (60*60)) executable = wrapper.sh should_transfer_files = YES WhenToTransferOutput = ON_EXIT # a list of files that the job needs transfer_input_files = HelloWorld.jar output = job.out error = job.err log = job.log notification = NEVER queue File: wrapper.sh 1 2 3 4 5 #!/bin/bash set -e java HelloWorld Sample Jobs and Workflows \u00b6 A set of sample jobs and workflows can be found under /opt/sample-jobs on the submit-1.osg.xsede.org host. README files are included with details for each sample. /opt/sample-jobs/single/ contains a single Condor job example. Single jobs can be used for smaller set of jobs or if the job structure is simple, such as parameter sweeps. A sample-app package ( sampleapp.tgz ) is available in the /opt/sample-jobs/sampleapp/ directory. This package shows how to build a library and an executable, both with dynamic and static linking, and submit the job to a set of different schedulers available on XSEDE. The package includes submit files for PBS, SGE and Condor. DAGMan is a HTCondor workflow tool. It allows the creation of a directed acyclic graph of jobs to be run, and then DAGMan submits and manages the jobs. DAGMan is also useful if you have a large number of jobs, even if there are no job inter-dependencies, as DAGMan can keep track of failures and provide a restart mechanism if part of the workflow fails. A sample DAGMan workflow can be found in /opt/sample-jobs/dag/ Pegasus is a workflow system that can be used for more complex workflows. It plans abstract workflow representations down to an executable workflow and uses Condor DAGMan as a workflow executor. Pegasus also provides debugging and monitoring tools that allow users to easily track failures in their workflows. Workflow provenance information is collected and can be summarized with the provided statistical and plotting tools. A sample Pegasus workflow can be found in /opt/sample-jobs/pegasus/ . Singularity Containers \u00b6 Singularity containers provide a great solution for complex software stacks or OS requirements, and OSG has easy to use integrated support for such containers. Full details can be found in the Singularity Containers . Distribute data with Stash \u00b6 Stash is a data solution used under OSGConnect , but is partly also available for OSG XSEDE users. Files under /local-scratch/public_stash/ will automatically synchronize to the globally available /cvmfs/stash.osgstorage.org/user/xd-login/public/ file system, which is available to the majority of OSG connected compute nodes. This is a great way to distribute software and commonly used data sets. To get started, create your own sub directory: $ mkdir -p /local-scratch/public_stash/$USER Now, populate that directory with the software and data required for your jobs. The synchronization can take couple of hours. You can verify the data has reached the /cvmfs system by using ls : $ ls /cvmfs/stash.osgstorage.org/user/xd-login/public/ To steer your jobs to compute nodes which can access the file system, add HAS_CVMFS_stash_osgstorage_org == True to your job requirements. For example: requirements = OSGVO_OS_STRING == \"RHEL 6\" && Arch == \"X86_64\" && HAS_MODULES == True && HAS_CVMFS_stash_osgstorage_org == True Once a job starts up on such a compute node, the job can read directly from /cvmfs/stash.osgstorage.org/user/xd-login/public/ How to get help using OSG \u00b6 XSEDE users of OSG may get technical support by contacting OSG Research Facilitation staff at email support@opensciencegrid.org . Users may also contact the XSEDE helpdesk .","title":"Running OSG jobs on XSEDE"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#running-osg-jobs-on-xsede","text":"","title":"Running OSG jobs on XSEDE"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#overview","text":"The OSG promotes science by: enabling a framework of distributed computing and storage resources making available a set of services and methods that enable better access to ever increasing computing resources for researchers and communities providing resource sharing principles and software that enable distributed High Throughput Computing (dHTC) for users and communities at all scales. The OSG facilitates access to dHTC for scientific research in the US. The resources accessible through the OSG are contributed by the community, organized by the OSG, and governed by the OSG Consortium ; an overview is available at An Introduction to OSG . In 2017, OSG is comprised of about 126 institutions with ~120 active sites that collectively support usage of ~4,000,000 core hours per day. Up-to-date usage metrics are available at the OSG Usage Display . Cores that are not being used at any point in time by the owning communities are made available for shared use by other researchers; this usage mode is called opportunistic access. OSG supports XSEDE users by providing a Virtual Cluster that forms an abstraction layer to access the opportunistic cores in the distributed OSG infrastructure. This interface allows XSEDE users to view the OSG as a single cluster to manage their jobs, provide the inputs and retrieve the outputs. XSEDE users access the OSG via the OSG-XSEDE login host that appears as a resource in the XSEDE infrastructure.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#computation-that-is-a-good-match-for-osg","text":"High throughput workflows with simple system and data dependencies are a good fit for OSG. The Condor manual has an overview of high throughput computing . Jobs submitted into the OSG Virtual Cluster will be executed on machines at several remote physical clusters. These machines may differ in terms of computing environment from the submit node. Therefore it is important that the jobs are as self-contained as possible by generic binaries and data that can be either carried with the job, or staged on demand. Please consider the following guidelines: Software should preferably be single threaded , using less than 2 GB memory and each invocation should run for 1-12 hours . Please contact the support listed below for more information about these capabilities. System level check pointing, such as the HTCondor standard universe, is not available. Application level check pointing, for example applications writing out state and restart files, can be made to work on the system. Compute sites in the OSG can be configured to use pre-emption, which means jobs can be automatically killed if higher priority jobs enter the system. Pre-empted jobs will restart on another site, but it is important that the jobs can handle multiple restarts. Binaries should preferably be statically linked. However, dynamically linked binaries with standard library dependencies, built for a 64-bit Red Hat Enterprise Linux (RHEL) 6 machines will also work. Also, interpreted languages such as Python or Perl will work as long as there are no special module requirements. Input and output data for each job should be < 10 GB to allow them to be pulled in by the jobs, processed and pushed back to the submit node. Note that the OSG Virtual Cluster does not currently have a global shared file system, so jobs with such dependencies will not work. Software dependencies can be difficult to accommodate unless the software can be staged with the job. The following are examples of computations that are not good matches for OSG: Tightly coupled computations, for example MPI based communication, will not work well on OSG due to the distributed nature of the infrastructure. Computations requiring a shared file system will not work, as there is no shared filesystem between the different clusters on OSG. Computations requiring complex software deployments are not a good fit. There is limited support for distributing software to the compute clusters, but for complex software, or licensed software, deployment can be a major task.","title":"Computation that is a good match for OSG"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#system-configuration","text":"The OSG Virtual Cluster is a Condor pool overlay on top of OSG resources. The pool is dynamically sized based on the demand, the number of jobs in the queue, and supply, resource availability at the OSG resources. It is expected that the average number of resources, on average, available to XSEDE users will be in the order of 1,000 cores. One important difference between the OSG Virtual Cluster and most of the other XSEDE resources is that the OSG Virtual Cluster does not have a shared file system. This means that your jobs will have to bring executables and input data. Condor can transfer the files for you, but you will have to identify and list the files in your Condor job description file. Local storage space at the submission site is controlled by quota. Your home directory has a quota of 10 GBs and your work directory /local-scratch/$USER has a quota of 1 TB. There are no global quotas on the remote compute nodes, but expect that about 10 GBs are available as scratch space for each job.","title":"System Configuration"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#system-access","text":"The preferred method to access the system is via the XSEDE Single Sign On (SSO) Hub. Please see the sso documentation for details. A secondary access methor is to use SSH public key authentication. Secure shell users should feel free to append their public RSA key to their ~/.ssh/authorized_keys file to enable access from their own computer. Please login once via the SSO Hub to install your key. Please make sure that the permissions on the .ssh directory and the authorized_keys file have appropiate permissions. For example $ chmod 755 ~/.ssh $ chmod 644 ~/.ssh/authorized_keys","title":"System Access"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#application-development","text":"Most of the clusters in OSG are running Red Hat Enterprise Linux (RHEL) 6 or 7, or some derivative thereof, on an x86_64 architecture. For your application to work well in this environment, it is recommend that the application is compiled on a similar system, for example on the OSG Virtual Cluster login system: submit-1.osg.xsede.org . It is also recommended that the application be statically linked, or alternatively dynamically linked against just a few standard libraries. What libraries a binary depends on can be checked using the Unix ldd command-line utility: $ ldd a.out a.out is a static executable In the case of interpreted languages like Python and Perl, applications have to either use only standard modules, or be able to ship the modules with the jobs. Please note that different compute nodes might have different versions of these tools installed. A good solution to complex software stack is Singularity containers which are described below.","title":"Application Development&lt;"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#running-your-application","text":"The OSG Virtual Cluster is based on Condor and the Condor manual provides a reference for command line tools. The commonly used tools are: **condor_submit** - Takes a Condor submit file and adds the job to the queue **condor_q** - Lists the jobs in the queue. Can be invoked with your username to limit the list of jobs to your jobs: condor_q $USER **condor_status** - Lists the available slots in the system. Note that this is a dynamic list and if there are no jobs in the system, condor_status may return an empty list **condor_rm** - Remove a job from the queue. If you are running a DAG, please condor_rm the id of the DAG to remove the whole workflow.","title":"Running Your Application"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#submitting-a-simple-job","text":"Below is a basic job description for the Virtual Cluster. universe = vanilla # specifies the XSEDE project to charge the job usage to - this is a # required attribute for all jobs submitted on the OSG-XSEDE resource +ProjectName = \"TG-NNNNNN\" # requirements is an expression to specify machines that can run jobs requirements = OSGVO_OS_STRING == \"RHEL 6\" && Arch == \"X86_64\" && HAS_MODULES == True request_cpus = 1 request_memory = 2 GB request_disk = 10 GB executable = /bin/hostname arguments = -f should_transfer_files = YES WhenToTransferOutput = ON_EXIT output = job.out error = job.err log = job.log notification = NEVER queue Create a file named job.condor containing the above text and then run: $ condor_submit job.condor You can check the status of the job using the condor_q command. Note: The Open Science Pool is a distributed resource, and there will be minor differences in the compute nodes, for example in what system libraries and tools are installed. Therefore, when running a large number of jobs, it is important to detect and handle job failures correctly in an automatic fashion. It is recommended that your application uses non-zero exit code convention to indicate failures, and that you enable retries in your Condor submit files. For example: # stay in queue on failures on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # retry job 3 times, pause 1 hour between retries periodic_release = (NumJobStarts &lt; 3) &amp;&amp; ((CurrentTime - EnteredCurrentStatus) &gt; (60*60))","title":"Submitting a Simple Job"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#job-example-java-with-a-job-wrapper","text":"The following is an example on how to run Java code on Open Science Pool. The job requirements specifies that the job requires Java, and a wrapper script is used to invoke Java. File: condor.sub universe = vanilla # specifies the XSEDE project to charge the job usage to - this is a # required attribute for all jobs submitted on the OSG-XSEDE resource +ProjectName = \"TG-NNNNNN\" # requirements is an expression to specify machines that can run jobs requirements = HAS_JAVA == True # stay in queue on failures on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # retry job 3 times, pause 1 hour between retries periodic_release = (NumJobStarts < 3) && ((CurrentTime - EnteredCurrentStatus) > (60*60)) executable = wrapper.sh should_transfer_files = YES WhenToTransferOutput = ON_EXIT # a list of files that the job needs transfer_input_files = HelloWorld.jar output = job.out error = job.err log = job.log notification = NEVER queue File: wrapper.sh 1 2 3 4 5 #!/bin/bash set -e java HelloWorld","title":"Job Example: Java with a job wrapper"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#sample-jobs-and-workflows","text":"A set of sample jobs and workflows can be found under /opt/sample-jobs on the submit-1.osg.xsede.org host. README files are included with details for each sample. /opt/sample-jobs/single/ contains a single Condor job example. Single jobs can be used for smaller set of jobs or if the job structure is simple, such as parameter sweeps. A sample-app package ( sampleapp.tgz ) is available in the /opt/sample-jobs/sampleapp/ directory. This package shows how to build a library and an executable, both with dynamic and static linking, and submit the job to a set of different schedulers available on XSEDE. The package includes submit files for PBS, SGE and Condor. DAGMan is a HTCondor workflow tool. It allows the creation of a directed acyclic graph of jobs to be run, and then DAGMan submits and manages the jobs. DAGMan is also useful if you have a large number of jobs, even if there are no job inter-dependencies, as DAGMan can keep track of failures and provide a restart mechanism if part of the workflow fails. A sample DAGMan workflow can be found in /opt/sample-jobs/dag/ Pegasus is a workflow system that can be used for more complex workflows. It plans abstract workflow representations down to an executable workflow and uses Condor DAGMan as a workflow executor. Pegasus also provides debugging and monitoring tools that allow users to easily track failures in their workflows. Workflow provenance information is collected and can be summarized with the provided statistical and plotting tools. A sample Pegasus workflow can be found in /opt/sample-jobs/pegasus/ .","title":"Sample Jobs and Workflows"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#singularity-containers","text":"Singularity containers provide a great solution for complex software stacks or OS requirements, and OSG has easy to use integrated support for such containers. Full details can be found in the Singularity Containers .","title":"Singularity Containers"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#distribute-data-with-stash","text":"Stash is a data solution used under OSGConnect , but is partly also available for OSG XSEDE users. Files under /local-scratch/public_stash/ will automatically synchronize to the globally available /cvmfs/stash.osgstorage.org/user/xd-login/public/ file system, which is available to the majority of OSG connected compute nodes. This is a great way to distribute software and commonly used data sets. To get started, create your own sub directory: $ mkdir -p /local-scratch/public_stash/$USER Now, populate that directory with the software and data required for your jobs. The synchronization can take couple of hours. You can verify the data has reached the /cvmfs system by using ls : $ ls /cvmfs/stash.osgstorage.org/user/xd-login/public/ To steer your jobs to compute nodes which can access the file system, add HAS_CVMFS_stash_osgstorage_org == True to your job requirements. For example: requirements = OSGVO_OS_STRING == \"RHEL 6\" && Arch == \"X86_64\" && HAS_MODULES == True && HAS_CVMFS_stash_osgstorage_org == True Once a job starts up on such a compute node, the job can read directly from /cvmfs/stash.osgstorage.org/user/xd-login/public/","title":"Distribute data with Stash"},{"location":"managing_htc_workloads_on_osg_connect/considerations_for_specific_resource_needs/osg-xsede/#how-to-get-help-using-osg","text":"XSEDE users of OSG may get technical support by contacting OSG Research Facilitation staff at email support@opensciencegrid.org . Users may also contact the XSEDE helpdesk .","title":"How to get help using OSG"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/","text":"Indicate the Duration Category of Your Jobs \u00b6 Why Job Duration Categories? \u00b6 To maximize the value of the capacity contributed by the different organizations to the Open Science Pool (OSPool), users are requested to identify one of three duration categories for their jobs. These categories should be selected based upon test jobs (run on the OSPool) and allow for more effective scheduling of the capacity contributed to the pool, honoring the community\u2019s shared responsibility for efficient use of the contributed resources. As a reminder, jobs with single executions longer than 20 hours in tests on the OSPool should not be submitted , without self-checkpointing (see further below). Every job submitted via an OSG Connect access point must be labeled with a Job Duration Category upon submission. By knowing the expected duration, the OSG is working to be able to direct longer-running jobs to resources that are faster and are interrupted less, while shorter jobs can run across more of the OSPool for better overall throughput. Specify a Job Duration Category \u00b6 The JobDurationCategory must be listed anywhere prior to the final \u2018queue\u2019 statement of the submit file, as below: +JobDurationCategory = \u201cLong\u201d JobDurationCategory Expected Job Duration Maximum Allowed Duration Medium (default) <10 hrs 20 hrs Long <20 hrs 40 hrs If the user does not indicate a JobDurationCategory in the submit file, the relevant job(s) will be labeled as Medium by default. Batches with jobs that individually execute for longer than 20 hours are not a good fit for the OSPool . If your jobs are self-checkpointing, see \u201cSelf-Checkpointing Jobs\u201d, further below. Test Jobs for Expected Duration \u00b6 As part of the preparation for running a full-scale job batch , users should test a subset (first ~10, then 100 or 1000) of their jobs with the Medium or Long categories, and then review actual job execution durations in the job log files. If the user expects potentially significant variation in job durations within a single batch, a longer JobDurationCategory may be warranted relative to the duration of test jobs. Or, if variations in job duration may be predictable, the user may choose to submit different subsets of jobs with different Job Duration Categories. OSG Facilitators have a lot of experience with approaches for achieving shorter jobs (e.g. breaking up work into shorter, more numerous jobs; self-checkpointing; automated sequential job submissions; etc.) Get in touch, and we'll help you work through a solution!! support@opensciencegrid.org Maximum Allowed Duration \u00b6 Jobs in each category will be placed on hold in the queue if they run longer than their Maximum Allowed Duration (starting Tuesday, Nov 16, 2021). In that case, the user may remove and resubmit the jobs, identifying a longer category. Jobs that test as longer than 20 hours are not a good fit for the OSPool resources, and should not be submitted prior to contacting support@opensciencegrid.org to discuss options . The Maximum Allowed Durations are longer than the Expected Job Durations in order to accommodate CPU speed variations across OSPool computing resources, as well as other contributions to job duration that may not be apparent in smaller test batches. Similarly, Long jobs held after running longer than 40 hours represent significant wasted capacity and should never be released or resubmitted by the user without first taking steps to modify and test the jobs to run shorter. Self-Checkpointing Jobs \u00b6 Jobs that self-checkpoint at least every 10 hours are an excellent way for users to run jobs that would otherwise be longer in total execution time than the durations listed above. Jobs that complete a checkpoint at least as often as allowed for their JobDurationCategory will not be held. We are excited to help you think through and implement self-checkpointing. Get in touch via support@opensciencegrid.org if you have questions. :)","title":"Indicate the Duration Category of Your Jobs"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/#indicate-the-duration-category-of-your-jobs","text":"","title":"Indicate the Duration Category of Your Jobs"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/#why-job-duration-categories","text":"To maximize the value of the capacity contributed by the different organizations to the Open Science Pool (OSPool), users are requested to identify one of three duration categories for their jobs. These categories should be selected based upon test jobs (run on the OSPool) and allow for more effective scheduling of the capacity contributed to the pool, honoring the community\u2019s shared responsibility for efficient use of the contributed resources. As a reminder, jobs with single executions longer than 20 hours in tests on the OSPool should not be submitted , without self-checkpointing (see further below). Every job submitted via an OSG Connect access point must be labeled with a Job Duration Category upon submission. By knowing the expected duration, the OSG is working to be able to direct longer-running jobs to resources that are faster and are interrupted less, while shorter jobs can run across more of the OSPool for better overall throughput.","title":"Why Job Duration Categories?"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/#specify-a-job-duration-category","text":"The JobDurationCategory must be listed anywhere prior to the final \u2018queue\u2019 statement of the submit file, as below: +JobDurationCategory = \u201cLong\u201d JobDurationCategory Expected Job Duration Maximum Allowed Duration Medium (default) <10 hrs 20 hrs Long <20 hrs 40 hrs If the user does not indicate a JobDurationCategory in the submit file, the relevant job(s) will be labeled as Medium by default. Batches with jobs that individually execute for longer than 20 hours are not a good fit for the OSPool . If your jobs are self-checkpointing, see \u201cSelf-Checkpointing Jobs\u201d, further below.","title":"Specify a Job Duration Category"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/#test-jobs-for-expected-duration","text":"As part of the preparation for running a full-scale job batch , users should test a subset (first ~10, then 100 or 1000) of their jobs with the Medium or Long categories, and then review actual job execution durations in the job log files. If the user expects potentially significant variation in job durations within a single batch, a longer JobDurationCategory may be warranted relative to the duration of test jobs. Or, if variations in job duration may be predictable, the user may choose to submit different subsets of jobs with different Job Duration Categories. OSG Facilitators have a lot of experience with approaches for achieving shorter jobs (e.g. breaking up work into shorter, more numerous jobs; self-checkpointing; automated sequential job submissions; etc.) Get in touch, and we'll help you work through a solution!! support@opensciencegrid.org","title":"Test Jobs for Expected Duration"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/#maximum-allowed-duration","text":"Jobs in each category will be placed on hold in the queue if they run longer than their Maximum Allowed Duration (starting Tuesday, Nov 16, 2021). In that case, the user may remove and resubmit the jobs, identifying a longer category. Jobs that test as longer than 20 hours are not a good fit for the OSPool resources, and should not be submitted prior to contacting support@opensciencegrid.org to discuss options . The Maximum Allowed Durations are longer than the Expected Job Durations in order to accommodate CPU speed variations across OSPool computing resources, as well as other contributions to job duration that may not be apparent in smaller test batches. Similarly, Long jobs held after running longer than 40 hours represent significant wasted capacity and should never be released or resubmitted by the user without first taking steps to modify and test the jobs to run shorter.","title":"Maximum Allowed Duration"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/jobdurationcategory/#self-checkpointing-jobs","text":"Jobs that self-checkpoint at least every 10 hours are an excellent way for users to run jobs that would otherwise be longer in total execution time than the durations listed above. Jobs that complete a checkpoint at least as often as allowed for their JobDurationCategory will not be held. We are excited to help you think through and implement self-checkpointing. Get in touch via support@opensciencegrid.org if you have questions. :)","title":"Self-Checkpointing Jobs"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/","text":"Determining the Amount of Resources to Request in a Submit File \u00b6 Learning Objectives \u00b6 This guide discuses the following: - Best practices for testing jobs and scaling up your analysis. - How to determine the amount of resources (CPU, memory, disk space) to request in a submit file. Overview \u00b6 Much of HTCondor's power comes from the ability to run a large number of jobs simultaneously. To optimize your work with a high-throughput computing (HTC) approach, you will need to test and optimize the resource requests of those jobs to only request the amount of memory, disk, and cpus truly needed. This is an important practice that will maximize your throughput by optimizing the number of potential 'slots' in the OSPool that your jobs can match to, reducing the overall turnaround time for completing a whole batch. If you have questions or are unsure if and how your work can be broken up, please contact us at support@opensciencegrid.org This guide will describe best practices and general tips for testing your job resource requests before scaling up to submit your full set of jobs. Additional information is also available from the following \"Introduction to High Throughput Computing with HTCondor\" 2020 OSG Virtual Pilot School lecture video: Always Start With Test Jobs \u00b6 Submitting test jobs is an important first step for optimizing the resource requests of your jobs. We always recommend submitting a few (3-10) test jobs first before scaling up, whether this is your first time using OSG or you're an experienced user starting a new workflow. If you plan to submit thousands of jobs, you may even want to run an intermediate test of 100-1,000 jobs to catch any failures or holds that mean your jobs have additional requirements they need to specify (and which OSG staff can help you to identify, based upon your tests). Some general tips for test jobs: Select smaller data sets or subsets of data for your first test jobs. Using smaller data will keep the resource needs of your jobs low which will help get test jobs to start and complete sooner, when you're just making sure that your submit file and other logistical aspects of jobs submission are as you want them. If possible, submit test jobs that will reproduce results you've gotten using another system. This approach can be used as a good \"sanity check\" as you'll be able to compare the results of the test to those previously obtained. After initial tests complete successfully, scale up to larger or full-size data sets; if your jobs span a range of input file sizes, submit tests using the smallest and largest inputs to examine the range of resources that these jobs may need. Give your test jobs and associated HTCondor log , error , output , and submit files meaningful names so you know which results refer to which tests. Requesting CPUs, Memory, and Disk Space in the HTCondor Submit File \u00b6 In the HTCondor submit file, you must explicitly request the number of CPUs (i.e. cores), and the amount of disk and memory that the job needs to complete successfully, and you may need to identify a JobDurationCategory . When you submit a job for the first time, you may not know just how much to request and that's OK. Below are some suggestions for making resource requests for initial test jobs. For requesting CPU cores start by requesting a single cpu. With single-cpu jobs, you will see your jobs start sooner. Ultimately you will be able to achieve greater throughput with single cpus jobs compared to jobs that request and use multiple cpus. Keep in mind, requesting more CPU cores for a job does not mean that your jobs will use more cpus. Rather, you want to make sure that your CPU request matches the number of cores (i.e. 'threads' or 'processes') that you expect your software to use. (Most softwares only use 1 CPU core, by default.) There is limited support for multicore work in OSG. To learn more, see our guide on Multicore Jobs Depending on how long you expect your test jobs to take on a single core, you may need to identify a non-default JobDurationCategory , or consider implementing self-checkpointing (email us!). To inform initial disk requests always look at the size of your input files. At a minimum, you need to request enough disk to support all of the input files, executable, and the output you expect, but don't forget that the standard 'error' and 'output' files you specify will capture 'terminal' output that may add up, too. If many of your input and output files are compressed (i.e. zipped or tarballs) you will need to factor that into your estimates for disk usage as these files will take up additional space once uncompressed in the job. For your initial tests it is OK to request more disk than your job may need so that the test completes successfully. The key is to adjust disk requests for subsequent jobs based on the results of these test jobs. Estimating memory requests can sometimes be tricky. If you've performed the same or similar work on another computer, consider using the amount of memory (i.e. RAM) from that computer as a starting point. For instance, most laptop computers these days will have 8 or 16 GB of memory, which is okay to start with if you know a single job will succeed on your laptop. For your initial tests it is OK to request more memory than your job may need so that the test completes successfully. The key is to adjust memory requests for subsequent jobs based on the results of these test jobs. If you find that memory usage will vary greatly across a batch of jobs, we can assist you with creating dynamic memory requests in your submit files. Optimize Job Resource Requests For Subsequent Jobs \u00b6 As always, reviewing the HTCondor log file from past jobs is a great way to learn about the resource needs of your jobs. Optimizing the resources requested for each job may help your job run faster and achieve more throughput. Save the HTCondor log files from your jobs. HTCondor will report the memory, disk, and cpu usage of your jobs at the end of this file. The amount of each resource requested in the submit file is listed under the \"Request\" column and information about the amount of each resource actually utilized to complete the job is provided in the \"Usage\" column. For example: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 12 1000000 26703078 Memory (MB) : 0 1000 1000 One quick option to query your log files is to use the Unix tool grep . For example: [user@login]$ grep \"Disk (KB)\" my-job.log The above will return all lines in my-job.log that report the disk usage, request, and allocation of all jobs reported in that log file. Alternatively, condor_history can be used to query details from recently completed job submissions. HTCondor's history is continuously updating with information from new jobs, so condor_history is best performed shortly after the jobs of interest enter/leave the queue. Submit Multiple Jobs Using A Single Submit File \u00b6 Once you have a single test job that completes successfully, the next step is to submit a small batch of test jobs (e.g. 5 or 10 jobs) using a single submit file . Use this small-scale multi-job submission test to ensure that all jobs complete successfully, produce the desired output, and do not conflict with each other when submitted together. Once you are confident that the jobs will complete as desired, then scale up to submitting the entire set of jobs. Monitoring Job Status and Obtaining Run Information \u00b6 Gathering information about how, what, and where a job ran can be important for both troubleshooting and optimizing a workflow. The following commands are a great way to learn more about your jobs: Command Description condor_q Shows the queue information for your jobs. Includes information such as batch name and total jobs. condor_q <JobID> -l Prints all information related to a job including attributes and run information about a job in the queue. Output includes JobDurationCategory , ServerTime , SubmitFile , etc. Also works with condor_history . condor_q <JobID> -af <AttributeName1> <AttributeName2> Prints information about an attribute or list of attributes for a single job using the autoformat -af flag. The list of possible attributes can be found using condor_q <JobID> -l . Also works with condor_history . condor_q -constraint '<Attribute> == \"<value>\"' The -constraint flag allows users to find all jobs with a certain value for a given parameter. This flag supports searching by more than one parameter and different operators (e.g. =!= ). Also works with condor_history . condor_q -better-analyze <JobID> -pool <PoolName> Shows a list of the number of slots matching a job's requirements. For more information, see Troubleshooting Job Errors . Additional condor_q flags involved in optimizing and troubleshooting jobs include: Flag Description -nobatch Combined with condor_q , this flag will list jobs individually and not by batch. -hold Show only jobs in the \"on hold\" state and the reason for that. An action from the user is expected to solve the problem. -run Show your running jobs and related info, like how much time they have been running, where they are running, etc. -dag Organize condor_q output by DAG. More information about the commands and flags above can be found in the HTCondor manual . Avoid Exceeding Disk Quotas in /home and /public \u00b6 Each OSG Connect user is granted 50 GB of storage in their /home directory and 500 GB of storage in their /public directory. This may seem like a lot, but when running 100's or 1,000's of jobs, even small output can add up quickly. If these quotas are exceeded, jobs will fail or go on hold when attempting returning output. To prevent errors or workflow interruption, be sure to estimate the input and output needed for all of your concurrently running jobs. By default, after your job terminates HTCondor will transfer back any new or modified files from the top-level directory where the job ran, back to your /home directory. Efficiently manage output by including steps to remove intermediate and/or unnecessary files as part of your job. Workflow Management \u00b6 To help manage complicated workflows, consider a workflow manager such as HTCondor's built-in DAGman or the HTCondor-compatible Pegasus workflow tool. Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org .","title":"Determining the Amount of Resources to Request in a Submit File "},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#determining-the-amount-of-resources-to-request-in-a-submit-file","text":"","title":"Determining the Amount of Resources to Request in a Submit File"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#learning-objectives","text":"This guide discuses the following: - Best practices for testing jobs and scaling up your analysis. - How to determine the amount of resources (CPU, memory, disk space) to request in a submit file.","title":"Learning Objectives"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#overview","text":"Much of HTCondor's power comes from the ability to run a large number of jobs simultaneously. To optimize your work with a high-throughput computing (HTC) approach, you will need to test and optimize the resource requests of those jobs to only request the amount of memory, disk, and cpus truly needed. This is an important practice that will maximize your throughput by optimizing the number of potential 'slots' in the OSPool that your jobs can match to, reducing the overall turnaround time for completing a whole batch. If you have questions or are unsure if and how your work can be broken up, please contact us at support@opensciencegrid.org This guide will describe best practices and general tips for testing your job resource requests before scaling up to submit your full set of jobs. Additional information is also available from the following \"Introduction to High Throughput Computing with HTCondor\" 2020 OSG Virtual Pilot School lecture video:","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#always-start-with-test-jobs","text":"Submitting test jobs is an important first step for optimizing the resource requests of your jobs. We always recommend submitting a few (3-10) test jobs first before scaling up, whether this is your first time using OSG or you're an experienced user starting a new workflow. If you plan to submit thousands of jobs, you may even want to run an intermediate test of 100-1,000 jobs to catch any failures or holds that mean your jobs have additional requirements they need to specify (and which OSG staff can help you to identify, based upon your tests). Some general tips for test jobs: Select smaller data sets or subsets of data for your first test jobs. Using smaller data will keep the resource needs of your jobs low which will help get test jobs to start and complete sooner, when you're just making sure that your submit file and other logistical aspects of jobs submission are as you want them. If possible, submit test jobs that will reproduce results you've gotten using another system. This approach can be used as a good \"sanity check\" as you'll be able to compare the results of the test to those previously obtained. After initial tests complete successfully, scale up to larger or full-size data sets; if your jobs span a range of input file sizes, submit tests using the smallest and largest inputs to examine the range of resources that these jobs may need. Give your test jobs and associated HTCondor log , error , output , and submit files meaningful names so you know which results refer to which tests.","title":"Always Start With Test Jobs"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#requesting-cpus-memory-and-disk-space-in-the-htcondor-submit-file","text":"In the HTCondor submit file, you must explicitly request the number of CPUs (i.e. cores), and the amount of disk and memory that the job needs to complete successfully, and you may need to identify a JobDurationCategory . When you submit a job for the first time, you may not know just how much to request and that's OK. Below are some suggestions for making resource requests for initial test jobs. For requesting CPU cores start by requesting a single cpu. With single-cpu jobs, you will see your jobs start sooner. Ultimately you will be able to achieve greater throughput with single cpus jobs compared to jobs that request and use multiple cpus. Keep in mind, requesting more CPU cores for a job does not mean that your jobs will use more cpus. Rather, you want to make sure that your CPU request matches the number of cores (i.e. 'threads' or 'processes') that you expect your software to use. (Most softwares only use 1 CPU core, by default.) There is limited support for multicore work in OSG. To learn more, see our guide on Multicore Jobs Depending on how long you expect your test jobs to take on a single core, you may need to identify a non-default JobDurationCategory , or consider implementing self-checkpointing (email us!). To inform initial disk requests always look at the size of your input files. At a minimum, you need to request enough disk to support all of the input files, executable, and the output you expect, but don't forget that the standard 'error' and 'output' files you specify will capture 'terminal' output that may add up, too. If many of your input and output files are compressed (i.e. zipped or tarballs) you will need to factor that into your estimates for disk usage as these files will take up additional space once uncompressed in the job. For your initial tests it is OK to request more disk than your job may need so that the test completes successfully. The key is to adjust disk requests for subsequent jobs based on the results of these test jobs. Estimating memory requests can sometimes be tricky. If you've performed the same or similar work on another computer, consider using the amount of memory (i.e. RAM) from that computer as a starting point. For instance, most laptop computers these days will have 8 or 16 GB of memory, which is okay to start with if you know a single job will succeed on your laptop. For your initial tests it is OK to request more memory than your job may need so that the test completes successfully. The key is to adjust memory requests for subsequent jobs based on the results of these test jobs. If you find that memory usage will vary greatly across a batch of jobs, we can assist you with creating dynamic memory requests in your submit files.","title":"Requesting CPUs, Memory, and Disk Space in the HTCondor Submit File"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#optimize-job-resource-requests-for-subsequent-jobs","text":"As always, reviewing the HTCondor log file from past jobs is a great way to learn about the resource needs of your jobs. Optimizing the resources requested for each job may help your job run faster and achieve more throughput. Save the HTCondor log files from your jobs. HTCondor will report the memory, disk, and cpu usage of your jobs at the end of this file. The amount of each resource requested in the submit file is listed under the \"Request\" column and information about the amount of each resource actually utilized to complete the job is provided in the \"Usage\" column. For example: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 12 1000000 26703078 Memory (MB) : 0 1000 1000 One quick option to query your log files is to use the Unix tool grep . For example: [user@login]$ grep \"Disk (KB)\" my-job.log The above will return all lines in my-job.log that report the disk usage, request, and allocation of all jobs reported in that log file. Alternatively, condor_history can be used to query details from recently completed job submissions. HTCondor's history is continuously updating with information from new jobs, so condor_history is best performed shortly after the jobs of interest enter/leave the queue.","title":"Optimize Job Resource Requests For Subsequent Jobs"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#submit-multiple-jobs-using-a-single-submit-file","text":"Once you have a single test job that completes successfully, the next step is to submit a small batch of test jobs (e.g. 5 or 10 jobs) using a single submit file . Use this small-scale multi-job submission test to ensure that all jobs complete successfully, produce the desired output, and do not conflict with each other when submitted together. Once you are confident that the jobs will complete as desired, then scale up to submitting the entire set of jobs.","title":"Submit Multiple Jobs Using A Single Submit File"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#monitoring-job-status-and-obtaining-run-information","text":"Gathering information about how, what, and where a job ran can be important for both troubleshooting and optimizing a workflow. The following commands are a great way to learn more about your jobs: Command Description condor_q Shows the queue information for your jobs. Includes information such as batch name and total jobs. condor_q <JobID> -l Prints all information related to a job including attributes and run information about a job in the queue. Output includes JobDurationCategory , ServerTime , SubmitFile , etc. Also works with condor_history . condor_q <JobID> -af <AttributeName1> <AttributeName2> Prints information about an attribute or list of attributes for a single job using the autoformat -af flag. The list of possible attributes can be found using condor_q <JobID> -l . Also works with condor_history . condor_q -constraint '<Attribute> == \"<value>\"' The -constraint flag allows users to find all jobs with a certain value for a given parameter. This flag supports searching by more than one parameter and different operators (e.g. =!= ). Also works with condor_history . condor_q -better-analyze <JobID> -pool <PoolName> Shows a list of the number of slots matching a job's requirements. For more information, see Troubleshooting Job Errors . Additional condor_q flags involved in optimizing and troubleshooting jobs include: Flag Description -nobatch Combined with condor_q , this flag will list jobs individually and not by batch. -hold Show only jobs in the \"on hold\" state and the reason for that. An action from the user is expected to solve the problem. -run Show your running jobs and related info, like how much time they have been running, where they are running, etc. -dag Organize condor_q output by DAG. More information about the commands and flags above can be found in the HTCondor manual .","title":"Monitoring Job Status and Obtaining Run Information"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#avoid-exceeding-disk-quotas-in-home-and-public","text":"Each OSG Connect user is granted 50 GB of storage in their /home directory and 500 GB of storage in their /public directory. This may seem like a lot, but when running 100's or 1,000's of jobs, even small output can add up quickly. If these quotas are exceeded, jobs will fail or go on hold when attempting returning output. To prevent errors or workflow interruption, be sure to estimate the input and output needed for all of your concurrently running jobs. By default, after your job terminates HTCondor will transfer back any new or modified files from the top-level directory where the job ran, back to your /home directory. Efficiently manage output by including steps to remove intermediate and/or unnecessary files as part of your job.","title":"Avoid Exceeding Disk Quotas in /home and /public"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#workflow-management","text":"To help manage complicated workflows, consider a workflow manager such as HTCondor's built-in DAGman or the HTCondor-compatible Pegasus workflow tool.","title":"Workflow Management"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/preparing-to-scale-up/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/","text":"Roadmap to HTC Workload Submission via OSG Connect \u00b6 Overview \u00b6 This guide lays out the steps needed to go from logging in to an OSG Connect login node to running a full scale high throughput computing (HTC) workload on OSG's Open Science Pool (OSPool) . The steps listed here apply to any new workload submission, whether you are a long-time OSG user or just getting started with your first workload, with helpful links to our documentation pages. This guide assumes that you have applied for an account on the OSG Connect service and have been approved after meeting with an OSG Research Computing Facilitator. If you don't yet have an account, you can apply for one at or contact us with any questions you have. Learning how to get started on the OSG does not need to end with this document or our guides! Learn about our training opportunities and personal facilitation support in the Getting Help section below. 1. Introduction to the OSPool and OSG Connect \u00b6 The OSG's Open Science Pool is best-suited for computing work that can be run as many, independent tasks, in an approach called \"high throughput computing.\" For more information on what kind of work is a good fit for the OSG, see Is the Open Science Pool for You? . Learn more about the services provided by the OSG that can support your HTC workload: 2. Get on OSG Connect \u00b6 After your OSG account has been approved, go through the following guides to complete your access to the login node and to enable your account to submit jobs. Generate ssh keys and login Set your default project 3. Learn to Submit HTCondor Jobs \u00b6 Computational work is run on the OSPool by submitting it as \u201cjobs\u201d to the HTCondor scheduler. Jobs submitted to HTCondor are then scheduled and run on different resources that are part of the Open Science Pool. Before submitting your own computational work, it is important to understand how HTCondor job submission works. The following guides show how to submit basic HTCondor jobs. The second example allows you to see where in the OSPool your jobs run. OSG Connect Quickstart Finding OSG Locations 4. Test a First Job \u00b6 After learning about the basics of HTCondor job submission, you will need to generate your own HTCondor job -- including the software needed by the job and the appropriate mechanism to handle the data. We recommend doing this using a single test job. Prepare your software \u00b6 Software is an integral part of your HTC workflow. Whether you\u2019ve written it yourself, inherited it from your research group, or use common open-source packages, any required executables and libraries will need to be made available to your jobs if they are to run on the OSPool. Read through this overview of Using Software in OSG Connect to help you determine the best way to provide your software. We also have the following guides/tutorials for each major software portability approach: To install your own software , begin with the guide on Compiling Software for OSG Connect and then complete the Example Software Compilation tutorial . To use precompiled binaries , try the example presented in the AutoDock Vina tutorial and/or the Julia tutorial . To use Docker containers for your jobs, start with the Docker and Singularity Containers guide , and (optionally) work through the Tensorflow tutorial (which uses Docker/Singularity) To use Distributed Environment Modules for your jobs, start with this Modules guide and then complete the Module example in this R tutorial Finally, here are some additional guides specific to some of the most common scripting languages and software tools used on OSG**: Python R Machine Learning BLAST **This is not a complete list. Feel free to search for your software in our Knowledge base . Manage your data \u00b6 The data for your jobs will need to be transferred to each job that runs in the OSPool, and HTCondor has built-in features for getting data to jobs. Our Data Management Policies guide discussed the relevant approaches, when to use them, and where to stage data for each. Assign the Appropriate Job Duration Category \u00b6 Jobs running in the OSPool may be interrupted at any time, and will be re-run by HTCondor, unless a single execution of a job exceeds the allowed duration. Jobs expected to take longer than 10 hours will need to identify themselves as 'Long' according to our Job Duration policies . Remember that jobs expected to take longer than 20 hours are not a good fit for the OSPool (see Is the Open Science Pool for You? ) without implementing self-checkpointing (further below). 5. Scale Up \u00b6 After you have a sample job running successfully, you\u2019ll want to scale up in one or two steps (first run several jobs, before running ALL of them). HTCondor has many useful features that make it easy to submit multiple jobs with the same submit file. Easily submit multiple jobs Scaling up after success with test jobs discusses how to test your jobs for duration, memory and disk usage, and the total amount of space you might need on the 6. Special Use Cases \u00b6 If you think any of the below applies to you, please get in touch and our facilitation team will be happy to discuss your individual case. Run sequential workflows of jobs: Workflows with HTCondor's DAGMan Implement self-checkpointing for long jobs: HTCondor Checkpointing Guide Build your own Docker container: Creating a Docker Container Image Submit more than 10,000 jobs at once: FAQ, search for 'max_idle' Larger or speciality resource requests: GPUs: GPU Jobs Multiple CPUs: Multicore Jobs Large Memory: Large Memory Jobs Getting Help \u00b6 The OSG Facilitation team is here to help with questions and issues that come up as you work through these roadmap steps. We are available via email, office hours, appointments, and offer regular training opportunities. See our Get Help page and OSG Training page for all the different ways you can reach us. Our purpose is to assist you with achieving your computational goals, so we want to hear from you!","title":"Roadmap to HTC Workload Submission via OSG Connect "},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#roadmap-to-htc-workload-submission-via-osg-connect","text":"","title":"Roadmap to HTC Workload Submission via OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#overview","text":"This guide lays out the steps needed to go from logging in to an OSG Connect login node to running a full scale high throughput computing (HTC) workload on OSG's Open Science Pool (OSPool) . The steps listed here apply to any new workload submission, whether you are a long-time OSG user or just getting started with your first workload, with helpful links to our documentation pages. This guide assumes that you have applied for an account on the OSG Connect service and have been approved after meeting with an OSG Research Computing Facilitator. If you don't yet have an account, you can apply for one at or contact us with any questions you have. Learning how to get started on the OSG does not need to end with this document or our guides! Learn about our training opportunities and personal facilitation support in the Getting Help section below.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#1-introduction-to-the-ospool-and-osg-connect","text":"The OSG's Open Science Pool is best-suited for computing work that can be run as many, independent tasks, in an approach called \"high throughput computing.\" For more information on what kind of work is a good fit for the OSG, see Is the Open Science Pool for You? . Learn more about the services provided by the OSG that can support your HTC workload:","title":"1. Introduction to the OSPool and OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#2-get-on-osg-connect","text":"After your OSG account has been approved, go through the following guides to complete your access to the login node and to enable your account to submit jobs. Generate ssh keys and login Set your default project","title":"2. Get on OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#3-learn-to-submit-htcondor-jobs","text":"Computational work is run on the OSPool by submitting it as \u201cjobs\u201d to the HTCondor scheduler. Jobs submitted to HTCondor are then scheduled and run on different resources that are part of the Open Science Pool. Before submitting your own computational work, it is important to understand how HTCondor job submission works. The following guides show how to submit basic HTCondor jobs. The second example allows you to see where in the OSPool your jobs run. OSG Connect Quickstart Finding OSG Locations","title":"3. Learn to Submit HTCondor Jobs"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#4-test-a-first-job","text":"After learning about the basics of HTCondor job submission, you will need to generate your own HTCondor job -- including the software needed by the job and the appropriate mechanism to handle the data. We recommend doing this using a single test job.","title":"4. Test a First Job"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#prepare-your-software","text":"Software is an integral part of your HTC workflow. Whether you\u2019ve written it yourself, inherited it from your research group, or use common open-source packages, any required executables and libraries will need to be made available to your jobs if they are to run on the OSPool. Read through this overview of Using Software in OSG Connect to help you determine the best way to provide your software. We also have the following guides/tutorials for each major software portability approach: To install your own software , begin with the guide on Compiling Software for OSG Connect and then complete the Example Software Compilation tutorial . To use precompiled binaries , try the example presented in the AutoDock Vina tutorial and/or the Julia tutorial . To use Docker containers for your jobs, start with the Docker and Singularity Containers guide , and (optionally) work through the Tensorflow tutorial (which uses Docker/Singularity) To use Distributed Environment Modules for your jobs, start with this Modules guide and then complete the Module example in this R tutorial Finally, here are some additional guides specific to some of the most common scripting languages and software tools used on OSG**: Python R Machine Learning BLAST **This is not a complete list. Feel free to search for your software in our Knowledge base .","title":"Prepare your software"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#manage-your-data","text":"The data for your jobs will need to be transferred to each job that runs in the OSPool, and HTCondor has built-in features for getting data to jobs. Our Data Management Policies guide discussed the relevant approaches, when to use them, and where to stage data for each.","title":"Manage your data"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#assign-the-appropriate-job-duration-category","text":"Jobs running in the OSPool may be interrupted at any time, and will be re-run by HTCondor, unless a single execution of a job exceeds the allowed duration. Jobs expected to take longer than 10 hours will need to identify themselves as 'Long' according to our Job Duration policies . Remember that jobs expected to take longer than 20 hours are not a good fit for the OSPool (see Is the Open Science Pool for You? ) without implementing self-checkpointing (further below).","title":"Assign the Appropriate Job Duration Category"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#5-scale-up","text":"After you have a sample job running successfully, you\u2019ll want to scale up in one or two steps (first run several jobs, before running ALL of them). HTCondor has many useful features that make it easy to submit multiple jobs with the same submit file. Easily submit multiple jobs Scaling up after success with test jobs discusses how to test your jobs for duration, memory and disk usage, and the total amount of space you might need on the","title":"5. Scale Up"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#6-special-use-cases","text":"If you think any of the below applies to you, please get in touch and our facilitation team will be happy to discuss your individual case. Run sequential workflows of jobs: Workflows with HTCondor's DAGMan Implement self-checkpointing for long jobs: HTCondor Checkpointing Guide Build your own Docker container: Creating a Docker Container Image Submit more than 10,000 jobs at once: FAQ, search for 'max_idle' Larger or speciality resource requests: GPUs: GPU Jobs Multiple CPUs: Multicore Jobs Large Memory: Large Memory Jobs","title":"6. Special Use Cases"},{"location":"managing_htc_workloads_on_osg_connect/htc_workload_planning_testing_scaling_up/roadmap/#getting-help","text":"The OSG Facilitation team is here to help with questions and issues that come up as you work through these roadmap steps. We are available via email, office hours, appointments, and offer regular training opportunities. See our Get Help page and OSG Training page for all the different ways you can reach us. Our purpose is to assist you with achieving your computational goals, so we want to hear from you!","title":"Getting Help"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-htcondor/","text":"Transfer Input Files Up To 100MB In Size \u00b6 Overview \u00b6 Due to the distributed configuration of the OSG, more often than not, your jobs will need to bring along a copy (i.e. transfer a copy) of data, code, packages, software, etc. from the login node where the job is submitted to the execute node where the job will run. This requirement applies to any and all files that are needed to successfully execute and complete your job that do not otherwise exist on OSG execute servers. This guide will describe steps and important considerations for transferring input files that are <100MB in size via the HTCondor submit file. Important Considerations \u00b6 As described in the Introduction to Data Management on OSG Connect any data, files, or even software that is <100MB should be staged in your /home directory on your login node. Files in your /home directory can be transferred to jobs via your HTCondor submit file. Transfer Files From /home Using HTCondor \u00b6 To transfer files from your /home directory use the transfer_input_files statement in your HTCondor submit file. For example: # submit file example log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out # transfer small file from /home transfer_input_files = my_data.csv ...other submit file details... Multiple files can be specified using a comma-separated list, for example: # transfer multiple files from /home transfer_input_files = my_data.csv, my_software.tar.gz, my_script.py When using transfer_input_files to transfer files located in /home , keep in mind that the path to the file is relative to the location of the submit file. If you have files located in a different /home subdirectory, we recommend specifying the full path to those files, which is also a matter of good practice, for example: transfer_input_files = /home/username/path/to/my_software.tar.gz Where username refers to your OSG Connect username. Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Transfer Input Files Up To 100MB In Size"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-htcondor/#transfer-input-files-up-to-100mb-in-size","text":"","title":"Transfer Input Files Up To 100MB In Size"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-htcondor/#overview","text":"Due to the distributed configuration of the OSG, more often than not, your jobs will need to bring along a copy (i.e. transfer a copy) of data, code, packages, software, etc. from the login node where the job is submitted to the execute node where the job will run. This requirement applies to any and all files that are needed to successfully execute and complete your job that do not otherwise exist on OSG execute servers. This guide will describe steps and important considerations for transferring input files that are <100MB in size via the HTCondor submit file.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-htcondor/#important-considerations","text":"As described in the Introduction to Data Management on OSG Connect any data, files, or even software that is <100MB should be staged in your /home directory on your login node. Files in your /home directory can be transferred to jobs via your HTCondor submit file.","title":"Important Considerations"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-htcondor/#transfer-files-from-home-using-htcondor","text":"To transfer files from your /home directory use the transfer_input_files statement in your HTCondor submit file. For example: # submit file example log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out # transfer small file from /home transfer_input_files = my_data.csv ...other submit file details... Multiple files can be specified using a comma-separated list, for example: # transfer multiple files from /home transfer_input_files = my_data.csv, my_software.tar.gz, my_script.py When using transfer_input_files to transfer files located in /home , keep in mind that the path to the file is relative to the location of the submit file. If you have files located in a different /home subdirectory, we recommend specifying the full path to those files, which is also a matter of good practice, for example: transfer_input_files = /home/username/path/to/my_software.tar.gz Where username refers to your OSG Connect username.","title":"Transfer Files From /home Using HTCondor"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-htcondor/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-http/","text":"Transfer HTTP-available Files up to 1GB In Size \u00b6 Overview \u00b6 If some of the data or software your jobs depend on is available via the web, you can have such files transferred by HTCondor using the appropriate HTTP address! Important Considerations \u00b6 While our Overview of Data Mangement on OSG Connect describes how you can stage data, files, or even software in OSG Connect locations, any web-accessible file can be transferred directly to your jobs IF : the file is accessible via an HTTP address the file is less than 1GB in size (if larger, you'll need to pre-stage them for stash-based transfer the server or website they're on can handle large numbers of your jobs accessing them simultaneously Importantly, you'll also want to make sure your job executable knows how to handle the file (un-tar, etc.) from within the working directory of the job, just like it would for any other input file. Transfer Files via HTTP \u00b6 Use an HTTP URL in combination with the transfer_input_files statement in your HTCondor submit file. For example: # submit file example log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out # transfer software tarball from public via http transfer_input_files = http://www.website.com/path/file.tar.gz ...other submit file details... Multiple URLs can be specified using a comma-separated list, and a combination of URLs and files from /home directory can be provided in a comma separated list. For example, # transfer software tarball from public via http # transfer input data from home via htcondor file transfer transfer_input_files = http://www.website.com/path/file1.tar.gz, http://www.website.com/path/file2.tar.gz, my_data.csv Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Transfer HTTP-available Files up to 1GB In Size"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-http/#transfer-http-available-files-up-to-1gb-in-size","text":"","title":"Transfer HTTP-available Files up to 1GB In Size"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-http/#overview","text":"If some of the data or software your jobs depend on is available via the web, you can have such files transferred by HTCondor using the appropriate HTTP address!","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-http/#important-considerations","text":"While our Overview of Data Mangement on OSG Connect describes how you can stage data, files, or even software in OSG Connect locations, any web-accessible file can be transferred directly to your jobs IF : the file is accessible via an HTTP address the file is less than 1GB in size (if larger, you'll need to pre-stage them for stash-based transfer the server or website they're on can handle large numbers of your jobs accessing them simultaneously Importantly, you'll also want to make sure your job executable knows how to handle the file (un-tar, etc.) from within the working directory of the job, just like it would for any other input file.","title":"Important Considerations"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-http/#transfer-files-via-http","text":"Use an HTTP URL in combination with the transfer_input_files statement in your HTCondor submit file. For example: # submit file example log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out # transfer software tarball from public via http transfer_input_files = http://www.website.com/path/file.tar.gz ...other submit file details... Multiple URLs can be specified using a comma-separated list, and a combination of URLs and files from /home directory can be provided in a comma separated list. For example, # transfer software tarball from public via http # transfer input data from home via htcondor file transfer transfer_input_files = http://www.website.com/path/file1.tar.gz, http://www.website.com/path/file2.tar.gz, my_data.csv","title":"Transfer Files via HTTP"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/file-transfer-via-http/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/","text":"==================================== Overview \u00b6 OSG Connect provides two locations for uploading files (data and software) that are needed for running jobs: home: /home/<username> (general storage from which ALL jobs should be submitted) public: /public/<username> (only for large job input and output using stash links (input) and stashcp (output)) In general, OSG Connect users are responsible for managing data in these folders and for using appropriate mechanisms for delivering data to/from jobs, as detailed below. Each is controlled with a quota and should be treated as temporary storage for active job execution. OSG Connect has no routine backup of data in these locations, and users should remove old data after jobs complete, in part, to make room for future submissions. If you think you'll need more space for a set of concurrently-queued jobs, even after cleaning up old data, please send a request to support@opensciencegrid.org ! For additional data information, see also the \"Data Storage and Transfer\" section of our FAQ . Note: OSG Connect staff reserve the right to monitor and/or remove data without notice to the user IF doing so is necessary for ensuring proper use or to quickly fix a performance or security issue. Data Locations and Quotas \u00b6 Your OSG Connect account includes access to two data storage locations: /home and /public . Where you store your files and how your files are made accessible to your jobs depends on the size of the file and how much data is needed or produced by your jobs. Location Storage Needs Network mounted Backed Up? Initial Quota /home Storage of submit files, input files <100MB each, and per-job output up to a 1GB. Jobs should ONLY be submitted from this folder. No No 50 GB /public Staging ONLY for large input files (100MB-50GB, each) for publicly-accessible download into jobs (using stash:/// links or stashcp see below) and large output files (1-10GB). Yes No 500 GB Your quota status will be displayed when you connect to your OSG Connect login node: Disk utilization for username: /public : [ ] 0% (0/500000 MB) /home : [ # ] 4% (2147/53687 MB) You can also display your quota usage at any time using the command quota while connected to your login node. Don't hesitate to contact us at support@opensciencegrid.org if you think you need a quota increase! We can support very large amounts of data. /home Usage And Policies \u00b6 User directories within /home are meant for general-use storage of your files needed for job submission. The initial quota per user is 50 GBs, and can be increased by request to support@opensciencegrid.org when a user needs more space for appropriately-sized files. ALL JOBS MUST BE SUBMITTED FROM WITHIN /home . Users are also prohibited from making their /home directory world-readable due to security concerns. See Policies for Using OSG via OSG Connect Submit Servers for more details. If you're unable to submit jobs or your jobs are going on hold because you've reached your /home quota, please contact us at support@opensciencegrid.org about a quota increase. /public Usage and Policies \u00b6 User directories within /public are meant ONLY for staging job files too large for regular HTCondor file transfer (per-job input greater than 100MB, or per-job output greater than 1GB), and should only OSG caching mechanisms (see tables for input and output, further below). JOBS MUST NEVER BE SUBMITTED FROM WITHIN /public , and should not list /public files in the transfer_input_files or other lines of a submit file, unless as a stash:/// address (see tables further below). Files placed in /public should only be accessed by jobs using the below tools (see Transferring Data To/From Jobs ). Users violating these policies may lose the ability to submit jobs until their submissions are corrected. The initial disk quota of /public is 500 GBs. Contact support@opensciencegrid.org if you will need an increase for concurrently running work, after cleaning up all data from past jobs. Given that users should not be storing long-term data (like submit files, software, etc.) in /public , files and directories that have not been accessed for over six months may be deleted by OSG Connect staff with or without notifying the user. Files placed within a user's /public directory are publicly accessible , discoverable and readable by anyone. Data is made public via the stash transfer mechanisms (which also make data public via http/https), and mirrored to a shared data repository which is available on a large number of systems around the world. Is there any support for private data? \u00b6 If you do not want your data to be downloadable by anyone, and it's small enough for HTCondor file transfer, then it should be staged in your /home directory and transferred to jobs with HTCondor file transfer ( transfer_input_files in the submit file). If it cannot be public (cannot use http or stash for job delivery), and is too large for HTCondor file transfer, then it's not a good fit for the open environment of the Open Science Pool, and another resource will likely be more appropriate. As a reminder, if the data is not being used for active computing work on OSG Connect, it should not be stored on OSG Connect systems, and our data storage locations are not backed up or suitable for project-duration storage. External Data Transfer to/from OSG Connect Login Nodes \u00b6 In general, common Unix tools such as rsync , scp , Putty, WinSCP, gFTP , etc. can be used to upload data from your computer to the OSG Connect login node, or to download files from the OSG Connect login node. Transferring Data To/From Jobs \u00b6 Transferring Input Data to Jobs \u00b6 This table summarizes the options for sending input files from the OSG Connect login node to the execution node where a job is running. This assumes that you have already uploaded these input files from your own computer to your OSG Connect login node. Transfer Method File Sizes File Location Command More Info regular HTCondor file transfer <100 MB; <500 MB total per job /home transfer_input_files HTCondor File Transfer OSG's transfer tools >1GB; <10 GB per job /public stash address in transfer_input_files Stash Transfer HTTP <1GB non-OSG web server http address in transfer_input_files HTTP Access GridFTP > 10 GB /public or non-OSG data server gfal-copy contact us Transferring Output Data from Jobs \u00b6 This table summarizes a job's options for returning output files generated by the job back to the OSG Connect login node. Transfer Method File Sizes Transfer To Command More Info regular HTCondor file transfer < 1 GB /home HTCondor default output transfer or transfer_output_files HTCondor Transfer OSG's transfer tools >1GB and < 10 GB /public stashcp in job executable stashcp GridFTP > 10 GB /public gfal-copy contact us Watch this video from the 2021 OSG Virtual School for more information about Handling Data on OSG: Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Overview: Data Staging and Transfer to Jobs"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#overview","text":"OSG Connect provides two locations for uploading files (data and software) that are needed for running jobs: home: /home/<username> (general storage from which ALL jobs should be submitted) public: /public/<username> (only for large job input and output using stash links (input) and stashcp (output)) In general, OSG Connect users are responsible for managing data in these folders and for using appropriate mechanisms for delivering data to/from jobs, as detailed below. Each is controlled with a quota and should be treated as temporary storage for active job execution. OSG Connect has no routine backup of data in these locations, and users should remove old data after jobs complete, in part, to make room for future submissions. If you think you'll need more space for a set of concurrently-queued jobs, even after cleaning up old data, please send a request to support@opensciencegrid.org ! For additional data information, see also the \"Data Storage and Transfer\" section of our FAQ . Note: OSG Connect staff reserve the right to monitor and/or remove data without notice to the user IF doing so is necessary for ensuring proper use or to quickly fix a performance or security issue.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#data-locations-and-quotas","text":"Your OSG Connect account includes access to two data storage locations: /home and /public . Where you store your files and how your files are made accessible to your jobs depends on the size of the file and how much data is needed or produced by your jobs. Location Storage Needs Network mounted Backed Up? Initial Quota /home Storage of submit files, input files <100MB each, and per-job output up to a 1GB. Jobs should ONLY be submitted from this folder. No No 50 GB /public Staging ONLY for large input files (100MB-50GB, each) for publicly-accessible download into jobs (using stash:/// links or stashcp see below) and large output files (1-10GB). Yes No 500 GB Your quota status will be displayed when you connect to your OSG Connect login node: Disk utilization for username: /public : [ ] 0% (0/500000 MB) /home : [ # ] 4% (2147/53687 MB) You can also display your quota usage at any time using the command quota while connected to your login node. Don't hesitate to contact us at support@opensciencegrid.org if you think you need a quota increase! We can support very large amounts of data.","title":"Data Locations and Quotas"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#home-usage-and-policies","text":"User directories within /home are meant for general-use storage of your files needed for job submission. The initial quota per user is 50 GBs, and can be increased by request to support@opensciencegrid.org when a user needs more space for appropriately-sized files. ALL JOBS MUST BE SUBMITTED FROM WITHIN /home . Users are also prohibited from making their /home directory world-readable due to security concerns. See Policies for Using OSG via OSG Connect Submit Servers for more details. If you're unable to submit jobs or your jobs are going on hold because you've reached your /home quota, please contact us at support@opensciencegrid.org about a quota increase.","title":"/home Usage And Policies"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#public-usage-and-policies","text":"User directories within /public are meant ONLY for staging job files too large for regular HTCondor file transfer (per-job input greater than 100MB, or per-job output greater than 1GB), and should only OSG caching mechanisms (see tables for input and output, further below). JOBS MUST NEVER BE SUBMITTED FROM WITHIN /public , and should not list /public files in the transfer_input_files or other lines of a submit file, unless as a stash:/// address (see tables further below). Files placed in /public should only be accessed by jobs using the below tools (see Transferring Data To/From Jobs ). Users violating these policies may lose the ability to submit jobs until their submissions are corrected. The initial disk quota of /public is 500 GBs. Contact support@opensciencegrid.org if you will need an increase for concurrently running work, after cleaning up all data from past jobs. Given that users should not be storing long-term data (like submit files, software, etc.) in /public , files and directories that have not been accessed for over six months may be deleted by OSG Connect staff with or without notifying the user. Files placed within a user's /public directory are publicly accessible , discoverable and readable by anyone. Data is made public via the stash transfer mechanisms (which also make data public via http/https), and mirrored to a shared data repository which is available on a large number of systems around the world.","title":"/public Usage and Policies"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#is-there-any-support-for-private-data","text":"If you do not want your data to be downloadable by anyone, and it's small enough for HTCondor file transfer, then it should be staged in your /home directory and transferred to jobs with HTCondor file transfer ( transfer_input_files in the submit file). If it cannot be public (cannot use http or stash for job delivery), and is too large for HTCondor file transfer, then it's not a good fit for the open environment of the Open Science Pool, and another resource will likely be more appropriate. As a reminder, if the data is not being used for active computing work on OSG Connect, it should not be stored on OSG Connect systems, and our data storage locations are not backed up or suitable for project-duration storage.","title":"Is there any support for private data?"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#external-data-transfer-tofrom-osg-connect-login-nodes","text":"In general, common Unix tools such as rsync , scp , Putty, WinSCP, gFTP , etc. can be used to upload data from your computer to the OSG Connect login node, or to download files from the OSG Connect login node.","title":"External Data Transfer to/from OSG Connect Login Nodes"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#transferring-data-tofrom-jobs","text":"","title":"Transferring Data To/From Jobs"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#transferring-input-data-to-jobs","text":"This table summarizes the options for sending input files from the OSG Connect login node to the execution node where a job is running. This assumes that you have already uploaded these input files from your own computer to your OSG Connect login node. Transfer Method File Sizes File Location Command More Info regular HTCondor file transfer <100 MB; <500 MB total per job /home transfer_input_files HTCondor File Transfer OSG's transfer tools >1GB; <10 GB per job /public stash address in transfer_input_files Stash Transfer HTTP <1GB non-OSG web server http address in transfer_input_files HTTP Access GridFTP > 10 GB /public or non-OSG data server gfal-copy contact us","title":"Transferring Input Data to Jobs"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#transferring-output-data-from-jobs","text":"This table summarizes a job's options for returning output files generated by the job back to the OSG Connect login node. Transfer Method File Sizes Transfer To Command More Info regular HTCondor file transfer < 1 GB /home HTCondor default output transfer or transfer_output_files HTCondor Transfer OSG's transfer tools >1GB and < 10 GB /public stashcp in job executable stashcp GridFTP > 10 GB /public gfal-copy contact us Watch this video from the 2021 OSG Virtual School for more information about Handling Data on OSG:","title":"Transferring Output Data from Jobs"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/","text":"Transfer Job Output Files Up To 1GB In Size \u00b6 Overview \u00b6 When your OSG Connect jobs run, any output that gets generated is specifically written to the execute node on which the job ran. In order to get access to your output files, a copy of the output must be transferred back to your OSG Connect login node. This guide will describe the necessary steps, along with important considerations, for transferring your output files back to your /home directory on your OSG Connect login node. Important Considerations \u00b6 As described in the Introduction to Data Management on OSG Connect , any output <1GB should be staged in your /home directory. For output files >1GB, please refer to our Transfer Large Input and Output Files >1GB In Size guide. If your jobs use any input files >1GB that are transferred from your /public directory using StacheCash, it is important that these files get deleted from the job's working directory or moved to a subdirectory so that HTCondor will not transfer these large files back to your /home directory. Use HTCondor To Transfer Output <1GB \u00b6 By default, HTCondor will transfer any new or modified files in the job's top-level directory back to your /home directory location from which the condor_submit command was performed. This behavior only applies to files in the top-level directory of where your job executes, meaning HTCondor will ignore any files created in subdirectories of the job's top-level directory. Several options exist for modifying this default output file transfer behavior, including those described in this guide. To learn more, please contact us at support@opensciencegrid.org . What is the top-level directory of a job? Before executing a job, HTCondor will create a new directory on the execute node just for your job - this is the top-level directory of the job and the path is stored in the environment variable _CONDOR_SCRATCH_DIR . All of the input files transferred via transfer_input_files will first be written to this directory and it is from this path that a job starts to execute. After a job has completed the top-level directory and all of it's contents are deleted. What if my output file(s) are not written to the top-level directory? If your output files are written to a subdirectory, use the steps described below to convert the output directory to a \"tarball\" that is written to the top-level directory. Alternatively, you can include steps in the executable bash script of your job to move (i.e. mv ) output files from a subdirectory to the top-level directory. For example, if there is an output file that needs to be transferred back to the login node named job_output.txt written to job_output/ : 1 2 3 4 5 6 #! /bin/bash # various commands needed to run your job # move csv files to scratch dir mv job_output/job_output.txt $_CONDOR_SCRATCH_DIR Group Multiple Output Files For Convenience \u00b6 If your jobs will generate multiple output files, we recommend combining all output into a compressed tar archive for convenience, particularly when transferring your results to your local computer from your login node. To create a compressed tar archive, include commands in your your bash executable script to create a new subdirectory, move all of the output to this new subdirectory, and create a tar archive. For example: 1 2 3 4 5 6 7 8 #! /bin/bash # various commands needed to run your job # create output tar archive mkidr my_output mv my_job_output.csv my_job_output.svg my_output/ tar -czf my_job.output.tar.gz my_ouput/ The example above will create a file called my_job.output.tar.gz that contains all the output that was moved to my_output . Be sure to create my_job.output.tar.gz in the top-level directory of where your job executes and HTCondor will automatically transfer this tar archive back to your /home directory. Select Specific Output Files To Transfer to /home Using HTCondor \u00b6 As described above, HTCondor will, by default, transfer any files that are generated during the execution of your job(s) back to your /home directory. If your job(s) will produce multiple output files but you only need to retain a subset of these output files, we recommend deleting the unrequired output files or moving them to a subdirectory as a step in the bash executable script of your job - only the output files that remain in the top-level directory will be transferred back to your /home directory. In cases where a bash script is not used as the excutable of your job and you wish to have only specific output files transferred back, please contact us at support@opensciencegrid.org . Get Additional Options For Managing Job Output \u00b6 Several options exist for managing output file transfers back to your /home directory and we encourage you to get in touch with us at support@opensciencegrid.org to help identify the best solution for your needs.","title":"Transfer Job Output Files Up To 1GB In Size"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#transfer-job-output-files-up-to-1gb-in-size","text":"","title":"Transfer Job Output Files Up To 1GB In Size"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#overview","text":"When your OSG Connect jobs run, any output that gets generated is specifically written to the execute node on which the job ran. In order to get access to your output files, a copy of the output must be transferred back to your OSG Connect login node. This guide will describe the necessary steps, along with important considerations, for transferring your output files back to your /home directory on your OSG Connect login node.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#important-considerations","text":"As described in the Introduction to Data Management on OSG Connect , any output <1GB should be staged in your /home directory. For output files >1GB, please refer to our Transfer Large Input and Output Files >1GB In Size guide. If your jobs use any input files >1GB that are transferred from your /public directory using StacheCash, it is important that these files get deleted from the job's working directory or moved to a subdirectory so that HTCondor will not transfer these large files back to your /home directory.","title":"Important Considerations"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#use-htcondor-to-transfer-output-1gb","text":"By default, HTCondor will transfer any new or modified files in the job's top-level directory back to your /home directory location from which the condor_submit command was performed. This behavior only applies to files in the top-level directory of where your job executes, meaning HTCondor will ignore any files created in subdirectories of the job's top-level directory. Several options exist for modifying this default output file transfer behavior, including those described in this guide. To learn more, please contact us at support@opensciencegrid.org . What is the top-level directory of a job? Before executing a job, HTCondor will create a new directory on the execute node just for your job - this is the top-level directory of the job and the path is stored in the environment variable _CONDOR_SCRATCH_DIR . All of the input files transferred via transfer_input_files will first be written to this directory and it is from this path that a job starts to execute. After a job has completed the top-level directory and all of it's contents are deleted. What if my output file(s) are not written to the top-level directory? If your output files are written to a subdirectory, use the steps described below to convert the output directory to a \"tarball\" that is written to the top-level directory. Alternatively, you can include steps in the executable bash script of your job to move (i.e. mv ) output files from a subdirectory to the top-level directory. For example, if there is an output file that needs to be transferred back to the login node named job_output.txt written to job_output/ : 1 2 3 4 5 6 #! /bin/bash # various commands needed to run your job # move csv files to scratch dir mv job_output/job_output.txt $_CONDOR_SCRATCH_DIR","title":"Use HTCondor To Transfer Output &lt;1GB"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#group-multiple-output-files-for-convenience","text":"If your jobs will generate multiple output files, we recommend combining all output into a compressed tar archive for convenience, particularly when transferring your results to your local computer from your login node. To create a compressed tar archive, include commands in your your bash executable script to create a new subdirectory, move all of the output to this new subdirectory, and create a tar archive. For example: 1 2 3 4 5 6 7 8 #! /bin/bash # various commands needed to run your job # create output tar archive mkidr my_output mv my_job_output.csv my_job_output.svg my_output/ tar -czf my_job.output.tar.gz my_ouput/ The example above will create a file called my_job.output.tar.gz that contains all the output that was moved to my_output . Be sure to create my_job.output.tar.gz in the top-level directory of where your job executes and HTCondor will automatically transfer this tar archive back to your /home directory.","title":"Group Multiple Output Files For Convenience"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#select-specific-output-files-to-transfer-to-home-using-htcondor","text":"As described above, HTCondor will, by default, transfer any files that are generated during the execution of your job(s) back to your /home directory. If your job(s) will produce multiple output files but you only need to retain a subset of these output files, we recommend deleting the unrequired output files or moving them to a subdirectory as a step in the bash executable script of your job - only the output files that remain in the top-level directory will be transferred back to your /home directory. In cases where a bash script is not used as the excutable of your job and you wish to have only specific output files transferred back, please contact us at support@opensciencegrid.org .","title":"Select Specific Output Files To Transfer to /home Using HTCondor"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/output-file-transfer-via-htcondor/#get-additional-options-for-managing-job-output","text":"Several options exist for managing output file transfers back to your /home directory and we encourage you to get in touch with us at support@opensciencegrid.org to help identify the best solution for your needs.","title":"Get Additional Options For Managing Job Output"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/","text":"Use scp To Transfer Files To and From OSG Connect \u00b6 Overview \u00b6 This tutorial assumes that you will be using a command line application for performing file transfers instead of a GUI-based application such as WinSCP. We can transfer files to and from the OSG Connect login node using the scp command. Note scp is a counterpart to the secure shell command, ssh , that allows for secure, encrypted file transfers between systems using your ssh credentials. When using scp , you will always need to specify both the source of the content that you wish to copy and the destination of where you would like the copy to end up. For example: $ scp <source> <destination> Files on remote systems (like an OSG Connect login node) are indicated using username@machine:/path/to/file . Transfer Files To OSG Connect \u00b6 Let's say you have a file you wish to transfer to OSG Connect named my_file.txt . Using the terminal application on your computer, navigate to the location of my_file.txt . Then use the following scp command to tranfer my_file.txt to your /home on OSG Connect. Note that you will not be logged into OSG Connect when you perform this step. $ scp my_file.txt username@loginNN.osgconnect.net:/home/username/ Where NN is the specific number of your assigned login node (i.e. 04 or 05 ). Large files (>100MB in size) can be uploaded to your /public directory also using scp : $ scp my_large_file.gz username@loginNN.osgconnect.net:/public/username/ Transfer Directories To OSG Connect \u00b6 To copy directories using scp , add the (recursive) -r option to your scp command. For example: $ scp -r my_Dir username@loginNN.osgconnect.net:/home/username/ Transfer Files From OSG Connect \u00b6 To transfer files from OSG Connect back to your laptop or desktop you can use the scp as shown above, but with the source being the copy that is located on OSG Connect: $ scp username@loginNN.osgconnect.net:/home/username/my_file.txt ./ where ./ sets the destination of the copy to your current location on your computer Transfer Files Between OSG Connect and Another Server \u00b6 scp can be used to transfer files between OSG Connect and another server that you have ssh access to. This means that files don't have to first be transferred to your personal computer which can save a lot of time and effort! For example, to transfer a file from another server and your OSG Connect login node /home directory: $ scp username@serverhostname:/path/to/my_file.txt username@loginNN.osgconnect.net:/home/username Be sure to use the username assigned to you on the other server and to provide the full path on the other server to your file. Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Use scp To Transfer Files To and From OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#use-scp-to-transfer-files-to-and-from-osg-connect","text":"","title":"Use scp To Transfer Files To and From OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#overview","text":"This tutorial assumes that you will be using a command line application for performing file transfers instead of a GUI-based application such as WinSCP. We can transfer files to and from the OSG Connect login node using the scp command. Note scp is a counterpart to the secure shell command, ssh , that allows for secure, encrypted file transfers between systems using your ssh credentials. When using scp , you will always need to specify both the source of the content that you wish to copy and the destination of where you would like the copy to end up. For example: $ scp <source> <destination> Files on remote systems (like an OSG Connect login node) are indicated using username@machine:/path/to/file .","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#transfer-files-to-osg-connect","text":"Let's say you have a file you wish to transfer to OSG Connect named my_file.txt . Using the terminal application on your computer, navigate to the location of my_file.txt . Then use the following scp command to tranfer my_file.txt to your /home on OSG Connect. Note that you will not be logged into OSG Connect when you perform this step. $ scp my_file.txt username@loginNN.osgconnect.net:/home/username/ Where NN is the specific number of your assigned login node (i.e. 04 or 05 ). Large files (>100MB in size) can be uploaded to your /public directory also using scp : $ scp my_large_file.gz username@loginNN.osgconnect.net:/public/username/","title":"Transfer Files To OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#transfer-directories-to-osg-connect","text":"To copy directories using scp , add the (recursive) -r option to your scp command. For example: $ scp -r my_Dir username@loginNN.osgconnect.net:/home/username/","title":"Transfer Directories To OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#transfer-files-from-osg-connect","text":"To transfer files from OSG Connect back to your laptop or desktop you can use the scp as shown above, but with the source being the copy that is located on OSG Connect: $ scp username@loginNN.osgconnect.net:/home/username/my_file.txt ./ where ./ sets the destination of the copy to your current location on your computer","title":"Transfer Files From OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#transfer-files-between-osg-connect-and-another-server","text":"scp can be used to transfer files between OSG Connect and another server that you have ssh access to. This means that files don't have to first be transferred to your personal computer which can save a lot of time and effort! For example, to transfer a file from another server and your OSG Connect login node /home directory: $ scp username@serverhostname:/path/to/my_file.txt username@loginNN.osgconnect.net:/home/username Be sure to use the username assigned to you on the other server and to provide the full path on the other server to your file.","title":"Transfer Files Between OSG Connect and Another Server"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/scp/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/","text":"Transfer Larger Input and Output Files \u00b6 As of June 2022, users are no longer recommended to transfer large output files using stashcp and instead should use HTCondor's transfer_output_remaps feature. More information on this transition can be found below. Please direct any questions about this change to support@osg-htc.org Overview \u00b6 For input files >100MB and output files >1GB in size, the default HTCondor file transfer mechanisms run the risk of over-taxing the login nodes and their network capacity. And this is exactly why the OSG Data Federation exists for researchers with larger per-job data! Users on an OSG Connect login node can handle such files via the OSG Connect data caching origin (mounted and visible as the /public location) and use OSG's caching tools to scalably transfer them between the running jobs and the origin. The OSG caching tools ensure faster delivery to and from execute nodes by taking adantage of regional data caches in the OSG Data Federation, while preserving login node performance. Important Considerations and Best Practices \u00b6 As described in OSG Connect's Introduction to Data Management on OSG Connect , the /public location must be used for: Any input data or software larger than 100MB for transfer to jobs using OSG caching tools Any per-job output >1GB and <10GB , which should ONLY be transferred back to the origin by setting transfer_output_remaps in the job's submit file. User must never submit jobs from the /public location, and should continue to ONLY submit jobs from within their /home directory. All log , error , output files and any other files smaller than the above values should ONLY ever exist within the user's /home directory, unless otherwise directed by an OSG staff member. Thus, files within the /public location should only be referenced within the submit file by using the methods described further below . The /public location is a mount of the OSG Connect origin filesystem. It is mounted to the OSG Connect login nodes only so that users can appropriately stage large job inputs or retrieve outputs via the login nodes. Because of impacts to the filesystem of the data origin, files in the data origin ( /public ) should be organized in one or very few files per job. The filesystem is likely to encounter performance issues if/when files accumulated there are highly numerous and/or small. The /public location is otherwise unnecessary for smaller files (which can and should be served via the user's /home directory and regular HTCondor file transfer). Smaller files should only be handled via /public with explicit instruction from an OSG staff member. Files placed within a user's /public directory are publicly accessible , discoverable and readable by anyone, via the web. Data is made public via stash transfer (and, thus, via http addresses), and mirrored to a shared data repository which is available on a large number of systems around the world. Use a 'stash' URL to Transfer Large Input Files to Jobs \u00b6 Jobs submitted from the OSG Connect login nodes will transfer data from the origin when files are indicated with an appropriate stash:/// URL in the transfer_input_files line of the submit file: Upload your larger input and/or software files to your /public directory which is accessible via your OSG Connect login node at /public/username for which our Using scp To Transfer Files To OSG Connect guide may be helpful. Because of the way your files in /public are cached across the Open Science Pool, any changes or modifications that you make after placing a file in /public will not be propagated. This means that if you add a new version of a file to your /public directory, it must first be given a unique name (or directory path) to distinguish it from previous versions of that file. Adding a date or version number to directories or file names is strongly encouraged to manage your files in /public . Add the necessary details to your HTCondor submit file to tell HTCondor which files to transfer, and that your jobs must run on executes nodes that have access to the Open Science Data Federation. # Submit file example of large input/software transfer log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out #Transfer input files transfer_input_files = stash:///osgconnect/public/<username>/<dir>/<filename>, <other files> ...other submit file details... Note how the /public mount (visible on the login node) corresponds to the /osgconnect/public namespace across the Open Science Federation. For example, if the data file is located at /public/<username>/samples/sample01.dat , then the stash:/// URL to transfer this file to the job's working directory on the execute point would be: stash:///osgconnect/public/<username>/samples/sample01.dat Use transfer_output_remaps to Transfer Larger Job Outputs to the Data Origin \u00b6 For output, users should use the transfer_output_remaps option within their job's submit file, which will transfer the user's specified file to the specific location in the data origin. By using transfer_output_remaps , it is possible to specify what path to save a file to and what name to save it under. Using this approach, it is possible to save files back to specific locations in /public (as well as your /home directory, if desired). The syntax for transfer_output_remaps is: transfer_output_remaps = \"Output.txt = path/to/save/file/under/output.txt; Output.txt = path/to/save/file/under/RenamedOutput.txt\" When saving large output files back to /public , the path provided will look like: transfer_output_remaps = \"Output.txt = stash:///osgconnect/public/<username>/Output.txt\" Using transfer_output_remaps , tell HTCondor which output files need to be transferred back to your /public directory and what name you want these files to be saved under. # submit file example for large output log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out requirements = (OSGVO_OS_STRING =?= \"RHEL 7\") transfer_output_remaps = \"Output.txt = stash:///osgconnect/public/<username>/Output.txt\" ...other submit file details... If you have several output files being sent to /public , you may wish to define a new submit file variable to avoid having to re-write the stash:/// path repeatedly. For example, # submit file example for large output log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out requirements = (OSGVO_OS_STRING =?= \"RHEL 7\") STASH_LOCATION = stash:///osgconnect/public/<username> transfer_output_remaps = \"file1.txt = $(STASH_LOCATION)/file1.txt; file2.txt = $(STASH_LOCATION)/file2.txt; file3.txt = $(STASH_LOCATION)/file3.txt\" ...other submit file details... Phase out of Stashcp command \u00b6 Historically, output files could be transferred from a job to a /public location using the stashcp command within the job's executable, however, this mechanism is no longer encouraged for OSPool users. Instead, jobs should use transfer_output_remaps (an HTCondor feature) to transfer output files to /public . By using transfer_output_remaps , HTCondor will manage the output data transfer for your jobs. Data transferred via HTCondor is more likely to be transferred successfully and errors with transfer are more likely to be reported to the user. Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@osg-htc.org or visit the help desk and community forums .","title":"Transfer Larger Input and Output Files"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#transfer-larger-input-and-output-files","text":"As of June 2022, users are no longer recommended to transfer large output files using stashcp and instead should use HTCondor's transfer_output_remaps feature. More information on this transition can be found below. Please direct any questions about this change to support@osg-htc.org","title":"Transfer Larger Input and Output Files"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#overview","text":"For input files >100MB and output files >1GB in size, the default HTCondor file transfer mechanisms run the risk of over-taxing the login nodes and their network capacity. And this is exactly why the OSG Data Federation exists for researchers with larger per-job data! Users on an OSG Connect login node can handle such files via the OSG Connect data caching origin (mounted and visible as the /public location) and use OSG's caching tools to scalably transfer them between the running jobs and the origin. The OSG caching tools ensure faster delivery to and from execute nodes by taking adantage of regional data caches in the OSG Data Federation, while preserving login node performance.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#important-considerations-and-best-practices","text":"As described in OSG Connect's Introduction to Data Management on OSG Connect , the /public location must be used for: Any input data or software larger than 100MB for transfer to jobs using OSG caching tools Any per-job output >1GB and <10GB , which should ONLY be transferred back to the origin by setting transfer_output_remaps in the job's submit file. User must never submit jobs from the /public location, and should continue to ONLY submit jobs from within their /home directory. All log , error , output files and any other files smaller than the above values should ONLY ever exist within the user's /home directory, unless otherwise directed by an OSG staff member. Thus, files within the /public location should only be referenced within the submit file by using the methods described further below . The /public location is a mount of the OSG Connect origin filesystem. It is mounted to the OSG Connect login nodes only so that users can appropriately stage large job inputs or retrieve outputs via the login nodes. Because of impacts to the filesystem of the data origin, files in the data origin ( /public ) should be organized in one or very few files per job. The filesystem is likely to encounter performance issues if/when files accumulated there are highly numerous and/or small. The /public location is otherwise unnecessary for smaller files (which can and should be served via the user's /home directory and regular HTCondor file transfer). Smaller files should only be handled via /public with explicit instruction from an OSG staff member. Files placed within a user's /public directory are publicly accessible , discoverable and readable by anyone, via the web. Data is made public via stash transfer (and, thus, via http addresses), and mirrored to a shared data repository which is available on a large number of systems around the world.","title":"Important Considerations and Best Practices"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#use-a-stash-url-to-transfer-large-input-files-to-jobs","text":"Jobs submitted from the OSG Connect login nodes will transfer data from the origin when files are indicated with an appropriate stash:/// URL in the transfer_input_files line of the submit file: Upload your larger input and/or software files to your /public directory which is accessible via your OSG Connect login node at /public/username for which our Using scp To Transfer Files To OSG Connect guide may be helpful. Because of the way your files in /public are cached across the Open Science Pool, any changes or modifications that you make after placing a file in /public will not be propagated. This means that if you add a new version of a file to your /public directory, it must first be given a unique name (or directory path) to distinguish it from previous versions of that file. Adding a date or version number to directories or file names is strongly encouraged to manage your files in /public . Add the necessary details to your HTCondor submit file to tell HTCondor which files to transfer, and that your jobs must run on executes nodes that have access to the Open Science Data Federation. # Submit file example of large input/software transfer log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out #Transfer input files transfer_input_files = stash:///osgconnect/public/<username>/<dir>/<filename>, <other files> ...other submit file details... Note how the /public mount (visible on the login node) corresponds to the /osgconnect/public namespace across the Open Science Federation. For example, if the data file is located at /public/<username>/samples/sample01.dat , then the stash:/// URL to transfer this file to the job's working directory on the execute point would be: stash:///osgconnect/public/<username>/samples/sample01.dat","title":"Use a 'stash' URL to Transfer Large Input Files to Jobs"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#use-transfer_output_remaps-to-transfer-larger-job-outputs-to-the-data-origin","text":"For output, users should use the transfer_output_remaps option within their job's submit file, which will transfer the user's specified file to the specific location in the data origin. By using transfer_output_remaps , it is possible to specify what path to save a file to and what name to save it under. Using this approach, it is possible to save files back to specific locations in /public (as well as your /home directory, if desired). The syntax for transfer_output_remaps is: transfer_output_remaps = \"Output.txt = path/to/save/file/under/output.txt; Output.txt = path/to/save/file/under/RenamedOutput.txt\" When saving large output files back to /public , the path provided will look like: transfer_output_remaps = \"Output.txt = stash:///osgconnect/public/<username>/Output.txt\" Using transfer_output_remaps , tell HTCondor which output files need to be transferred back to your /public directory and what name you want these files to be saved under. # submit file example for large output log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out requirements = (OSGVO_OS_STRING =?= \"RHEL 7\") transfer_output_remaps = \"Output.txt = stash:///osgconnect/public/<username>/Output.txt\" ...other submit file details... If you have several output files being sent to /public , you may wish to define a new submit file variable to avoid having to re-write the stash:/// path repeatedly. For example, # submit file example for large output log = my_job.$(Cluster).$(Process).log error = my_job.$(Cluster).$(Process).err output = my_job.$(Cluster).$(Process).out requirements = (OSGVO_OS_STRING =?= \"RHEL 7\") STASH_LOCATION = stash:///osgconnect/public/<username> transfer_output_remaps = \"file1.txt = $(STASH_LOCATION)/file1.txt; file2.txt = $(STASH_LOCATION)/file2.txt; file3.txt = $(STASH_LOCATION)/file3.txt\" ...other submit file details...","title":"Use transfer_output_remaps to Transfer Larger Job Outputs to the Data Origin"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#phase-out-of-stashcp-command","text":"Historically, output files could be transferred from a job to a /public location using the stashcp command within the job's executable, however, this mechanism is no longer encouraged for OSPool users. Instead, jobs should use transfer_output_remaps (an HTCondor feature) to transfer output files to /public . By using transfer_output_remaps , HTCondor will manage the output data transfer for your jobs. Data transferred via HTCondor is more likely to be transferred successfully and errors with transfer are more likely to be reported to the user.","title":"Phase out of Stashcp command"},{"location":"managing_htc_workloads_on_osg_connect/managing_data_for_jobs/stashcache/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@osg-htc.org or visit the help desk and community forums .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/","text":"Checkpointing Jobs \u00b6 What is Checkpointing? \u00b6 Checkpointing is a technique that provides fault tolerance for a user's analysis. It consists of saving snapshots of a job's progress so the job can be restarted without losing its progress and having to restart from the beginning. We highly encourage checkpointing as a solution for jobs that will exceed the 10 hour maximum suggested runtime on the OSPool. This section is about jobs capable of periodically saving checkpoint information, and how to make HTCondor store that information safely, in case it's needed to continue the job on another machine or at a later time. There are two types of checkpointing: exit driven and eviction driven. In a vast majority of cases, exit driven checkpointing is preferred over eviction driven checkpointing. Therefore, this guide will focus on how to utilize exit driven checkpointing for your analysis. Note that not all software, programs, or code are capable of creating checkpoint files and knowing how to resume from them. Consult the manual for your software or program to determine if it supports checkpointing features. Some manuals will refer this ability as \"checkpoint\" features, as the ability to \"resume\" mid-analysis if a job is interrupted, or as \"checkpoint/restart\" capabilities. Contact a Research Computing Facilitator if you would like help determining if your software, program, or code is able to checkpoint. Why Checkpoint? \u00b6 Checkpointing allows a job to automatically resume from approximately where it left off instead of having to start over if interrupted. This behavior is advantageous for jobs limited by a maximum runtime policy. It is also advantageous for jobs submitted to backfill resources with no runtime guarantee (i.e. jobs on the OSPool) where the compute resources may also be more prone to hardware or networking failures. For example, checkpointing jobs that are limited by a runtime policy can enable HTCondor to exit a job and automatically requeue it to avoid hitting the maximum runtime limit. By using checkpointing, jobs circumvent hitting the maximum runtime limit and can run for extended periods of time until the completion of the analysis. This behavior avoids costly setbacks that may be caused by loosing results mid-way through an analysis due to hitting a runtime limit. Process of Exit Driven Checkpointing \u00b6 Using exit driven checkpointing, a job is specified to time out after a user-specified amount of time with an exit code value of 85 (more on this below). Upon hitting this time limit, HTCondor transfers any checkpoint files listed in the submit file attribute transfer_checkpoint_files to a directory called /spool . This directory acts as a storage location for these files in case the job is interrupted. HTCondor then knows that jobs with exit code 85 should be automatically requeued, and will transfer the checkpoint files in /spool to your job's working directory prior to restarting your executable. The process of exit driven checkpointing relies heavily on the use of exit codes to determine the next appropriate steps for HTCondor to take with a job. In general, exit codes are used to report system responses, such as when an analysis is running, encountered an error, or successfully completes. HTCondor recognizes exit code 85 as checkpointing jobs and therefore will know to handle these jobs differently than non-checkpoiting jobs. Requirements for Exit Driven Checkpointing \u00b6 Requirements for your code or software: Checkpoint : The software, program, or code you are using must be able to capture checkpoint files (i.e. snapshots of the progress made thus far) and know how to resume from them. Resume : This means your code must be able to recognize checkpoint files and know to resume from them instead of the original input data when the code is restarted. Exit : Jobs should exit with an exit code value of 85 after successfully creating checkpoint files. Additionally, jobs need to be able to exit with a non- 85 value if they encounter an error or write the writing the final outputs. In some cases, these requirements can be achieved by using a wrapper script. This means that your executable may be a script, rather than the code that is writing the checkpoint. An example wrapper script that enables some of these behaviors is below. Contact a Research Computing Facilitator for help determining if your job is capable of using checkpointing. Changes to the Submit File \u00b6 Several modifications to the submit file are needed to enable HTCondor's checkpointing feature. The line checkpoint_exit_code = 85 must be added. HTCondor recognizes code 85 as a checkpoint job. This means HTCondor knows to end a job with this code but to then to requeue it repeatedly until the analysis completes. The value of when_to_transfer_output should be set to ON_EXIT . The name of the checkpoint files or directories to be transferred to /spool should be specified using transfer_checkpoint_files . Optional In some cases, it is necessary to write a wrapper script to tell a job when to timeout and exit. In cases such as this, the executable will need to be changed to the name of that wrapper script. An example of a wrapper script that enables a job to checkout and exit with the proper exit codes can be found below. An example submit file for an exit driven checkpointing job looks like: # exit-driven-example.submit executable = exit-driven.sh arguments = argument1 argument2 checkpoint_exit_code = 85 transfer_checkpoint_files = my_output.txt, temp_dir, temp_file.txt should_transfer_files = yes when_to_transfer_output = ON_EXIT output = example.out error = example.err log = example.log cpu = 1 request_disk = 2 GB request_memory = 2 GB queue 1 Example Wrapper Script for Checkpointing Job \u00b6 As previously described, it may be necessary to use a wrapper script to tell your job when and how to exit as it checkpoints. An example of a wrapper script that tells a job to exit every 4 hours looks like: 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash timeout 4h do_science arg1 arg2 timeout_exit_status = $? if [ $timeout_exit_status -eq 124 ] ; then exit 85 fi exit $timeout_exit_status Let's take a moment to understand what each section of this wrapper script is doing: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash timeout 4h do_science argument1 argument2 # The `timeout` command will stop the job after 4 hours (4h). # This number can be increased or decreased depending on how frequent your code/software/program # is creating checkpoint files and how long it takes to create/resume from these files. # Replace `do_science argument1 argument2` with the execution command and arguments for your job. timeout_exit_status = $? # Uses the bash notation of `$?` to call the exit value of the last executed command # and to save it in a variable called `timeout_exit_status`. if [ $timeout_exit_status -eq 124 ] ; then exit 85 fi exit $timeout_exit_status # Programs typically have an exit code of `124` while they are actively running. # The portion above replaces exit code `124` with code `85`. HTCondor recognizes # code `85` and knows to end a job with this code once the time specified by `timeout` # has been reached. Upon exiting, HTCondor saves the files from jobs with exit code `85` # in the temporary directory within `/spool`. Once the files have been transferred, # HTCondor automatically requeues that job and fetches the files found in `/spool`. # If an exit code of `124` is not observed (for example if the program is done running # or has encountered an error), HTCondor will end the job and will not automaticlally requeue it. The ideal timeout frequency for a job is every 1-5 hours with a maximum of 10 hours. For jobs that checkpoint and timeout in under an hour, it is possible that a job may spend more time with checkpointing procedures than moving forward with the analysis. After 10 hours, the likelihood of a job being inturrupted on the OSPool is higher. Checking the Progress of Checkpointing Jobs \u00b6 It is possible to investigate checkpoint files once they have been transferred to /spool . You can explore the checkpointed files in /spool by navigating to /home/condor/spool on an OSG Connect login node. The directories in this folder are the last four digits of a job's cluster ID with leading zeros removed. Sub folders are labeled with the process ID for each job. For example, to investigate the checkpoint files for 17870068.220 , the files in /spool would be found in folder 68 in a subdirectory called 220 . More Information \u00b6 More information on checkpointing HTCondor jobs can be found in HTCondor's manual: https://htcondor.readthedocs.io/en/latest/users-manual/self-checkpointing-applications.html This documentation contains additional features available to checkpointing jobs, as well as additional examples such as a python checkpointing job.","title":"Checkpointing Jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#checkpointing-jobs","text":"","title":"Checkpointing Jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#what-is-checkpointing","text":"Checkpointing is a technique that provides fault tolerance for a user's analysis. It consists of saving snapshots of a job's progress so the job can be restarted without losing its progress and having to restart from the beginning. We highly encourage checkpointing as a solution for jobs that will exceed the 10 hour maximum suggested runtime on the OSPool. This section is about jobs capable of periodically saving checkpoint information, and how to make HTCondor store that information safely, in case it's needed to continue the job on another machine or at a later time. There are two types of checkpointing: exit driven and eviction driven. In a vast majority of cases, exit driven checkpointing is preferred over eviction driven checkpointing. Therefore, this guide will focus on how to utilize exit driven checkpointing for your analysis. Note that not all software, programs, or code are capable of creating checkpoint files and knowing how to resume from them. Consult the manual for your software or program to determine if it supports checkpointing features. Some manuals will refer this ability as \"checkpoint\" features, as the ability to \"resume\" mid-analysis if a job is interrupted, or as \"checkpoint/restart\" capabilities. Contact a Research Computing Facilitator if you would like help determining if your software, program, or code is able to checkpoint.","title":"What is Checkpointing?"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#why-checkpoint","text":"Checkpointing allows a job to automatically resume from approximately where it left off instead of having to start over if interrupted. This behavior is advantageous for jobs limited by a maximum runtime policy. It is also advantageous for jobs submitted to backfill resources with no runtime guarantee (i.e. jobs on the OSPool) where the compute resources may also be more prone to hardware or networking failures. For example, checkpointing jobs that are limited by a runtime policy can enable HTCondor to exit a job and automatically requeue it to avoid hitting the maximum runtime limit. By using checkpointing, jobs circumvent hitting the maximum runtime limit and can run for extended periods of time until the completion of the analysis. This behavior avoids costly setbacks that may be caused by loosing results mid-way through an analysis due to hitting a runtime limit.","title":"Why Checkpoint?"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#process-of-exit-driven-checkpointing","text":"Using exit driven checkpointing, a job is specified to time out after a user-specified amount of time with an exit code value of 85 (more on this below). Upon hitting this time limit, HTCondor transfers any checkpoint files listed in the submit file attribute transfer_checkpoint_files to a directory called /spool . This directory acts as a storage location for these files in case the job is interrupted. HTCondor then knows that jobs with exit code 85 should be automatically requeued, and will transfer the checkpoint files in /spool to your job's working directory prior to restarting your executable. The process of exit driven checkpointing relies heavily on the use of exit codes to determine the next appropriate steps for HTCondor to take with a job. In general, exit codes are used to report system responses, such as when an analysis is running, encountered an error, or successfully completes. HTCondor recognizes exit code 85 as checkpointing jobs and therefore will know to handle these jobs differently than non-checkpoiting jobs.","title":"Process of Exit Driven Checkpointing"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#requirements-for-exit-driven-checkpointing","text":"Requirements for your code or software: Checkpoint : The software, program, or code you are using must be able to capture checkpoint files (i.e. snapshots of the progress made thus far) and know how to resume from them. Resume : This means your code must be able to recognize checkpoint files and know to resume from them instead of the original input data when the code is restarted. Exit : Jobs should exit with an exit code value of 85 after successfully creating checkpoint files. Additionally, jobs need to be able to exit with a non- 85 value if they encounter an error or write the writing the final outputs. In some cases, these requirements can be achieved by using a wrapper script. This means that your executable may be a script, rather than the code that is writing the checkpoint. An example wrapper script that enables some of these behaviors is below. Contact a Research Computing Facilitator for help determining if your job is capable of using checkpointing.","title":"Requirements for Exit Driven Checkpointing"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#changes-to-the-submit-file","text":"Several modifications to the submit file are needed to enable HTCondor's checkpointing feature. The line checkpoint_exit_code = 85 must be added. HTCondor recognizes code 85 as a checkpoint job. This means HTCondor knows to end a job with this code but to then to requeue it repeatedly until the analysis completes. The value of when_to_transfer_output should be set to ON_EXIT . The name of the checkpoint files or directories to be transferred to /spool should be specified using transfer_checkpoint_files . Optional In some cases, it is necessary to write a wrapper script to tell a job when to timeout and exit. In cases such as this, the executable will need to be changed to the name of that wrapper script. An example of a wrapper script that enables a job to checkout and exit with the proper exit codes can be found below. An example submit file for an exit driven checkpointing job looks like: # exit-driven-example.submit executable = exit-driven.sh arguments = argument1 argument2 checkpoint_exit_code = 85 transfer_checkpoint_files = my_output.txt, temp_dir, temp_file.txt should_transfer_files = yes when_to_transfer_output = ON_EXIT output = example.out error = example.err log = example.log cpu = 1 request_disk = 2 GB request_memory = 2 GB queue 1","title":"Changes to the Submit File"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#example-wrapper-script-for-checkpointing-job","text":"As previously described, it may be necessary to use a wrapper script to tell your job when and how to exit as it checkpoints. An example of a wrapper script that tells a job to exit every 4 hours looks like: 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash timeout 4h do_science arg1 arg2 timeout_exit_status = $? if [ $timeout_exit_status -eq 124 ] ; then exit 85 fi exit $timeout_exit_status Let's take a moment to understand what each section of this wrapper script is doing: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash timeout 4h do_science argument1 argument2 # The `timeout` command will stop the job after 4 hours (4h). # This number can be increased or decreased depending on how frequent your code/software/program # is creating checkpoint files and how long it takes to create/resume from these files. # Replace `do_science argument1 argument2` with the execution command and arguments for your job. timeout_exit_status = $? # Uses the bash notation of `$?` to call the exit value of the last executed command # and to save it in a variable called `timeout_exit_status`. if [ $timeout_exit_status -eq 124 ] ; then exit 85 fi exit $timeout_exit_status # Programs typically have an exit code of `124` while they are actively running. # The portion above replaces exit code `124` with code `85`. HTCondor recognizes # code `85` and knows to end a job with this code once the time specified by `timeout` # has been reached. Upon exiting, HTCondor saves the files from jobs with exit code `85` # in the temporary directory within `/spool`. Once the files have been transferred, # HTCondor automatically requeues that job and fetches the files found in `/spool`. # If an exit code of `124` is not observed (for example if the program is done running # or has encountered an error), HTCondor will end the job and will not automaticlally requeue it. The ideal timeout frequency for a job is every 1-5 hours with a maximum of 10 hours. For jobs that checkpoint and timeout in under an hour, it is possible that a job may spend more time with checkpointing procedures than moving forward with the analysis. After 10 hours, the likelihood of a job being inturrupted on the OSPool is higher.","title":"Example Wrapper Script for Checkpointing Job"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#checking-the-progress-of-checkpointing-jobs","text":"It is possible to investigate checkpoint files once they have been transferred to /spool . You can explore the checkpointed files in /spool by navigating to /home/condor/spool on an OSG Connect login node. The directories in this folder are the last four digits of a job's cluster ID with leading zeros removed. Sub folders are labeled with the process ID for each job. For example, to investigate the checkpoint files for 17870068.220 , the files in /spool would be found in folder 68 in a subdirectory called 220 .","title":"Checking the Progress of Checkpointing Jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/checkpointing-on-OSPool/#more-information","text":"More information on checkpointing HTCondor jobs can be found in HTCondor's manual: https://htcondor.readthedocs.io/en/latest/users-manual/self-checkpointing-applications.html This documentation contains additional features available to checkpointing jobs, as well as additional examples such as a python checkpointing job.","title":"More Information"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/","text":"Easily Submit Multiple Jobs \u00b6 Overview \u00b6 HTCondor has several convenient features for streamlining high-throughput job submission. This guide provides several examples of how to leverage these features to submit multiple jobs with a single submit file. Why submit multiple jobs with a single submit file? As described in our Policies for using an OSG Connect submit server , users should submit multiple jobs using a single submit file, or where applicable, as few separate submit files as needed. Using HTCondor multi-job submission features is more efficient for users and will help ensure reliable operation of the the login nodes. Many options exist for streamlining your submission of multiple jobs, and this guide only covers a few examples of what is truly possible with HTCondor. If you are interested in a particular approach that isn't described here, please contact OSG Connect support and we will work with you to identify options to meet the needs of your work. Submit Multiple Jobs Using queue \u00b6 All HTCondor submit files require a queue attribute (which must also be the last line of the submit file). By default, queue will submit one job, but users can also configure the queue attribute to behave like a for loop that will submit multiple jobs, with each job varying as predefined by the user. Below are different HTCondor submit file examples for submitting batches of multiple jobs and, where applicable, how to indicate the differences between jobs in a batch with user-defined variables. Additional examples and use cases are provided further below: queue <N> - will submit N number of jobs. Examples include performing replications, where the same job must be repeated N number of times, looping through files named with numbers, and looping through a matrix where each job uses information from a specific row or column. queue <var> from <list> - will loop through a list of file names, parameters, etc. as defined in separate text file (i.e. ). This queue option is very flexible and provides users with many options for submitting multiple jobs. Organizing Jobs Into Individual Directories - another option that can be helpful in organizing multi-job submissions. These queue options are also described in the following video from HTCondor Week 2020: Submitting Multiple Jobs Using HTCondor Video What makes these queue options powerful is the ability to use user-defined variables to specify details about your jobs in the HTCondor submit file. The examples below will include the use of $(variable_name) to specify details like input file names, file locations (aka paths), etc. When selecting a variable name, users must avoid bespoke HTCondor submit file variables such as Cluster , Process , output , and input , arguments , etc. 1. Use queue N in you HTCondor submit files \u00b6 When using queue N , HTCondor will submit a total of N jobs, counting from 0 to N - 1 and each job will be assigned a unique Process id number spanning this range of values. Because the Process variable will be unique for each job, it can be used in the submit file to indicate unique filenames and filepaths for each job. The most straightforward example of using queue N is to submit N number of identical jobs. The example shown below demonstrates how to use the Cluster and Process variables to assign unique names for the HTCondor error , output , and log files for each job in the batch: # 100jobs.sub # submit 100 identical jobs log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out ... remaining submit details ... queue 100 For each job, the appropriate number, 0, 1, 2, ... 99 will replace $(Process) . $(Cluster) will be a unique number assigned to the entire 100 job batch. Each time you run condor_submit job.sub , you will be provided with the Cluster number which you will also see in the output produced by the command condor_q . If a uniquely named results file needs to be returned by each job, $(Process) and $(Cluster) can also be used as arguments , and anywhere else as needed, in the submit file: arguments = $(Cluster)_$(Process).results ... remaining submit details ... queue 100 Be sure to properly format the arguments statement according to the executable used by the job. What if my jobs are not identical? queue N may still be a great option! Additional examples for using this option include: A. Use integer numbered input files \u00b6 [user@login]$ ls *.data 0.data 1.data 2.data 3.data ... 97.data 98.data 99.data In the submit file, use: transfer_input_files = $(Process).data ... remaining submit details ... queue 100 B. Specify a row or column number for each job \u00b6 $(Process) can be used to specify a unique row or column of information in a matrix to be used by each job in the batch. The matrix needs to then be transferred with each job as input. For exmaple: transfer_input_files = matrix.csv arguments = $(Process) ... remaining submit details ... queue 100 The above exmaples assumes that your job is set up to use an argument to specify the row or column to be used by your software. C. Need N to start at 1 \u00b6 If your input files are numbered 1 - 100 instead of 0 - 99, or your matrix row starts with 1 instead of 0, you can perform basic arithmetic in the submit file: plusone = $(Process) + 1 NewProcess = $INT(plusone, %d) arguments = $(NewProcess) ... remaining submit details ... queue 100 Then use $(NewProcess) anywhere in the submit file that you would have otherwise used $(Process) . Note that there is nothing special about the names plusone and NewProcess , you can use any names you want as variables. 2. Submit multiple jobs with one or more distinct variables per job \u00b6 Think about what's different between each job that needs to be submitted. Will each job use a different input file or combination of software parameters? Do some of the jobs need more memory or disk space? Do you want to use a different software or script on a common set of input files? Using queue <var> from <list> in your submit files can make that possible! <var> can be a single user-defined variable or comma-separated list of variables to be used anywhere in the submit file. <list> is a plain text file that defines <var> for each individual job to be submitted in the batch. Suppose you need to run a program called compare_states that will run on on the following set of input files: illinois.data , nebraska.data , and wisconsin.data and each input file can analyzed as a separate job. To create a submit file that will submit all three jobs, first create a text file that lists each .data file (one file per line). This step can be performed directly on the login node, for example: [user@state-analysis]$ ls *.data > states.txt [user@state-analysis]$ cat states.txt illinois.data nebraska.data wisconsin.data Then, in the submit file, following the pattern queue <var> from <list> , replace <var> with a variable name like state and replace <list> with the list of .data files saved in states.txt : queue state from states.txt For each line in states.txt , HTCondor will submit a job and the variable $(state) can be used anywhere in the submit file to represent the name of the .data file to be used by that job. For the first job, $(state) will be illinois.data , for the second job $(state) will be nebraska.data , and so on. For example: # run_compare_states_per_state.sub transfer_input_files = $(state) arguments = $(state) executable = compare_states ... remaining submit details ... queue state from states.txt For a working example of this kind of job submission, see our Word Frequency Tutorial . Use multiple variables for each job \u00b6 Let's imagine that each state .data file contains data spanning several years and that each job needs to analyze a specific year of data. Then the states.txt file can be modified to specify this information: [user@state-analysis]$ cat states.txt illinois.data, 1995 illinois.data, 2005 nebraska.data, 1999 nebraska.data, 2005 wisconsin.data, 2000 wisconsin.data, 2015 Then modify the queue to define two <var> named state and year : queue state,year from states.txt Then the variables $(state) and $(year) can be used in the submit file: # run_compare_states_by_year.sub arguments = $(state) $(year) transfer_input_files = $(state) executable = compare_states ... remaining submit details ... queue state,year from states.txt 3. Organizing Jobs Into Individual Directories \u00b6 One way to organize jobs is to assign each job to its own directory, instead of putting files in the same directory with unique names. To continue our \\\"compare_states\\\" example, suppose there\\'s a directory for each state you want to analyze, and each of those directories has its own input file named input.data : [user@state-analysis]$ ls -F compare_states illinois/ nebraska/ wisconsin/ [user@state-analysis]$ ls -F illinois/ input.data [user@state-analysis]$ ls -F nebraska/ input.data [user@state-analysis]$ ls -F wisconsin/ input.data The HTCondor submit file attribute initialdir can be used to define a specific directory from which each job in the batch will be submitted. The default initialdir location is the directory from which the command condor_submit myjob.sub is executed. Combining queue var from list with initiadir , each line of will include the path to each state directory and initialdir set to this path for each job: #state-per-dir-job.sub initial_dir = $(state_dir) transfer_input_files = input.data executable = compare_states ... remaining submit details ... queue state_dir from state-dirs.txt Where state-dirs.txt is a list of each directory with state data: [user@state-analysis]$ cat state-dirs.txt illinois nebraska wisconsin Notice that executable = compare_states has remained unchanged in the above example. When using initialdir , only the input and output file path (including the HTCondor log, error, and output files) will be changed by initialdir . In this example, HTCondor will create a job for each directory in state-dirs.txt and use that state\\'s directory as the initialdir from which the job will be submitted. Therefore, transfer_input_files = input.data can be used without specifying the path to this input.data file. Any output generated by the job will then be returned to the initialdir location. Get Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Easily Submit Multiple Jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#easily-submit-multiple-jobs","text":"","title":"Easily Submit Multiple Jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#overview","text":"HTCondor has several convenient features for streamlining high-throughput job submission. This guide provides several examples of how to leverage these features to submit multiple jobs with a single submit file. Why submit multiple jobs with a single submit file? As described in our Policies for using an OSG Connect submit server , users should submit multiple jobs using a single submit file, or where applicable, as few separate submit files as needed. Using HTCondor multi-job submission features is more efficient for users and will help ensure reliable operation of the the login nodes. Many options exist for streamlining your submission of multiple jobs, and this guide only covers a few examples of what is truly possible with HTCondor. If you are interested in a particular approach that isn't described here, please contact OSG Connect support and we will work with you to identify options to meet the needs of your work.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#submit-multiple-jobs-using-queue","text":"All HTCondor submit files require a queue attribute (which must also be the last line of the submit file). By default, queue will submit one job, but users can also configure the queue attribute to behave like a for loop that will submit multiple jobs, with each job varying as predefined by the user. Below are different HTCondor submit file examples for submitting batches of multiple jobs and, where applicable, how to indicate the differences between jobs in a batch with user-defined variables. Additional examples and use cases are provided further below: queue <N> - will submit N number of jobs. Examples include performing replications, where the same job must be repeated N number of times, looping through files named with numbers, and looping through a matrix where each job uses information from a specific row or column. queue <var> from <list> - will loop through a list of file names, parameters, etc. as defined in separate text file (i.e. ). This queue option is very flexible and provides users with many options for submitting multiple jobs. Organizing Jobs Into Individual Directories - another option that can be helpful in organizing multi-job submissions. These queue options are also described in the following video from HTCondor Week 2020: Submitting Multiple Jobs Using HTCondor Video What makes these queue options powerful is the ability to use user-defined variables to specify details about your jobs in the HTCondor submit file. The examples below will include the use of $(variable_name) to specify details like input file names, file locations (aka paths), etc. When selecting a variable name, users must avoid bespoke HTCondor submit file variables such as Cluster , Process , output , and input , arguments , etc.","title":"Submit Multiple Jobs Using queue"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#1-use-queue-n-in-you-htcondor-submit-files","text":"When using queue N , HTCondor will submit a total of N jobs, counting from 0 to N - 1 and each job will be assigned a unique Process id number spanning this range of values. Because the Process variable will be unique for each job, it can be used in the submit file to indicate unique filenames and filepaths for each job. The most straightforward example of using queue N is to submit N number of identical jobs. The example shown below demonstrates how to use the Cluster and Process variables to assign unique names for the HTCondor error , output , and log files for each job in the batch: # 100jobs.sub # submit 100 identical jobs log = job_$(Cluster)_$(Process).log error = job_$(Cluster)_$(Process).err output = job_$(Cluster)_$(Process).out ... remaining submit details ... queue 100 For each job, the appropriate number, 0, 1, 2, ... 99 will replace $(Process) . $(Cluster) will be a unique number assigned to the entire 100 job batch. Each time you run condor_submit job.sub , you will be provided with the Cluster number which you will also see in the output produced by the command condor_q . If a uniquely named results file needs to be returned by each job, $(Process) and $(Cluster) can also be used as arguments , and anywhere else as needed, in the submit file: arguments = $(Cluster)_$(Process).results ... remaining submit details ... queue 100 Be sure to properly format the arguments statement according to the executable used by the job. What if my jobs are not identical? queue N may still be a great option! Additional examples for using this option include:","title":"1. Use queue N in you HTCondor submit files"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#a-use-integer-numbered-input-files","text":"[user@login]$ ls *.data 0.data 1.data 2.data 3.data ... 97.data 98.data 99.data In the submit file, use: transfer_input_files = $(Process).data ... remaining submit details ... queue 100","title":"A. Use integer numbered input files"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#b-specify-a-row-or-column-number-for-each-job","text":"$(Process) can be used to specify a unique row or column of information in a matrix to be used by each job in the batch. The matrix needs to then be transferred with each job as input. For exmaple: transfer_input_files = matrix.csv arguments = $(Process) ... remaining submit details ... queue 100 The above exmaples assumes that your job is set up to use an argument to specify the row or column to be used by your software.","title":"B. Specify a row or column number for each job"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#c-need-n-to-start-at-1","text":"If your input files are numbered 1 - 100 instead of 0 - 99, or your matrix row starts with 1 instead of 0, you can perform basic arithmetic in the submit file: plusone = $(Process) + 1 NewProcess = $INT(plusone, %d) arguments = $(NewProcess) ... remaining submit details ... queue 100 Then use $(NewProcess) anywhere in the submit file that you would have otherwise used $(Process) . Note that there is nothing special about the names plusone and NewProcess , you can use any names you want as variables.","title":"C. Need N to start at 1"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#2-submit-multiple-jobs-with-one-or-more-distinct-variables-per-job","text":"Think about what's different between each job that needs to be submitted. Will each job use a different input file or combination of software parameters? Do some of the jobs need more memory or disk space? Do you want to use a different software or script on a common set of input files? Using queue <var> from <list> in your submit files can make that possible! <var> can be a single user-defined variable or comma-separated list of variables to be used anywhere in the submit file. <list> is a plain text file that defines <var> for each individual job to be submitted in the batch. Suppose you need to run a program called compare_states that will run on on the following set of input files: illinois.data , nebraska.data , and wisconsin.data and each input file can analyzed as a separate job. To create a submit file that will submit all three jobs, first create a text file that lists each .data file (one file per line). This step can be performed directly on the login node, for example: [user@state-analysis]$ ls *.data > states.txt [user@state-analysis]$ cat states.txt illinois.data nebraska.data wisconsin.data Then, in the submit file, following the pattern queue <var> from <list> , replace <var> with a variable name like state and replace <list> with the list of .data files saved in states.txt : queue state from states.txt For each line in states.txt , HTCondor will submit a job and the variable $(state) can be used anywhere in the submit file to represent the name of the .data file to be used by that job. For the first job, $(state) will be illinois.data , for the second job $(state) will be nebraska.data , and so on. For example: # run_compare_states_per_state.sub transfer_input_files = $(state) arguments = $(state) executable = compare_states ... remaining submit details ... queue state from states.txt For a working example of this kind of job submission, see our Word Frequency Tutorial .","title":"2. Submit multiple jobs with one or more distinct variables per job"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#use-multiple-variables-for-each-job","text":"Let's imagine that each state .data file contains data spanning several years and that each job needs to analyze a specific year of data. Then the states.txt file can be modified to specify this information: [user@state-analysis]$ cat states.txt illinois.data, 1995 illinois.data, 2005 nebraska.data, 1999 nebraska.data, 2005 wisconsin.data, 2000 wisconsin.data, 2015 Then modify the queue to define two <var> named state and year : queue state,year from states.txt Then the variables $(state) and $(year) can be used in the submit file: # run_compare_states_by_year.sub arguments = $(state) $(year) transfer_input_files = $(state) executable = compare_states ... remaining submit details ... queue state,year from states.txt","title":"Use multiple variables for each job"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#3-organizing-jobs-into-individual-directories","text":"One way to organize jobs is to assign each job to its own directory, instead of putting files in the same directory with unique names. To continue our \\\"compare_states\\\" example, suppose there\\'s a directory for each state you want to analyze, and each of those directories has its own input file named input.data : [user@state-analysis]$ ls -F compare_states illinois/ nebraska/ wisconsin/ [user@state-analysis]$ ls -F illinois/ input.data [user@state-analysis]$ ls -F nebraska/ input.data [user@state-analysis]$ ls -F wisconsin/ input.data The HTCondor submit file attribute initialdir can be used to define a specific directory from which each job in the batch will be submitted. The default initialdir location is the directory from which the command condor_submit myjob.sub is executed. Combining queue var from list with initiadir , each line of will include the path to each state directory and initialdir set to this path for each job: #state-per-dir-job.sub initial_dir = $(state_dir) transfer_input_files = input.data executable = compare_states ... remaining submit details ... queue state_dir from state-dirs.txt Where state-dirs.txt is a list of each directory with state data: [user@state-analysis]$ cat state-dirs.txt illinois nebraska wisconsin Notice that executable = compare_states has remained unchanged in the above example. When using initialdir , only the input and output file path (including the HTCondor log, error, and output files) will be changed by initialdir . In this example, HTCondor will create a job for each directory in state-dirs.txt and use that state\\'s directory as the initialdir from which the job will be submitted. Therefore, transfer_input_files = input.data can be used without specifying the path to this input.data file. Any output generated by the job will then be returned to the initialdir location.","title":"3. Organizing Jobs Into Individual Directories"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/submit-multiple-jobs/#get-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Get Help"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-command/","text":"Use OSG Connect Tutorials \u00b6 OSG Connect tutorials on Github \u00b6 All of the OSG Connect tutorials are available as repositories on Github . These tutorials are tested regularly and should work as is, but if you experience any issues please contact us. Tutorial commands \u00b6 From the OSG Connect login node, the following tutorial commands are available: $ tutorial $ tutorial list $ tutorial info <tutorial-name> $ tutorial <tutorial-name> Available tutorials \u00b6 The OSG Connect login nodes have the following tutorials pre-installed. To see what is available: $ tutorial list Currently available tutorials: R ...................... Estimate Pi using the R programming language R-addlibSNA ............ Shows how to add R external libraries for the R jobs on the OSG ScalingUp-Python ....... Scaling up compute resources - Python example to optimize a function on grid points annex .................. None blast-split ............ How to run BLAST on the OSG by splitting a large input file connect-client ......... Demonstrates how to use the connect client for remote job submission dagman-wordfreq ........ DAGMan based wordfreq example dagman-wordfreq-stash .. DAGMan based wordfreq - data from stash error101 ............... Use condor_q -better-analyze to analyze stuck jobs exitcode ............... Use HTCondor's periodic_release to retry failed jobs htcondor-transfer ...... Transfer data via HTCondor's own mechanisms matlab-HelloWorld ...... Creating standalone MATLAB application - Hello World nelle-nemo ............. Running Nelle Nemo's goostats on the grid oasis-parrot ........... Software access with OASIS and Parrot octave ................. Matrix manipulation via the Octave programming language osg-locations .......... Tutorial based on OSG location exercise from the User School pegasus ................ An introduction to the Pegasus job workflow manager photodemo .............. A complete analysis workflow using HTTP transfer quickstart ............. How to run your first OSG job root ................... Inspect ntuples using the ROOT analysis framework scaling ................ Learn to steer jobs to particular resources scaling-up-resources ... A simple multi-job demonstration software ............... Software access tutorial stash-cvmfs ............ Shows how to use stash-cvmfs for input data transfer stash-http ............. Retrieve job input files from Stash via HTTP tensorflow-matmul ...... Tensorflow math operations as a singularity container job on the OSG - matrix multiplication Install and setup a tutorial \u00b6 On the OSG Connect login node, create a directory, cd to it, and invoke the command: $ tutorial <tutorial-name> This command will clone the tutorial repository to your current working directory. cd to the repository directory and follow the steps described in the readme.md file. Alternatively, you can view the readme.md file at the tutorial's corresponding GitHub page.","title":"Use OSG Connect Tutorials"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-command/#use-osg-connect-tutorials","text":"","title":"Use OSG Connect Tutorials"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-command/#osg-connect-tutorials-on-github","text":"All of the OSG Connect tutorials are available as repositories on Github . These tutorials are tested regularly and should work as is, but if you experience any issues please contact us.","title":"OSG Connect tutorials on Github"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-command/#tutorial-commands","text":"From the OSG Connect login node, the following tutorial commands are available: $ tutorial $ tutorial list $ tutorial info <tutorial-name> $ tutorial <tutorial-name>","title":"Tutorial commands"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-command/#available-tutorials","text":"The OSG Connect login nodes have the following tutorials pre-installed. To see what is available: $ tutorial list Currently available tutorials: R ...................... Estimate Pi using the R programming language R-addlibSNA ............ Shows how to add R external libraries for the R jobs on the OSG ScalingUp-Python ....... Scaling up compute resources - Python example to optimize a function on grid points annex .................. None blast-split ............ How to run BLAST on the OSG by splitting a large input file connect-client ......... Demonstrates how to use the connect client for remote job submission dagman-wordfreq ........ DAGMan based wordfreq example dagman-wordfreq-stash .. DAGMan based wordfreq - data from stash error101 ............... Use condor_q -better-analyze to analyze stuck jobs exitcode ............... Use HTCondor's periodic_release to retry failed jobs htcondor-transfer ...... Transfer data via HTCondor's own mechanisms matlab-HelloWorld ...... Creating standalone MATLAB application - Hello World nelle-nemo ............. Running Nelle Nemo's goostats on the grid oasis-parrot ........... Software access with OASIS and Parrot octave ................. Matrix manipulation via the Octave programming language osg-locations .......... Tutorial based on OSG location exercise from the User School pegasus ................ An introduction to the Pegasus job workflow manager photodemo .............. A complete analysis workflow using HTTP transfer quickstart ............. How to run your first OSG job root ................... Inspect ntuples using the ROOT analysis framework scaling ................ Learn to steer jobs to particular resources scaling-up-resources ... A simple multi-job demonstration software ............... Software access tutorial stash-cvmfs ............ Shows how to use stash-cvmfs for input data transfer stash-http ............. Retrieve job input files from Stash via HTTP tensorflow-matmul ...... Tensorflow math operations as a singularity container job on the OSG - matrix multiplication","title":"Available tutorials"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-command/#install-and-setup-a-tutorial","text":"On the OSG Connect login node, create a directory, cd to it, and invoke the command: $ tutorial <tutorial-name> This command will clone the tutorial repository to your current working directory. cd to the repository directory and follow the steps described in the readme.md file. Alternatively, you can view the readme.md file at the tutorial's corresponding GitHub page.","title":"Install and setup a tutorial"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-error101/","text":"Overview Troubleshooting techniques Diagnostics with condor_q Held jobs and condor_release Overview \u00b6 In this lesson, we'll learn how to troubleshoot jobs that never start or fail in unexpected ways. Troubleshooting techniques \u00b6 Diagnostics with condor_q \u00b6 The condor_q command shows the status of the jobs and it can be used to diagnose why jobs are not running. Using the -better-analyze flag with condor_q can show you detailed information about why a job isn't starting on a specific pool. Since OSG Connect sends jobs to many places, we also need to specify a pool name with the -pool flag. Unless you know a specific pool you would like to query, checking the flock.opensciencegrid.org pool is usually a good place to start. $ condor_q -better-analyze JOB-ID -pool POOL-NAME Let's do an example. First we'll need to login as usual, and then load the tutorial error101 . $ ssh username@login.osgconnect.net $ tutorial error101 $ cd tutorial-error101 $ condor_submit error101_job.submit We'll check the job status the normal way: condor_q username For some reason, our job is still idle. Why? Try using condor_q -better-analyze to find out. Remember that you will also need to specify a pool name. In this case we'll use flock.opensciencegrid.org : $ condor_q -better-analyze JOB-ID -pool flock.opensciencegrid.org # Produces a long ouput. # The following lines are part of the output regarding the job requirements. The Requirements expression for your job reduces to these conditions: Slots Step Matched Condition ----- -------- --------- [0] 10674 TARGET.Arch == \"X86_64\" [1] 10674 TARGET.OpSys == \"LINUX\" [3] 10674 TARGET.Disk >= RequestDisk [5] 0 TARGET.Memory >= RequestMemory [8] 10674 TARGET.HasFileTransfer By looking through the match conditions, we see that many nodes match our requests for the Linux operating system and the x86_64 architecture, but none of them match our requirement for 51200 MB of memory. Let's look at our submit script and see if we can find the source of this error: $ cat error101_job.submit Universe = vanilla Executable = error101.sh # to sleep an hour Arguments = 3600 request_memory = 2 TB Error = job.err Output = job.out Log = job.log Queue 1 See the request_memory line? We are asking for 2 Terabytes of memory, when we meant to only ask for 2 Gigabytes of memory. Our job is not matching any available job slots because none of the slots offer 2 TB of memory. Let's fix that by changing that line to read request_memory = 2 GB . $ nano error101_job.submit Let's cancel our idle job with the condor_rm command and then resubmit our edited job: $ condor_rm JOB-ID $ condor_submit error101_job.submit Alternatively, you can edit the resource requirements of the idle job in queue: condor_qedit JOB_ID RequestMemory 2048 Held jobs and condor_release \u00b6 Occasionally, a job can fail in various ways and go into \"Held\" state. Held state means that the job has encountered some error, and cannot run. This doesn't necessarily mean that your job has failed, but, for whatever reason, Condor cannot fulfill your request(s). In this particular case, a user had this in his or her Condor submit file: transfer_output_files = outputfile However, when the job executed, it went into Held state: $ condor_q -analyze 372993.0 -- Submitter: login01.osgconnect.net : <192.170.227.195:56174> : login01.osgconnect.net --- 372993.000: Request is held. Hold reason: Error from glidein_9371@compute-6-28.tier2: STARTER at 10.3.11.39 failed to send file(s) to <192.170.227.195:40485>: error reading from /wntmp/condor/compute-6-28/execute/dir_9368/glide_J6I1HT/execute/dir_16393/outputfile: (errno 2) No such file or directory; SHADOW failed to receive file(s) from <192.84.86.100:50805> Let's break down this error message piece by piece: Hold reason: Error from glidein_9371@compute-6-28.tier2: STARTER at 10.3.11.39 failed to send file(s) to <192.170.227.195:40485> This part is quite cryptic, but it simply means that the worker node where your job executed (glidein_9371@compute-6-28.tier2 or 10.3.11.39) tried to transfer a file to the OSG Connect login node (192.170.227.195) but did not succeed. The next part explains why: error reading from /wntmp/condor/compute-6-28/execute/dir_9368/glide_J6I1HT/execute/dir_16393/outputfile: (errno 2) No such file or directory This bit has the full path of the file that Condor tried to transfer back to login.osgconnect.net . The reason why the file transfer failed is because outputfile was never created on the worker node. Remember that at the beginning we said that the user specifically requested transfer_outputfiles = outputfile ! Condor could not complete this request, and so the job went into Held state instead of finishing normally. It's quite possible that the error was simply transient, and if we retry, the job will succeed. We can re-queue a job that is in Held state by using condor_release : condor_release JOB-ID","title":"Troubleshooting Job Errors"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-error101/#overview","text":"In this lesson, we'll learn how to troubleshoot jobs that never start or fail in unexpected ways.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-error101/#troubleshooting-techniques","text":"","title":"Troubleshooting techniques"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-error101/#diagnostics-with-condor_q","text":"The condor_q command shows the status of the jobs and it can be used to diagnose why jobs are not running. Using the -better-analyze flag with condor_q can show you detailed information about why a job isn't starting on a specific pool. Since OSG Connect sends jobs to many places, we also need to specify a pool name with the -pool flag. Unless you know a specific pool you would like to query, checking the flock.opensciencegrid.org pool is usually a good place to start. $ condor_q -better-analyze JOB-ID -pool POOL-NAME Let's do an example. First we'll need to login as usual, and then load the tutorial error101 . $ ssh username@login.osgconnect.net $ tutorial error101 $ cd tutorial-error101 $ condor_submit error101_job.submit We'll check the job status the normal way: condor_q username For some reason, our job is still idle. Why? Try using condor_q -better-analyze to find out. Remember that you will also need to specify a pool name. In this case we'll use flock.opensciencegrid.org : $ condor_q -better-analyze JOB-ID -pool flock.opensciencegrid.org # Produces a long ouput. # The following lines are part of the output regarding the job requirements. The Requirements expression for your job reduces to these conditions: Slots Step Matched Condition ----- -------- --------- [0] 10674 TARGET.Arch == \"X86_64\" [1] 10674 TARGET.OpSys == \"LINUX\" [3] 10674 TARGET.Disk >= RequestDisk [5] 0 TARGET.Memory >= RequestMemory [8] 10674 TARGET.HasFileTransfer By looking through the match conditions, we see that many nodes match our requests for the Linux operating system and the x86_64 architecture, but none of them match our requirement for 51200 MB of memory. Let's look at our submit script and see if we can find the source of this error: $ cat error101_job.submit Universe = vanilla Executable = error101.sh # to sleep an hour Arguments = 3600 request_memory = 2 TB Error = job.err Output = job.out Log = job.log Queue 1 See the request_memory line? We are asking for 2 Terabytes of memory, when we meant to only ask for 2 Gigabytes of memory. Our job is not matching any available job slots because none of the slots offer 2 TB of memory. Let's fix that by changing that line to read request_memory = 2 GB . $ nano error101_job.submit Let's cancel our idle job with the condor_rm command and then resubmit our edited job: $ condor_rm JOB-ID $ condor_submit error101_job.submit Alternatively, you can edit the resource requirements of the idle job in queue: condor_qedit JOB_ID RequestMemory 2048","title":"Diagnostics with condor_q"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-error101/#held-jobs-and-condor_release","text":"Occasionally, a job can fail in various ways and go into \"Held\" state. Held state means that the job has encountered some error, and cannot run. This doesn't necessarily mean that your job has failed, but, for whatever reason, Condor cannot fulfill your request(s). In this particular case, a user had this in his or her Condor submit file: transfer_output_files = outputfile However, when the job executed, it went into Held state: $ condor_q -analyze 372993.0 -- Submitter: login01.osgconnect.net : <192.170.227.195:56174> : login01.osgconnect.net --- 372993.000: Request is held. Hold reason: Error from glidein_9371@compute-6-28.tier2: STARTER at 10.3.11.39 failed to send file(s) to <192.170.227.195:40485>: error reading from /wntmp/condor/compute-6-28/execute/dir_9368/glide_J6I1HT/execute/dir_16393/outputfile: (errno 2) No such file or directory; SHADOW failed to receive file(s) from <192.84.86.100:50805> Let's break down this error message piece by piece: Hold reason: Error from glidein_9371@compute-6-28.tier2: STARTER at 10.3.11.39 failed to send file(s) to <192.170.227.195:40485> This part is quite cryptic, but it simply means that the worker node where your job executed (glidein_9371@compute-6-28.tier2 or 10.3.11.39) tried to transfer a file to the OSG Connect login node (192.170.227.195) but did not succeed. The next part explains why: error reading from /wntmp/condor/compute-6-28/execute/dir_9368/glide_J6I1HT/execute/dir_16393/outputfile: (errno 2) No such file or directory This bit has the full path of the file that Condor tried to transfer back to login.osgconnect.net . The reason why the file transfer failed is because outputfile was never created on the worker node. Remember that at the beginning we said that the user specifically requested transfer_outputfiles = outputfile ! Condor could not complete this request, and so the job went into Held state instead of finishing normally. It's quite possible that the error was simply transient, and if we retry, the job will succeed. We can re-queue a job that is in Held state by using condor_release : condor_release JOB-ID","title":"Held jobs and condor_release"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-organizing/","text":"Organizing and Submitting HTC Workloads \u00b6 Imagine you have a collection of books, and you want to analyze how word usage varies from book to book or author to author. This tutorial starts with the same set up as our Wordcount Tutorial for Submitting Multiple Jobs , but focuses on how to organize that example more effectively on the Access Point, with an eye to scaling up to a larger HTC workload in the future. Our Workload \u00b6 We can analyze one book by running the wordcount.py script, with the name of the book we want to analyze: $ ./wordcount.py Alice_in_Wonderland.txt Try running the command to see what the output is for the script. Once you have done that delete the output file created ( rm counts.Alice_in_Wonderland.txt ). We want to run this script on all the books we have copies of. What is the input set for this HTC workload? What is the output set? Make an Organization Plan \u00b6 Based on what you know about the script, inputs, and outputs, how would you organize this HTC workload in directories (folders) on the access point? There will also be system and HTCondor files produced when we submit a job -- how would you organize the log, standard error and standard output files? Try making those changes before moving on to the next section of the tutorial. Organize Files \u00b6 There are many different ways to organize files; a simple example that works for most workloads is having a directory for your input files and a directory for your output files. We can set up this structure on the command line by running: $ mkdir input $ mv *.txt input/ $ mkdir output/ We can view our current directory and its subdirectories by using the recursive flag with the ls command: $ ls -R README.md books.submit input output wordcount.py ./input: Alice_in_Wonderland.txt Huckleberry_Finn.txt Ulysses.txt Dracula.txt Pride_and_Prejudice.txt ./output: We are also going to create directories for the HTCondor log files and the standard error and standard output files (in one directory): $ mkdir logs $ mkdir errout Submit One Job \u00b6 Now we want to submit a test job that uses this organizing scheme, using just one item in our input set -- in this example, we'll use the Alice_in_Wonderland.txt file from our input/ directory. The lines that need to be filled in are shown below and can be edited using the nano text editor: $ nano books.submit executable = wordcount.py arguments = Alice_in_Wonderland.txt transfer_input_files = input/Alice_in_Wonderland.txt transfer_output_files = counts.Alice_in_Wonderland.txt transfer_output_remaps = \"counts.Alice_in_Wonderland.txt=output/counts.Alice_in_Wonderland.txt\" Note that to tell HTCondor the location of the input file, we need to include the input directory. We're also using a submit file option called transfer_output_remaps that will essentially move the output file to our output/ directory by renaming or remapping it. We also want to edit the submit file lines that tell the log, error and output files where to go: $ nano books.submit output = logs/job.$(ClusterID).$(ProcID).out error = errout/job.$(ClusterID).$(ProcID).err log = errout/job.$(ClusterID).$(ProcID).log Once you've made the above changes to the books.submit file, you can submit it, and monitor its progress: $ condor_submit books.submit $ condor_watch_q (Type CTRL - C to stop the condor_watch_q command.) Submit Multiple Jobs \u00b6 We are now sufficiently organized to submit our whole workload. First, we need to create a file with our input set -- in this case, it will be a list of the book files we want to analyze. We can do this by using the shell's listing command ls and redirecting the output to a file: $ cd input $ ls > booklist.txt $ cat booklist.txt $ mv booklist.txt .. $ cd .. Then, we modify our submit file to reference this input list and replace the static values from our test job ( Alice_in_Wonderland.txt ) with a variable -- we've chosen $(book) below: $ nano books.submit executable = wordcount.py arguments = $(book) transfer_input_files = input/$(book) transfer_output_files = counts.$(book) transfer_output_remaps = \"counts.$(book)=output/counts.$(book)\" # other options queue book from booklist.txt Once this is done, you can submit the jobs as usual: $ condor_submit books.submit","title":"Organizing and Submitting HTC Workloads"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-organizing/#organizing-and-submitting-htc-workloads","text":"Imagine you have a collection of books, and you want to analyze how word usage varies from book to book or author to author. This tutorial starts with the same set up as our Wordcount Tutorial for Submitting Multiple Jobs , but focuses on how to organize that example more effectively on the Access Point, with an eye to scaling up to a larger HTC workload in the future.","title":"Organizing and Submitting HTC Workloads"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-organizing/#our-workload","text":"We can analyze one book by running the wordcount.py script, with the name of the book we want to analyze: $ ./wordcount.py Alice_in_Wonderland.txt Try running the command to see what the output is for the script. Once you have done that delete the output file created ( rm counts.Alice_in_Wonderland.txt ). We want to run this script on all the books we have copies of. What is the input set for this HTC workload? What is the output set?","title":"Our Workload"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-organizing/#make-an-organization-plan","text":"Based on what you know about the script, inputs, and outputs, how would you organize this HTC workload in directories (folders) on the access point? There will also be system and HTCondor files produced when we submit a job -- how would you organize the log, standard error and standard output files? Try making those changes before moving on to the next section of the tutorial.","title":"Make an Organization Plan"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-organizing/#organize-files","text":"There are many different ways to organize files; a simple example that works for most workloads is having a directory for your input files and a directory for your output files. We can set up this structure on the command line by running: $ mkdir input $ mv *.txt input/ $ mkdir output/ We can view our current directory and its subdirectories by using the recursive flag with the ls command: $ ls -R README.md books.submit input output wordcount.py ./input: Alice_in_Wonderland.txt Huckleberry_Finn.txt Ulysses.txt Dracula.txt Pride_and_Prejudice.txt ./output: We are also going to create directories for the HTCondor log files and the standard error and standard output files (in one directory): $ mkdir logs $ mkdir errout","title":"Organize Files"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-organizing/#submit-one-job","text":"Now we want to submit a test job that uses this organizing scheme, using just one item in our input set -- in this example, we'll use the Alice_in_Wonderland.txt file from our input/ directory. The lines that need to be filled in are shown below and can be edited using the nano text editor: $ nano books.submit executable = wordcount.py arguments = Alice_in_Wonderland.txt transfer_input_files = input/Alice_in_Wonderland.txt transfer_output_files = counts.Alice_in_Wonderland.txt transfer_output_remaps = \"counts.Alice_in_Wonderland.txt=output/counts.Alice_in_Wonderland.txt\" Note that to tell HTCondor the location of the input file, we need to include the input directory. We're also using a submit file option called transfer_output_remaps that will essentially move the output file to our output/ directory by renaming or remapping it. We also want to edit the submit file lines that tell the log, error and output files where to go: $ nano books.submit output = logs/job.$(ClusterID).$(ProcID).out error = errout/job.$(ClusterID).$(ProcID).err log = errout/job.$(ClusterID).$(ProcID).log Once you've made the above changes to the books.submit file, you can submit it, and monitor its progress: $ condor_submit books.submit $ condor_watch_q (Type CTRL - C to stop the condor_watch_q command.)","title":"Submit One Job"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-organizing/#submit-multiple-jobs","text":"We are now sufficiently organized to submit our whole workload. First, we need to create a file with our input set -- in this case, it will be a list of the book files we want to analyze. We can do this by using the shell's listing command ls and redirecting the output to a file: $ cd input $ ls > booklist.txt $ cat booklist.txt $ mv booklist.txt .. $ cd .. Then, we modify our submit file to reference this input list and replace the static values from our test job ( Alice_in_Wonderland.txt ) with a variable -- we've chosen $(book) below: $ nano books.submit executable = wordcount.py arguments = $(book) transfer_input_files = input/$(book) transfer_output_files = counts.$(book) transfer_output_remaps = \"counts.$(book)=output/counts.$(book)\" # other options queue book from booklist.txt Once this is done, you can submit the jobs as usual: $ condor_submit books.submit","title":"Submit Multiple Jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-osg-locations/","text":"Overview Gathering network information from the OSG Hostname fetching code Collating your results Mapping your results Overview \u00b6 In this section, we will learn how to quickly submit multiple jobs simultaneously using HTCondor and we will visualize where these jobs run so we can get an idea of where and jobs are distributed on the Open Science Pool. Gathering network information from the OSG \u00b6 Now to create a submit file that will run in the OSG! Use the tutorial command to download the job submission files: tutorial osg-locations . Change into the tutorial-osg-locations directory with cd tutorial-osg-locations . Hostname fetching code \u00b6 The following Python script finds the ClassAd of the machine it's running on and finds a network identity that can be used to perform lookups: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/env python import re import os import socket machine_ad_file_name = os . getenv ( '_CONDOR_MACHINE_AD' ) try : machine_ad_file = open ( machine_ad_file_name , 'r' ) machine_ad = machine_ad_file . read () machine_ad_file . close () except TypeError : print socket . getfqdn () exit ( 1 ) try : print re . search ( r 'GLIDEIN_Gatekeeper = \"(.*):\\d*/jobmanager-\\w*\"' , machine_ad , re . MULTILINE ) . group ( 1 ) except AttributeError : try : print re . search ( r 'GLIDEIN_Gatekeeper = \"(\\S+) \\S+:9619\"' , machine_ad , re . MULTILINE ) . group ( 1 ) except AttributeError : exit ( 1 ) This script ( wn-geoip.py ) is contained in the zipped archive ( wn-geoip.tar.gz ) that is transferred to the job and unpacked by the job wrapper script location-wrapper.sh . You will be using location-wrapper.sh as your executable and wn-geoip.tar.gz as an input file. The submit file for this job, scalingup.submit , is setup to specify these files and submit 100 jobs simultaneously. It also uses the job's process value to create unique output, error and log files for each of the job. $ cat scalingup.submit # The following requirments ensure we land on compute nodes # which have all the dependencies (modules, so we can # module load python2.7) and avoid some machines where # GeoIP does not work (such as Kubernetes containers) requirements = OSG_OS_STRING == \"RHEL 7\" && HAS_MODULES && GLIDEIN_Gatekeeper =!= UNDEFINED # We need the job to run our executable script, with the # input.txt filename as an argument, and to transfer the # relevant input and output files: executable = location-wrapper.sh transfer_input_files = wn-geoip.tar.gz # We can specify unique filenames for each job by using # the job's 'process' value. error = job.$(Process).error output = job.$(Process).output log = job.$(Process).log # The below are good base requirements for first testing jobs on OSG, # if you don't have a good idea of memory and disk usage. request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Queue 100 jobs with the above specifications. queue 100 Submit this job using the condor_submit command: $ condor_submit scalingup.submit Wait for the results. Remember, you can use watch condor_q to monitor the status of your jobs. Collating your results \u00b6 Now that you have your results, it's time to summarize them. Rather than inspecting each output file individually, you can use the cat command to print the results from all of your output files at once. If all of your output files have the format job.#.output (e.g., job.10.output ), your command will look something like this: $ cat job.*.output The * is a wildcard so the above cat command runs on all files that start with job- and end in .output . Additionally, you can use cat in combination with the sort and uniq commands to print only the unique results: $ cat job.*.output | sort | uniq Mapping your results \u00b6 To visualize the locations of the machines that your jobs ran on, you will be using http://www.mapcustomizer.com/. Copy and paste the collated results into the text box that pops up when clicking on the 'Bulk Entry' button on the right-hand side. Where did your jobs run?","title":"Finding OSG Locations"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-osg-locations/#overview","text":"In this section, we will learn how to quickly submit multiple jobs simultaneously using HTCondor and we will visualize where these jobs run so we can get an idea of where and jobs are distributed on the Open Science Pool.","title":"Overview"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-osg-locations/#gathering-network-information-from-the-osg","text":"Now to create a submit file that will run in the OSG! Use the tutorial command to download the job submission files: tutorial osg-locations . Change into the tutorial-osg-locations directory with cd tutorial-osg-locations .","title":"Gathering network information from the OSG"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-osg-locations/#hostname-fetching-code","text":"The following Python script finds the ClassAd of the machine it's running on and finds a network identity that can be used to perform lookups: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/env python import re import os import socket machine_ad_file_name = os . getenv ( '_CONDOR_MACHINE_AD' ) try : machine_ad_file = open ( machine_ad_file_name , 'r' ) machine_ad = machine_ad_file . read () machine_ad_file . close () except TypeError : print socket . getfqdn () exit ( 1 ) try : print re . search ( r 'GLIDEIN_Gatekeeper = \"(.*):\\d*/jobmanager-\\w*\"' , machine_ad , re . MULTILINE ) . group ( 1 ) except AttributeError : try : print re . search ( r 'GLIDEIN_Gatekeeper = \"(\\S+) \\S+:9619\"' , machine_ad , re . MULTILINE ) . group ( 1 ) except AttributeError : exit ( 1 ) This script ( wn-geoip.py ) is contained in the zipped archive ( wn-geoip.tar.gz ) that is transferred to the job and unpacked by the job wrapper script location-wrapper.sh . You will be using location-wrapper.sh as your executable and wn-geoip.tar.gz as an input file. The submit file for this job, scalingup.submit , is setup to specify these files and submit 100 jobs simultaneously. It also uses the job's process value to create unique output, error and log files for each of the job. $ cat scalingup.submit # The following requirments ensure we land on compute nodes # which have all the dependencies (modules, so we can # module load python2.7) and avoid some machines where # GeoIP does not work (such as Kubernetes containers) requirements = OSG_OS_STRING == \"RHEL 7\" && HAS_MODULES && GLIDEIN_Gatekeeper =!= UNDEFINED # We need the job to run our executable script, with the # input.txt filename as an argument, and to transfer the # relevant input and output files: executable = location-wrapper.sh transfer_input_files = wn-geoip.tar.gz # We can specify unique filenames for each job by using # the job's 'process' value. error = job.$(Process).error output = job.$(Process).output log = job.$(Process).log # The below are good base requirements for first testing jobs on OSG, # if you don't have a good idea of memory and disk usage. request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Queue 100 jobs with the above specifications. queue 100 Submit this job using the condor_submit command: $ condor_submit scalingup.submit Wait for the results. Remember, you can use watch condor_q to monitor the status of your jobs.","title":"Hostname fetching code"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-osg-locations/#collating-your-results","text":"Now that you have your results, it's time to summarize them. Rather than inspecting each output file individually, you can use the cat command to print the results from all of your output files at once. If all of your output files have the format job.#.output (e.g., job.10.output ), your command will look something like this: $ cat job.*.output The * is a wildcard so the above cat command runs on all files that start with job- and end in .output . Additionally, you can use cat in combination with the sort and uniq commands to print only the unique results: $ cat job.*.output | sort | uniq","title":"Collating your results"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-osg-locations/#mapping-your-results","text":"To visualize the locations of the machines that your jobs ran on, you will be using http://www.mapcustomizer.com/. Copy and paste the collated results into the text box that pops up when clicking on the 'Bulk Entry' button on the right-hand side. Where did your jobs run?","title":"Mapping your results"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/","text":"Login to OSG Connect Pretyped setup Manual setup Tutorial jobs Job 1: A simple, nonparallel job Run the job locally Create an HTCondor submit file More about projects Submit the job Check the job status Job history Check the job output Job 2: Passing arguments to executables Job 3: Submitting jobs concurrently Where did jobs run? Removing jobs What's next? Login to OSG Connect \u00b6 If you have not already registered for OSG Connect, go to the registration site and follow the instructions there. Once registered, you will be assigned a login node which you can use for the rest of this tutorial. Pretyped setup \u00b6 To save some typing, you can install the tutorial into your home directory on the login node. This is highly recommended to ensure that you don't encounter transcription errors during the tutorials. $ tutorial usage: tutorial name-of-tutorial tutorial info name-of-tutorial Available tutorials: quickstart ..... How to run your first OSG job Now, run the quickstart tutorial: $ tutorial quickstart $ cd tutorial-quickstart Manual setup \u00b6 Alternatively, if you want the full manual experience, create a new directory for the tutorial work: $ mkdir tutorial-quickstart $ cd tutorial-quickstart Tutorial jobs \u00b6 Job 1: A simple, nonparallel job \u00b6 Inside the tutorial directory that you created or installed previously, let's create a test script to execute as your job. For pretyped setup, this is the short.sh file: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash # short.sh: a short discovery job printf \"Start time: \" ; /bin/date printf \"Job is running on node: \" ; /bin/hostname printf \"Job running as user: \" ; /usr/bin/id printf \"Job is running in directory: \" ; /bin/pwd echo echo \"Working hard...\" sleep 20 echo \"Science complete!\" Now, make the script executable. chmod +x short.sh Run the job locally \u00b6 When setting up a new job submission, it's important to test your job outside of HTCondor before submitting into the Open Science Pool. $ ./short.sh Start time: Wed Aug 21 09:21:35 CDT 2013 Job is running on node: loginNN.osgconnect.net Job running as user: uid=54161(netid) gid=1000(users) groups=1000(users),0(root),1001(osg-connect),1002(osg-staff),1003(osg-connect-test),9948(staff),19012(osgconnect) Job is running in directory: /home/netid/quickstart Working hard... Science complete! Create an HTCondor submit file \u00b6 So far, so good! Let's create a simple (if verbose) HTCondor submit file. This can be found in tutorial01.submit . # Our executable is the main program or script that we've created # to do the 'work' of a single job. executable = short.sh # We need to name the files that HTCondor should create to save the # terminal output (stdout) and error (stderr) created by our job. # Similarly, we need to name the log file where HTCondor will save # information about job execution steps. error = short.error output = short.output log = short.log # We need to request the resources that this job will need: request_cpus = 1 request_memory = 1 MB request_disk = 1 MB # The last line of a submit file indicates how many jobs of the above # description should be queued. We'll start with one job. queue 1 More about projects \u00b6 You can join projects after you login at https://osgconnect.net/ . Within minutes of joining and being approved for a project, you will have access via condor_submit as well. For more information on creating a project, please see this page You have two ways to set the project name for your jobs: Add the +ProjectName = \"MyProject\" line to the HTCondor submit file. Remember to quote the project name! Set the default project with the command connect project . If you do not set a project name, or you use a project that you're not a member of, then your job submission will fail. Submit the job \u00b6 Submit the job using condor_submit : $ condor_submit tutorial01.submit Submitting job(s). 1 job(s) submitted to cluster 144121. Check the job status \u00b6 The condor_q command tells the status of currently running jobs. Generally you will want to limit it to your own jobs: $ condor_q netid -- Schedd: loginNN.osgconnect.net : <192.170.227.22:9618?... @ 12/10/18 14:19:08 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS netid ID: 1441271 12/10 14:18 _ 1 _ 1 1441271.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for netid: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 3001 jobs; 0 completed, 0 removed, 2189 idle, 754 running, 58 held, 0 suspended You can also get status on a specific job cluster: $ condor_q 1441271 -- Schedd: loginNN.osgconnect.net : <192.170.227.22:9618?... @ 12/10/18 14:19:08 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS netid ID: 1441271 12/10 14:18 _ 1 _ 1 1441271.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for netid: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 3001 jobs; 0 completed, 0 removed, 2189 idle, 754 running, 58 held, 0 suspended Note the DONE , RUN , and IDLE columns. Your job will be listed in the IDLE column if it hasn't started yet. If it's currently scheduled and running, it will appear in the RUN column. As it finishes up, it will then show in the DONE column. Once the job completes completely, it will not appear in condor_q . Let's wait for your job to finish \u2013 that is, for condor_q not to show the job in its output. A useful tool for this is watch \u2013 it runs a program repeatedly, letting you see how the output differs at fixed time intervals. Let's submit the job again, and watch condor_q output at two-second intervals: $ condor_submit tutorial01.submit Submitting job(s). 1 job(s) submitted to cluster 1441272 $ watch -n2 condor_q netid ... When your job has completed, it will disappear from the list. Note : To close watch, hold down Ctrl and press C. Job history \u00b6 Once your job has finished, you can get information about its execution from the condor_history command: $ condor_history 1441272 ID OWNER SUBMITTED RUN_TIME ST COMPLETED CMD 1441272.0 netid 12/10 14:18 0+00:00:29 C 12/10 14:19 /home/netid/tutorial-quickstart/short.sh Note : You can see much more information about your job's final status using the -long option. Check the job output \u00b6 Once your job has finished, you can look at the files that HTCondor has returned to the working directory. The names of these files were specified in our submit file. If everything was successful, it should have returned: a log file from HTCondor for the job cluster: short.log an output file for each job's output: short.output an error file for each job's errors: short.error Read the output file. It should be something like this: $ cat short.output Start time: Mon Dec 10 20:18:56 UTC 2018 Job is running on node: osg-84086-0-cmswn2030.fnal.gov Job running as user: uid=12740(osg) gid=9652(osg) groups=9652(osg) Job is running in directory: /srv Working hard... Science complete! Job 2: Passing arguments to executables \u00b6 Sometimes it's useful to pass arguments to your executable from your submit file. For example, you might want to use the same job script for more than one run, varying only the parameters. You can do that by adding Arguments to your submission file. First, let's edit our existing short.sh script to accept arguments. To avoid losing our original script, we make a copy of the file under the name short_transfer.sh $ cp short.sh short_transfer.sh Now, edit the file to include the added lines below: 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash # short.sh: a short discovery job printf \"Start time: \" ; /bin/date printf \"Job is running on node: \" ; /bin/hostname printf \"Job running as user: \" ; /usr/bin/id printf \"Job is running in directory: \" ; /bin/pwd printf \"The command line argument is: \" ; $1 printf \"Contents of $1 is \" ; cat $1 cat $1 > output.txt printf \"Working hard...\" ls -l $PWD sleep 20 echo \"Science complete!\" We need to make our new script executable just as we did before: $ chmod +x short_transfer.sh Notice that with our changes, the new script will now print out the contents of whatever file we specify in our arguments, specified by the $1 . It will also copy the contents of that file into another file called output.txt . Make a simple text file called input.txt that we can pass to our script: \"Hello World\" Once again, before submitting our job we should test it locally to ensure it runs as we expect: $ ./short_transfer.sh input.txt Start time: Tue Dec 11 10:19:12 CST 2018 Job is running on node: loginNN.osgconnect.net Job running as user: uid=100279(netid) gid=1000(users) groups=1000(users),5532(connect),5782(osg),7021(osg.ConnectTrain) Job is running in directory: /home/netid/tutorial-quickstart The command line argument is: Contents of input.txt is \"Hello World\"Working hard...total 28 drwxrwxr-x 2 netid users 34 Oct 15 09:37 Images -rw-rw-r-- 1 netid users 13 Oct 15 09:37 input.txt drwxrwxr-x 2 netid users 114 Dec 11 09:50 log -rw-r--r-- 1 netid users 13 Dec 11 10:19 output.txt -rwxrwxr-x 1 netid users 291 Oct 15 09:37 short.sh -rwxrwxr-x 1 netid users 390 Dec 11 10:18 short_transfer.sh -rw-rw-r-- 1 netid users 806 Oct 15 09:37 tutorial01.submit -rw-rw-r-- 1 netid users 547 Dec 11 09:49 tutorial02.submit -rw-rw-r-- 1 netid users 1321 Oct 15 09:37 tutorial03.submit Science complete! Now, let's edit our submit file to properly handle these new arguments and output files and save this as tutorial02.submit # We need the job to run our executable script, with the # input.txt filename as an argument, and to transfer the # relevant input and output files: executable = short_transfer.sh arguments = input.txt transfer_input_files = input.txt transfer_output_files = output.txt error = job.error output = job.output log = job.log # The below are good base requirements for first testing jobs on OSG, # if you don't have a good idea of memory and disk usage. request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Queue one job with the above specifications. queue 1 Notice the added arguments = input.txt information. The arguements option specifies what arguments should be passed to the executable. The transfer_input_files and transfer_output_files options need to be included as well. When jobs are executed on the Open Science Pool via HTCondor, they are sent only with files that are specified. Additionally, only the specified output files are returned with the job. Any output not transferred back, with the exception of our error , output , and log files, are discarded at the end of the job. Submit the new submit file using condor_submit . Be sure to check your output files once the job completes. $ condor_submit tutorial02.submit Submitting job(s). 1 job(s) submitted to cluster 1444781. Job 3: Submitting jobs concurrently \u00b6 What do we need to do to submit several jobs simultaneously? In the first example, Condor returned three files: out, error, and log. If we want to submit several jobs, we need to track these three files for each job. An easy way to do this is to add the $(Cluster) and $(Process) macros to the HTCondor submit file. Since this can make our working directory really messy with a large number of jobs, let's tell HTCondor to put the files in a directory called log. Here's what the third submit file looks like, called tutorial03.submit : # We need the job to run our executable script, arguments and files. # Also, we'll specify unique filenames for each job by using # the job's 'cluster' value. executable = short_transfer.sh arguments = input.txt transfer_input_files = input.txt transfer_output_files = output.txt error = log/job.$(Cluster).$(Process)error output = log/job.$(Cluster).$(Process).output log = log/job.$(Cluster).$(Process).log request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Let's queue ten jobs with the above specifications queue 10 Before submitting, we also need to make sure the log directory exists. $ mkdir -p log You'll see something like the following upon submission: $ condor_submit tutorial03.submit Submitting job(s).......... 10 job(s) submitted to cluster 1444786. Look at the output files in the log directory and notice how each job received its own separate output file: $ ls ./log job.1444786.0.error job.1444786.1.error job.1444786.2.error job.1444786.3.error job.1444786.4.error job.1444786.5.error job.1444786.6.error job.1444786.7.error job.1444786.8.error job.1444786.9.error job.1444786.0.log job.1444786.1.log job.1444786.2.log job.1444786.3.log job.1444786.4.log job.1444786.5.log job.1444786.6.log job.1444786.7.log job.1444786.8.log job.1444786.9.log job.1444786.0.output job.1444786.1.output job.1444786.2.output job.1444786.3.output job.1444786.4.output job.1444786.5.output job.1444786.6.output job.1444786.7.output job.1444786.8.output job.1444786.9.output Where did jobs run? \u00b6 It might be interesting to see where our jobs actually ran. To get that information for a single job, we can use the command condor_history . First, select a job you want to investigate and then run condor_history -long jobid . In this example, we will investigate the job ID 1444786.9. Again the output is quite long: $ condor_history -limit 1 -long 1444786.9 BlockWriteKbytes = 0 BlockReads = 0 DiskUsage_RAW = 36 ... MATCH_EXP_JOBGLIDEIN_ResourceName = \"Georgia_Tech_PACE_CE_2\" ... Looking through here for a hostname, we can see that the parameter MATCH_EXP_JOBGLIDEIN_ResourceName . The output of this parameter is the slot our job ran on. In this example, our job ran on a slot at the Georgia Institute of Technology. To visualize where multiple jobs have run, visit our Finding OSG Locations tutorial. Removing jobs \u00b6 On occasion, jobs will need to be removed for a variety of reasons (incorrect parameters, errors in submission, etc.). In these instances, the condor_rm command can be used to remove an entire job submission or just particular jobs in a submission. The condor_rm command accepts a cluster id, a job id, or username and will remove an entire cluster of jobs, a single job, or all the jobs belonging to a given user respectively. E.g. if a job submission generates 100 jobs and is assigned a cluster id of 103, then condor_rm 103.0 will remove the first job in the cluster. Likewise, condor_rm 103 will remove all the jobs in the job submission and condor_rm [username] will remove all jobs belonging to the user. The condor_rm documenation has more details on using condor_rm including ways to remove jobs based on other constraints. What's next? \u00b6 We recommend you read about how to steer your jobs with HTCondor job requirements - this will allow you to select good resources for your workload. Please see this page Watch this video from the 2021 OSG Virtual School for an introduction to HTC Job Execution with HTCondor:","title":"Quickstart-Submit Example HTCondor Jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#login-to-osg-connect","text":"If you have not already registered for OSG Connect, go to the registration site and follow the instructions there. Once registered, you will be assigned a login node which you can use for the rest of this tutorial.","title":"Login to OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#pretyped-setup","text":"To save some typing, you can install the tutorial into your home directory on the login node. This is highly recommended to ensure that you don't encounter transcription errors during the tutorials. $ tutorial usage: tutorial name-of-tutorial tutorial info name-of-tutorial Available tutorials: quickstart ..... How to run your first OSG job Now, run the quickstart tutorial: $ tutorial quickstart $ cd tutorial-quickstart","title":"Pretyped setup"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#manual-setup","text":"Alternatively, if you want the full manual experience, create a new directory for the tutorial work: $ mkdir tutorial-quickstart $ cd tutorial-quickstart","title":"Manual setup"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#tutorial-jobs","text":"","title":"Tutorial jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#job-1-a-simple-nonparallel-job","text":"Inside the tutorial directory that you created or installed previously, let's create a test script to execute as your job. For pretyped setup, this is the short.sh file: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash # short.sh: a short discovery job printf \"Start time: \" ; /bin/date printf \"Job is running on node: \" ; /bin/hostname printf \"Job running as user: \" ; /usr/bin/id printf \"Job is running in directory: \" ; /bin/pwd echo echo \"Working hard...\" sleep 20 echo \"Science complete!\" Now, make the script executable. chmod +x short.sh","title":"Job 1: A simple, nonparallel job"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#run-the-job-locally","text":"When setting up a new job submission, it's important to test your job outside of HTCondor before submitting into the Open Science Pool. $ ./short.sh Start time: Wed Aug 21 09:21:35 CDT 2013 Job is running on node: loginNN.osgconnect.net Job running as user: uid=54161(netid) gid=1000(users) groups=1000(users),0(root),1001(osg-connect),1002(osg-staff),1003(osg-connect-test),9948(staff),19012(osgconnect) Job is running in directory: /home/netid/quickstart Working hard... Science complete!","title":"Run the job locally"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#create-an-htcondor-submit-file","text":"So far, so good! Let's create a simple (if verbose) HTCondor submit file. This can be found in tutorial01.submit . # Our executable is the main program or script that we've created # to do the 'work' of a single job. executable = short.sh # We need to name the files that HTCondor should create to save the # terminal output (stdout) and error (stderr) created by our job. # Similarly, we need to name the log file where HTCondor will save # information about job execution steps. error = short.error output = short.output log = short.log # We need to request the resources that this job will need: request_cpus = 1 request_memory = 1 MB request_disk = 1 MB # The last line of a submit file indicates how many jobs of the above # description should be queued. We'll start with one job. queue 1","title":"Create an HTCondor submit file"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#more-about-projects","text":"You can join projects after you login at https://osgconnect.net/ . Within minutes of joining and being approved for a project, you will have access via condor_submit as well. For more information on creating a project, please see this page You have two ways to set the project name for your jobs: Add the +ProjectName = \"MyProject\" line to the HTCondor submit file. Remember to quote the project name! Set the default project with the command connect project . If you do not set a project name, or you use a project that you're not a member of, then your job submission will fail.","title":"More about projects"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#submit-the-job","text":"Submit the job using condor_submit : $ condor_submit tutorial01.submit Submitting job(s). 1 job(s) submitted to cluster 144121.","title":"Submit the job"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#check-the-job-status","text":"The condor_q command tells the status of currently running jobs. Generally you will want to limit it to your own jobs: $ condor_q netid -- Schedd: loginNN.osgconnect.net : <192.170.227.22:9618?... @ 12/10/18 14:19:08 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS netid ID: 1441271 12/10 14:18 _ 1 _ 1 1441271.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for netid: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 3001 jobs; 0 completed, 0 removed, 2189 idle, 754 running, 58 held, 0 suspended You can also get status on a specific job cluster: $ condor_q 1441271 -- Schedd: loginNN.osgconnect.net : <192.170.227.22:9618?... @ 12/10/18 14:19:08 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS netid ID: 1441271 12/10 14:18 _ 1 _ 1 1441271.0 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for netid: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 3001 jobs; 0 completed, 0 removed, 2189 idle, 754 running, 58 held, 0 suspended Note the DONE , RUN , and IDLE columns. Your job will be listed in the IDLE column if it hasn't started yet. If it's currently scheduled and running, it will appear in the RUN column. As it finishes up, it will then show in the DONE column. Once the job completes completely, it will not appear in condor_q . Let's wait for your job to finish \u2013 that is, for condor_q not to show the job in its output. A useful tool for this is watch \u2013 it runs a program repeatedly, letting you see how the output differs at fixed time intervals. Let's submit the job again, and watch condor_q output at two-second intervals: $ condor_submit tutorial01.submit Submitting job(s). 1 job(s) submitted to cluster 1441272 $ watch -n2 condor_q netid ... When your job has completed, it will disappear from the list. Note : To close watch, hold down Ctrl and press C.","title":"Check the job status"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#job-history","text":"Once your job has finished, you can get information about its execution from the condor_history command: $ condor_history 1441272 ID OWNER SUBMITTED RUN_TIME ST COMPLETED CMD 1441272.0 netid 12/10 14:18 0+00:00:29 C 12/10 14:19 /home/netid/tutorial-quickstart/short.sh Note : You can see much more information about your job's final status using the -long option.","title":"Job history"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#check-the-job-output","text":"Once your job has finished, you can look at the files that HTCondor has returned to the working directory. The names of these files were specified in our submit file. If everything was successful, it should have returned: a log file from HTCondor for the job cluster: short.log an output file for each job's output: short.output an error file for each job's errors: short.error Read the output file. It should be something like this: $ cat short.output Start time: Mon Dec 10 20:18:56 UTC 2018 Job is running on node: osg-84086-0-cmswn2030.fnal.gov Job running as user: uid=12740(osg) gid=9652(osg) groups=9652(osg) Job is running in directory: /srv Working hard... Science complete!","title":"Check the job output"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#job-2-passing-arguments-to-executables","text":"Sometimes it's useful to pass arguments to your executable from your submit file. For example, you might want to use the same job script for more than one run, varying only the parameters. You can do that by adding Arguments to your submission file. First, let's edit our existing short.sh script to accept arguments. To avoid losing our original script, we make a copy of the file under the name short_transfer.sh $ cp short.sh short_transfer.sh Now, edit the file to include the added lines below: 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash # short.sh: a short discovery job printf \"Start time: \" ; /bin/date printf \"Job is running on node: \" ; /bin/hostname printf \"Job running as user: \" ; /usr/bin/id printf \"Job is running in directory: \" ; /bin/pwd printf \"The command line argument is: \" ; $1 printf \"Contents of $1 is \" ; cat $1 cat $1 > output.txt printf \"Working hard...\" ls -l $PWD sleep 20 echo \"Science complete!\" We need to make our new script executable just as we did before: $ chmod +x short_transfer.sh Notice that with our changes, the new script will now print out the contents of whatever file we specify in our arguments, specified by the $1 . It will also copy the contents of that file into another file called output.txt . Make a simple text file called input.txt that we can pass to our script: \"Hello World\" Once again, before submitting our job we should test it locally to ensure it runs as we expect: $ ./short_transfer.sh input.txt Start time: Tue Dec 11 10:19:12 CST 2018 Job is running on node: loginNN.osgconnect.net Job running as user: uid=100279(netid) gid=1000(users) groups=1000(users),5532(connect),5782(osg),7021(osg.ConnectTrain) Job is running in directory: /home/netid/tutorial-quickstart The command line argument is: Contents of input.txt is \"Hello World\"Working hard...total 28 drwxrwxr-x 2 netid users 34 Oct 15 09:37 Images -rw-rw-r-- 1 netid users 13 Oct 15 09:37 input.txt drwxrwxr-x 2 netid users 114 Dec 11 09:50 log -rw-r--r-- 1 netid users 13 Dec 11 10:19 output.txt -rwxrwxr-x 1 netid users 291 Oct 15 09:37 short.sh -rwxrwxr-x 1 netid users 390 Dec 11 10:18 short_transfer.sh -rw-rw-r-- 1 netid users 806 Oct 15 09:37 tutorial01.submit -rw-rw-r-- 1 netid users 547 Dec 11 09:49 tutorial02.submit -rw-rw-r-- 1 netid users 1321 Oct 15 09:37 tutorial03.submit Science complete! Now, let's edit our submit file to properly handle these new arguments and output files and save this as tutorial02.submit # We need the job to run our executable script, with the # input.txt filename as an argument, and to transfer the # relevant input and output files: executable = short_transfer.sh arguments = input.txt transfer_input_files = input.txt transfer_output_files = output.txt error = job.error output = job.output log = job.log # The below are good base requirements for first testing jobs on OSG, # if you don't have a good idea of memory and disk usage. request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Queue one job with the above specifications. queue 1 Notice the added arguments = input.txt information. The arguements option specifies what arguments should be passed to the executable. The transfer_input_files and transfer_output_files options need to be included as well. When jobs are executed on the Open Science Pool via HTCondor, they are sent only with files that are specified. Additionally, only the specified output files are returned with the job. Any output not transferred back, with the exception of our error , output , and log files, are discarded at the end of the job. Submit the new submit file using condor_submit . Be sure to check your output files once the job completes. $ condor_submit tutorial02.submit Submitting job(s). 1 job(s) submitted to cluster 1444781.","title":"Job 2: Passing arguments to executables"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#job-3-submitting-jobs-concurrently","text":"What do we need to do to submit several jobs simultaneously? In the first example, Condor returned three files: out, error, and log. If we want to submit several jobs, we need to track these three files for each job. An easy way to do this is to add the $(Cluster) and $(Process) macros to the HTCondor submit file. Since this can make our working directory really messy with a large number of jobs, let's tell HTCondor to put the files in a directory called log. Here's what the third submit file looks like, called tutorial03.submit : # We need the job to run our executable script, arguments and files. # Also, we'll specify unique filenames for each job by using # the job's 'cluster' value. executable = short_transfer.sh arguments = input.txt transfer_input_files = input.txt transfer_output_files = output.txt error = log/job.$(Cluster).$(Process)error output = log/job.$(Cluster).$(Process).output log = log/job.$(Cluster).$(Process).log request_cpus = 1 request_memory = 1 GB request_disk = 1 GB # Let's queue ten jobs with the above specifications queue 10 Before submitting, we also need to make sure the log directory exists. $ mkdir -p log You'll see something like the following upon submission: $ condor_submit tutorial03.submit Submitting job(s).......... 10 job(s) submitted to cluster 1444786. Look at the output files in the log directory and notice how each job received its own separate output file: $ ls ./log job.1444786.0.error job.1444786.1.error job.1444786.2.error job.1444786.3.error job.1444786.4.error job.1444786.5.error job.1444786.6.error job.1444786.7.error job.1444786.8.error job.1444786.9.error job.1444786.0.log job.1444786.1.log job.1444786.2.log job.1444786.3.log job.1444786.4.log job.1444786.5.log job.1444786.6.log job.1444786.7.log job.1444786.8.log job.1444786.9.log job.1444786.0.output job.1444786.1.output job.1444786.2.output job.1444786.3.output job.1444786.4.output job.1444786.5.output job.1444786.6.output job.1444786.7.output job.1444786.8.output job.1444786.9.output","title":"Job 3: Submitting jobs concurrently"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#where-did-jobs-run","text":"It might be interesting to see where our jobs actually ran. To get that information for a single job, we can use the command condor_history . First, select a job you want to investigate and then run condor_history -long jobid . In this example, we will investigate the job ID 1444786.9. Again the output is quite long: $ condor_history -limit 1 -long 1444786.9 BlockWriteKbytes = 0 BlockReads = 0 DiskUsage_RAW = 36 ... MATCH_EXP_JOBGLIDEIN_ResourceName = \"Georgia_Tech_PACE_CE_2\" ... Looking through here for a hostname, we can see that the parameter MATCH_EXP_JOBGLIDEIN_ResourceName . The output of this parameter is the slot our job ran on. In this example, our job ran on a slot at the Georgia Institute of Technology. To visualize where multiple jobs have run, visit our Finding OSG Locations tutorial.","title":"Where did jobs run?"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#removing-jobs","text":"On occasion, jobs will need to be removed for a variety of reasons (incorrect parameters, errors in submission, etc.). In these instances, the condor_rm command can be used to remove an entire job submission or just particular jobs in a submission. The condor_rm command accepts a cluster id, a job id, or username and will remove an entire cluster of jobs, a single job, or all the jobs belonging to a given user respectively. E.g. if a job submission generates 100 jobs and is assigned a cluster id of 103, then condor_rm 103.0 will remove the first job in the cluster. Likewise, condor_rm 103 will remove all the jobs in the job submission and condor_rm [username] will remove all jobs belonging to the user. The condor_rm documenation has more details on using condor_rm including ways to remove jobs based on other constraints.","title":"Removing jobs"},{"location":"managing_htc_workloads_on_osg_connect/submitting_htc_workloads_with_htcondor/tutorial-quickstart/#whats-next","text":"We recommend you read about how to steer your jobs with HTCondor job requirements - this will allow you to select good resources for your workload. Please see this page Watch this video from the 2021 OSG Virtual School for an introduction to HTC Job Execution with HTCondor:","title":"What's next?"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/available-containers-list/","text":"View Existing OSPool-Supported Containers \u00b6 This is list of commonly used containers in the Open Science Pool. These can be used directly in your jobs or as base images if you want to define your own. Please see the pages on container overview and creating containers for detailed instructions on how to use containers and/or have Docker containers added to the OSPool's approved list. Also note that this list is not complete. There are many images under /cvmfs/singularity.opensciencegrid.org/ which are either project specific or not described well enough to make this list. Base \u00b6 Name CVMFS Locations Description EL 6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6:latest Enterprise Linux (CentOS) 6 base image Project Website Container Definition EL 7 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest Enterprise Linux (CentOS) 7 base image Project Website Container Definition EL 8 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest Enterprise Linux (CentOS) 8 base image Project Website Container Definition Ubuntu 16.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest Ubuntu 16.04 (Xenial) base image Project Website Container Definition Ubuntu 18.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-18.04:latest Ubuntu 18.04 (Bionic) base image Project Website Container Definition Ubuntu 20.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest Ubuntu 20.04 (Focal) base image Project Website Container Definition Languages \u00b6 Name CVMFS Locations Description Julia /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:latest Ubuntu based image with Julia Project Website Container Definition Matlab Runtime /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2018b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020b This is the Matlab runtime component you can use to execute compiled Matlab codes Project Website Container Definition R /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:4.0.2 Example for building R images Project Website Container Definition Project \u00b6 Name CVMFS Locations Description XENONnT /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.06 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.25 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:development /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:latest /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:py38 Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition XENONnT /cvmfs/singularity.opensciencegrid.org/xenonnt/osg_dev:latest Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition XENONnT /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.06 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.25 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:development /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:latest /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:py38 Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition XENONnT /cvmfs/singularity.opensciencegrid.org/xenonnt/montecarlo:development /cvmfs/singularity.opensciencegrid.org/xenonnt/montecarlo:latest Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition Tools \u00b6 Name CVMFS Locations Description FreeSurfer /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:7.0.0 A software package for the analysis and visualization of structural and functional neuroimaging data from cross-sectional or longitudinal studies Project Website Container Definition GROMACS /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:latest A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. Project Website Container Definition GROMACS GPU /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs-gpu:latest A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a GPU enabled version. Project Website Container Definition Quantum Espresso /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-quantum-espresso:6.6 A suite for first-principles electronic-structure calculations and materials modeling Project Website Container Definition TensorFlow /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest TensorFlow image (CPU only) Project Website Container Definition TensorFlow GPU /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.2-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:latest TensorFlow image with GPU support Project Website Container Definition","title":"View Existing OSPool-Supported Containers"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/available-containers-list/#view-existing-ospool-supported-containers","text":"This is list of commonly used containers in the Open Science Pool. These can be used directly in your jobs or as base images if you want to define your own. Please see the pages on container overview and creating containers for detailed instructions on how to use containers and/or have Docker containers added to the OSPool's approved list. Also note that this list is not complete. There are many images under /cvmfs/singularity.opensciencegrid.org/ which are either project specific or not described well enough to make this list.","title":"View Existing OSPool-Supported Containers"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/available-containers-list/#base","text":"Name CVMFS Locations Description EL 6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6:latest Enterprise Linux (CentOS) 6 base image Project Website Container Definition EL 7 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest Enterprise Linux (CentOS) 7 base image Project Website Container Definition EL 8 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest Enterprise Linux (CentOS) 8 base image Project Website Container Definition Ubuntu 16.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest Ubuntu 16.04 (Xenial) base image Project Website Container Definition Ubuntu 18.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-18.04:latest Ubuntu 18.04 (Bionic) base image Project Website Container Definition Ubuntu 20.04 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest Ubuntu 20.04 (Focal) base image Project Website Container Definition","title":"Base"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/available-containers-list/#languages","text":"Name CVMFS Locations Description Julia /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-julia:latest Ubuntu based image with Julia Project Website Container Definition Matlab Runtime /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2018b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2019b /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020a /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-matlab-runtime:R2020b This is the Matlab runtime component you can use to execute compiled Matlab codes Project Website Container Definition R /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:4.0.2 Example for building R images Project Website Container Definition","title":"Languages"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/available-containers-list/#project","text":"Name CVMFS Locations Description XENONnT /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.06 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:2020.11.25 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:development /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:latest /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-xenon:py38 Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition XENONnT /cvmfs/singularity.opensciencegrid.org/xenonnt/osg_dev:latest Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition XENONnT /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.06 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:2020.11.25 /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:development /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:latest /cvmfs/singularity.opensciencegrid.org/xenonnt/base-environment:py38 Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition XENONnT /cvmfs/singularity.opensciencegrid.org/xenonnt/montecarlo:development /cvmfs/singularity.opensciencegrid.org/xenonnt/montecarlo:latest Base software environment for XENONnT, including Python 3.6 and data management tools Project Website Container Definition","title":"Project"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/available-containers-list/#tools","text":"Name CVMFS Locations Description FreeSurfer /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.0 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:6.0.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:7.0.0 A software package for the analysis and visualization of structural and functional neuroimaging data from cross-sectional or longitudinal studies Project Website Container Definition GROMACS /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs:latest A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. Project Website Container Definition GROMACS GPU /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-gromacs-gpu:latest A versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. This is a GPU enabled version. Project Website Container Definition Quantum Espresso /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-quantum-espresso:6.6 A suite for first-principles electronic-structure calculations and materials modeling Project Website Container Definition TensorFlow /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest TensorFlow image (CPU only) Project Website Container Definition TensorFlow GPU /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.2-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3-cuda-10.1 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:latest TensorFlow image with GPU support Project Website Container Definition","title":"Tools"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/","text":"Compiling Software for OSG Connect \u00b6 Compiling Software for OSG Connect \u00b6 Introduction \u00b6 Due to the distributed nature of the Open Science Pool, you will always need to ensure that your jobs have access to the software that will be executed. This guide provides useful information for compiling and using your software in OSG Connect. A detailed example of performing software compilation is additionally available at OSG Connect Example Compilation Guide . What is compiling? The process of compiling converts human readable code into binary, machine readable code that will execute the steps of the program. Get software source code \u00b6 The first step to compiling your software is to locate and download the source code, being sure to select the version that you want. Source code will often be made available as a compressed tar archive which will need to be extracted for before compilation. You should also carefully review the installation instructions provided by the software developers. The installation instructions should include important information regarding various options for configuring and performing the compilation. Also carefully note any system dependencies (hardware, other software, and libraries) that are required for your software. Select the appropriate compiler and compilation options \u00b6 A compiler is a program that is used to peform source code compilation. The GNU Compiler Collection (GCC) is a common, open source collection of compilers with support for C, C++, fotran, and other languages, and includes important libraries for supporting your compilation and sometimes software execution. Your software compilation may require certain versions of a compiler which should be noted in the installation instructions or system dependencies documention. Currently the login nodes have GCC 4.8.5 as the default version, but newer versions of GCC may also be available - to learn more please contact support@opensciencegrid.org . CMake is a commonly used compilation platform. Your software may have dependencies for specific cmake versions. Currently the login nodes have two versions of CMake, 3.12.3 and 3.13.0 available as modules . Static versus dynamic linking during compilation \u00b6 Binary code often depends on additional information (i.e. instructions) from other software, known as libraries, for proper execution. The default behavior when compiling, is for the final binary to be \"dynamically linked\" to libraries that it depends on, such that when the binary is executed, it will look for these library files on the system that it is running on. Thus a copy of the appropriate library files will need to be available to your software wherever it runs. OSG Connect users can transfer a copy of the necessary libraries along with with their jobs to manage such dependencies if not supported by the execute node that your jobs run on. However, the option exists to \"statically link\" the library dependencies of your software. By statically linking libraries during compilation, the library code will be directly packaged with your software binary meaning the libraries will always be available to your software which your software to run on more execute nodes. To statically link libraries during compilation, use the -static flag when running gcc , use --enable-static when running a configure script, or set your LD_FLAGS environment variable to --enable-static (e.g. export LD_FLAGS=\"--enable-static\" ). Get access to libraries needed for your software \u00b6 As described above, your software may require additional software, known as libraries, for compilation and execution. For greatest portability of your software, we recommend installing the libraries needed for your software and transferring a copy of the libraries along with your subsequent jobs. When using libraries that you have installed yourself, you will likely need to add these libraries to your LIBRARY_PATH environment variable before compiling your software. There may also be additional environment variables that will need to be defined or modified for software compilation, this information should be provided in the installtion instructions of your software. For any libraries added to LIBRARY_PATH before software compilation, you'll also need to add these same libraries to your LD_LIBRARY_PATH as a step in your job's executable bash script before executing your software. The distributed environment modules system available on OSG Connect also provides several commonly used software libraries (lapack, atlas, hdf5, netcdf, etc.) that can be used for your software compilation and execution. The appropriate modules should be loaded before performing your software compilation. The process of loading a module will modify all appropriate environment variables (e.g. CPATH and LIBRARY_PATH ) for you. If you do use modules from the modules system, you will need to modify your job scripts to load the appropriate modules before running you software. Perform your compilation \u00b6 Software compilation is easiest to perform interactively, and OSG Connect users are welcome to compile software directly on their assigned login node. This will ensure that your application is built on an environment that is similar to the majority of the compute nodes on OSG. Because OSG Connect login nodes currently use the Red Hat Enterprise Linux 7 operating system, your software will, generally, only be compatible for execution on RHEL 7 or similar operating systems. You can use the requirements statement of your HTCondor submit file to direct your jobs to execute nodes with specific operating systems, for instance: requirements = (OSGVO_OS_STRING == \"RHEL 7\") Software installation typically includes three steps: 1.) configuration, 2.) compilation, and 3.) \"installation\" which places the compiled code in a specific location. In most cases, these steps will be achieved with the following commands: ./configure make make install Most software is written to install to a default location, however your OSG Connect account is not authorized to write to these default system locations. Instead, you will want to create a folder for your software installation in your home directory and use an option in the configuration step that will install the software to this folder: ./configure --prefix=/home/username/path where username should be replaced with your OSG Connect username and path replaced with the path to the directory you created for your software installation. Use Your Software \u00b6 When submitting jobs, you will need to transfer a copy of your compiled software, and any dynamically-linked dependencies that you also installed. Our Introduction to Data Management on OSG Connect guide is a good starting point for more information for selecting the appropriate methods for transferring you software. Depending on your job workflow, it may be possible to directly specify your executable binary as the executable in your HTCondor submit file. When using your software in subsequent job submissions, be sure to add additional commands to the executable bash script to define evironment variables, like for instance LD_LIBRARY_PATH , that may be needed to properly execute your software. Get Additional Assistance \u00b6 If you have questions or need assistance, please contact support@opensciencegrid.org .","title":"Compiling Software for OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#compiling-software-for-osg-connect","text":"","title":"Compiling Software for OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#compiling-software-for-osg-connect_1","text":"","title":"Compiling Software for OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#introduction","text":"Due to the distributed nature of the Open Science Pool, you will always need to ensure that your jobs have access to the software that will be executed. This guide provides useful information for compiling and using your software in OSG Connect. A detailed example of performing software compilation is additionally available at OSG Connect Example Compilation Guide . What is compiling? The process of compiling converts human readable code into binary, machine readable code that will execute the steps of the program.","title":"Introduction"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#get-software-source-code","text":"The first step to compiling your software is to locate and download the source code, being sure to select the version that you want. Source code will often be made available as a compressed tar archive which will need to be extracted for before compilation. You should also carefully review the installation instructions provided by the software developers. The installation instructions should include important information regarding various options for configuring and performing the compilation. Also carefully note any system dependencies (hardware, other software, and libraries) that are required for your software.","title":"Get software source code"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#select-the-appropriate-compiler-and-compilation-options","text":"A compiler is a program that is used to peform source code compilation. The GNU Compiler Collection (GCC) is a common, open source collection of compilers with support for C, C++, fotran, and other languages, and includes important libraries for supporting your compilation and sometimes software execution. Your software compilation may require certain versions of a compiler which should be noted in the installation instructions or system dependencies documention. Currently the login nodes have GCC 4.8.5 as the default version, but newer versions of GCC may also be available - to learn more please contact support@opensciencegrid.org . CMake is a commonly used compilation platform. Your software may have dependencies for specific cmake versions. Currently the login nodes have two versions of CMake, 3.12.3 and 3.13.0 available as modules .","title":"Select the appropriate compiler and compilation options"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#static-versus-dynamic-linking-during-compilation","text":"Binary code often depends on additional information (i.e. instructions) from other software, known as libraries, for proper execution. The default behavior when compiling, is for the final binary to be \"dynamically linked\" to libraries that it depends on, such that when the binary is executed, it will look for these library files on the system that it is running on. Thus a copy of the appropriate library files will need to be available to your software wherever it runs. OSG Connect users can transfer a copy of the necessary libraries along with with their jobs to manage such dependencies if not supported by the execute node that your jobs run on. However, the option exists to \"statically link\" the library dependencies of your software. By statically linking libraries during compilation, the library code will be directly packaged with your software binary meaning the libraries will always be available to your software which your software to run on more execute nodes. To statically link libraries during compilation, use the -static flag when running gcc , use --enable-static when running a configure script, or set your LD_FLAGS environment variable to --enable-static (e.g. export LD_FLAGS=\"--enable-static\" ).","title":"Static versus dynamic linking during compilation"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#get-access-to-libraries-needed-for-your-software","text":"As described above, your software may require additional software, known as libraries, for compilation and execution. For greatest portability of your software, we recommend installing the libraries needed for your software and transferring a copy of the libraries along with your subsequent jobs. When using libraries that you have installed yourself, you will likely need to add these libraries to your LIBRARY_PATH environment variable before compiling your software. There may also be additional environment variables that will need to be defined or modified for software compilation, this information should be provided in the installtion instructions of your software. For any libraries added to LIBRARY_PATH before software compilation, you'll also need to add these same libraries to your LD_LIBRARY_PATH as a step in your job's executable bash script before executing your software. The distributed environment modules system available on OSG Connect also provides several commonly used software libraries (lapack, atlas, hdf5, netcdf, etc.) that can be used for your software compilation and execution. The appropriate modules should be loaded before performing your software compilation. The process of loading a module will modify all appropriate environment variables (e.g. CPATH and LIBRARY_PATH ) for you. If you do use modules from the modules system, you will need to modify your job scripts to load the appropriate modules before running you software.","title":"Get access to libraries needed for your software"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#perform-your-compilation","text":"Software compilation is easiest to perform interactively, and OSG Connect users are welcome to compile software directly on their assigned login node. This will ensure that your application is built on an environment that is similar to the majority of the compute nodes on OSG. Because OSG Connect login nodes currently use the Red Hat Enterprise Linux 7 operating system, your software will, generally, only be compatible for execution on RHEL 7 or similar operating systems. You can use the requirements statement of your HTCondor submit file to direct your jobs to execute nodes with specific operating systems, for instance: requirements = (OSGVO_OS_STRING == \"RHEL 7\") Software installation typically includes three steps: 1.) configuration, 2.) compilation, and 3.) \"installation\" which places the compiled code in a specific location. In most cases, these steps will be achieved with the following commands: ./configure make make install Most software is written to install to a default location, however your OSG Connect account is not authorized to write to these default system locations. Instead, you will want to create a folder for your software installation in your home directory and use an option in the configuration step that will install the software to this folder: ./configure --prefix=/home/username/path where username should be replaced with your OSG Connect username and path replaced with the path to the directory you created for your software installation.","title":"Perform your compilation"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#use-your-software","text":"When submitting jobs, you will need to transfer a copy of your compiled software, and any dynamically-linked dependencies that you also installed. Our Introduction to Data Management on OSG Connect guide is a good starting point for more information for selecting the appropriate methods for transferring you software. Depending on your job workflow, it may be possible to directly specify your executable binary as the executable in your HTCondor submit file. When using your software in subsequent job submissions, be sure to add additional commands to the executable bash script to define evironment variables, like for instance LD_LIBRARY_PATH , that may be needed to properly execute your software.","title":"Use Your Software"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/compiling-applications/#get-additional-assistance","text":"If you have questions or need assistance, please contact support@opensciencegrid.org .","title":"Get Additional Assistance"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/","text":"Create/Register a Docker Container Image \u00b6 This guide is meant to accompany the instructions for using containers in the Open Science Pool. You can use your own custom container to run jobs in the Open Science Pool, and we assume that those containers are built using Docker. This guide describes how to create your own Docker container \"image\" (the blueprint for the container). Once you have created your custom image, you will need to register the image as described further down in this guide. For an overview and how to execute images on OSG, please see Use Containers on the OSG Install Docker and Get a Docker Hub Account \u00b6 You'll need a Docker Hub account in order to download Docker and share your Docker container images with the OSG: DockerHub Install Docker Desktop to your computer using the appropriate version for your operating system. Note that OSG does not provide any container build hosts. Identify Components \u00b6 What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. We strongly recommend using one of the OSG's published containers as your starting point. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name. If you prefer, you can base your image on images not already published by OSG, but if you do this, we recommend that as one of the creation steps you create the /cvmfs directory. See Special Cases below. Build a Container \u00b6 There are two main methods for generating your own container image. Editing the Dockerfile Editing the default image using local Docker We recommend the first option, as it is more reproducible, but the second option can be useful for troubleshooting or especially tricky installs. Editing the Dockerfile \u00b6 Create a folder on your computer and inside it, create a blank text file called Dockerfile . The first line of this file should include the keyword FROM and then the name of a Docker image (from Docker Hub) you want to use as your starting point. If using the OSG's Ubuntu Xenial image that would look like this: FROM opensciencegrid/osgvo-ubuntu-xenial Then, for each command you want to run to add libraries or software, use the keyword RUN and then the command. Sometimes it makes sense to string commands together using the && operator and line breaks \\ , like so: RUN apt-get update && \\ apt-get install -yy build-essentials or RUN wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz && \\ tar -xzf R-3.6.0.tar.gz && \\ cd R-3.6.0 && \\ ./configure && \\ make && \\ make install Typically it's good to group together commands installing the same kind of thing (system libraries, or software packages, or an installation process) under one RUN command, and then have multiple RUN commands, one for each of the different type of software or package you're installing. (For all the possible Dockerfile keywords, see the Docker Documentation ) Once your Dockerfile is ready, you can \"build\" the container image by running this command: $ docker build -t namespace/repository_name . Note that the naming convention for Docker images is your Docker Hub username and then a name you choose for that particular container image. So if my Docker Hub username is alice and I created an image with the NCBI blast tool, I might use this name: $ docker build -t alice/NCBI-blast . Editing the default image using local Docker \u00b6 You can also build an image interactively, without a Dockerfile. First, get the desired starting image from Docker Hub. Again, we will look at the OSG Ubuntu Xenial image. $ docker pull opensciencegrid/osgvo-ubuntu-xenial We will run the image in a docker interactive session $ docker run -it --name <docker_session_name_here> opensciencegrid/osgvo-ubuntu-xenial /bin/bash Giving the session a name is important because it will make it easier to reattach the session later and commit the changes later on. Now you will be greeted by a new command line prompt that will look something like this [root@740b9db736a1 /]# You can now install the software that you need through the default package manager, in this case apt-get . [root@740b9db736a1 /]# apt-get install build-essentials Once you have installed all the software, you simply exit [root@740b9db736a1 /]# exit Now you can commit the changes to the image and give it a name: docker commit <docker_session_name_here> namespace/repository_name You can also use the session's hash as found in the command prompt ( 740b9db736a1 in the above example) in place of the docker session name. Upload Docker Container to Docker Hub \u00b6 Once your container is complete and tagged, it should appear in the list of local Docker container images, which you can see by running: $ docker images From there, you need to put it in Docker Hub, which can be done via the docker push command: $ docker push namespace/repository_name From here, if you're planning to use this container in OSG, return to our Containers in OSG Guide to learn how to upload your container to the OSG's container repository. Submit your Docker Container to the OSG Repository \u00b6 Once your Docker image has been published on Docker Hub, it needs to be submitted to the OSG Singularity repository ( /cvmfs/singularity.opensciencegrid.org/ ), which also hosts the OSG-provided default images. To get your images included, please create a git pull request with the container identifier in docker_images.txt in the cvmfs-singularity-sync repository , or contact support@opensciencegrid.org and we can help you. Once your submission has been accepted, it will be automatically converted to a Singularity image and pushed to the OSG Singularity repository (see further above). Note: some common Dockerfile features, like ENV and ENTRYPOINT, are ignored when the Docker image is converted to a Singularity image. Once your container has been added to CVMFS, if you update your original Docker image, new versions pushed to Docker Hub will automatically be detected and the version on the OSG (in the CVMFS filesystem) will be updated accordingly. Special Cases \u00b6 Accessing CVMFS \u00b6 If you want your jobs to access CVMFS, make sure that you either: Use one of the base containers provided by the Open Science Pool or Add a /cvmfs folder to your container: If using a Dockerfile, you can do this with the line RUN mkdir /cvmfs If building your container interactively, run $ mkdir -p /cvmfs This will enable the container to access tools and data published on /cvmfs . If you do not want /cvmfs mounted in the container, please add +SingularityBindCVMFS = False to your job submit file. ENTRYPOINT and ENV \u00b6 Two options that can be used in the Dockerfile to set the environment or default command are ENTRYPOINT and ENV . Unfortunately, both of these aspects of the Docker container are deleted when it is converted to a Singularity image in the Open Science Pool. Email us if you would like to preserve these attributes.","title":"Create/Register a Docker Container Image"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#createregister-a-docker-container-image","text":"This guide is meant to accompany the instructions for using containers in the Open Science Pool. You can use your own custom container to run jobs in the Open Science Pool, and we assume that those containers are built using Docker. This guide describes how to create your own Docker container \"image\" (the blueprint for the container). Once you have created your custom image, you will need to register the image as described further down in this guide. For an overview and how to execute images on OSG, please see Use Containers on the OSG","title":"Create/Register a Docker Container Image"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#install-docker-and-get-a-docker-hub-account","text":"You'll need a Docker Hub account in order to download Docker and share your Docker container images with the OSG: DockerHub Install Docker Desktop to your computer using the appropriate version for your operating system. Note that OSG does not provide any container build hosts.","title":"Install Docker and Get a Docker Hub Account"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#identify-components","text":"What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. We strongly recommend using one of the OSG's published containers as your starting point. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name. If you prefer, you can base your image on images not already published by OSG, but if you do this, we recommend that as one of the creation steps you create the /cvmfs directory. See Special Cases below.","title":"Identify Components"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#build-a-container","text":"There are two main methods for generating your own container image. Editing the Dockerfile Editing the default image using local Docker We recommend the first option, as it is more reproducible, but the second option can be useful for troubleshooting or especially tricky installs.","title":"Build a Container"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#editing-the-dockerfile","text":"Create a folder on your computer and inside it, create a blank text file called Dockerfile . The first line of this file should include the keyword FROM and then the name of a Docker image (from Docker Hub) you want to use as your starting point. If using the OSG's Ubuntu Xenial image that would look like this: FROM opensciencegrid/osgvo-ubuntu-xenial Then, for each command you want to run to add libraries or software, use the keyword RUN and then the command. Sometimes it makes sense to string commands together using the && operator and line breaks \\ , like so: RUN apt-get update && \\ apt-get install -yy build-essentials or RUN wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz && \\ tar -xzf R-3.6.0.tar.gz && \\ cd R-3.6.0 && \\ ./configure && \\ make && \\ make install Typically it's good to group together commands installing the same kind of thing (system libraries, or software packages, or an installation process) under one RUN command, and then have multiple RUN commands, one for each of the different type of software or package you're installing. (For all the possible Dockerfile keywords, see the Docker Documentation ) Once your Dockerfile is ready, you can \"build\" the container image by running this command: $ docker build -t namespace/repository_name . Note that the naming convention for Docker images is your Docker Hub username and then a name you choose for that particular container image. So if my Docker Hub username is alice and I created an image with the NCBI blast tool, I might use this name: $ docker build -t alice/NCBI-blast .","title":"Editing the Dockerfile"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#editing-the-default-image-using-local-docker","text":"You can also build an image interactively, without a Dockerfile. First, get the desired starting image from Docker Hub. Again, we will look at the OSG Ubuntu Xenial image. $ docker pull opensciencegrid/osgvo-ubuntu-xenial We will run the image in a docker interactive session $ docker run -it --name <docker_session_name_here> opensciencegrid/osgvo-ubuntu-xenial /bin/bash Giving the session a name is important because it will make it easier to reattach the session later and commit the changes later on. Now you will be greeted by a new command line prompt that will look something like this [root@740b9db736a1 /]# You can now install the software that you need through the default package manager, in this case apt-get . [root@740b9db736a1 /]# apt-get install build-essentials Once you have installed all the software, you simply exit [root@740b9db736a1 /]# exit Now you can commit the changes to the image and give it a name: docker commit <docker_session_name_here> namespace/repository_name You can also use the session's hash as found in the command prompt ( 740b9db736a1 in the above example) in place of the docker session name.","title":"Editing the default image using local Docker"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#upload-docker-container-to-docker-hub","text":"Once your container is complete and tagged, it should appear in the list of local Docker container images, which you can see by running: $ docker images From there, you need to put it in Docker Hub, which can be done via the docker push command: $ docker push namespace/repository_name From here, if you're planning to use this container in OSG, return to our Containers in OSG Guide to learn how to upload your container to the OSG's container repository.","title":"Upload Docker Container to Docker Hub"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#submit-your-docker-container-to-the-osg-repository","text":"Once your Docker image has been published on Docker Hub, it needs to be submitted to the OSG Singularity repository ( /cvmfs/singularity.opensciencegrid.org/ ), which also hosts the OSG-provided default images. To get your images included, please create a git pull request with the container identifier in docker_images.txt in the cvmfs-singularity-sync repository , or contact support@opensciencegrid.org and we can help you. Once your submission has been accepted, it will be automatically converted to a Singularity image and pushed to the OSG Singularity repository (see further above). Note: some common Dockerfile features, like ENV and ENTRYPOINT, are ignored when the Docker image is converted to a Singularity image. Once your container has been added to CVMFS, if you update your original Docker image, new versions pushed to Docker Hub will automatically be detected and the version on the OSG (in the CVMFS filesystem) will be updated accordingly.","title":"Submit your Docker Container to the OSG Repository"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#special-cases","text":"","title":"Special Cases"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#accessing-cvmfs","text":"If you want your jobs to access CVMFS, make sure that you either: Use one of the base containers provided by the Open Science Pool or Add a /cvmfs folder to your container: If using a Dockerfile, you can do this with the line RUN mkdir /cvmfs If building your container interactively, run $ mkdir -p /cvmfs This will enable the container to access tools and data published on /cvmfs . If you do not want /cvmfs mounted in the container, please add +SingularityBindCVMFS = False to your job submit file.","title":"Accessing CVMFS"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-docker/#entrypoint-and-env","text":"Two options that can be used in the Dockerfile to set the environment or default command are ENTRYPOINT and ENV . Unfortunately, both of these aspects of the Docker container are deleted when it is converted to a Singularity image in the Open Science Pool. Email us if you would like to preserve these attributes.","title":"ENTRYPOINT and ENV"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-singularity/","text":"Create a Singularity Container Image \u00b6 NOTE: Building Singularity containers are currently not supported on the OSGConnect access point. The guide assumes that you have your own Linux machine where you can install Singularity and execute commands via sudo. This guide is meant to accompany the instructions for using containers in the Open Science Pool. You can use your own custom container to run jobs in the Open Science Pool. This guide describes how to create your own Singularity container \"image\" (the blueprint for the container). For an overview and how to execute images on OSG, please see Use Containers on the OSG Install Singularity \u00b6 Install Singularity to your computer using the appropriate version for your operating system. Note that OSG does not provide any container build hosts. sudo is required to build the images. Identify Components \u00b6 What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. We strongly recommend using one of the OSG's published containers as your starting point. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name. Editing the Build Spec \u00b6 Create a folder on your computer and inside it, create a blank text file called image.def . The first lines of this file should include where to get the base image from. If using the OSG's Ubuntu 20.04 image that would look like this: Bootstrap: docker From: opensciencegrid/osgvo-ubuntu-20.04:latest Then there is a section called %post where you put the additional commands to make the image just like you need it. For example: %post apt-get update -y apt-get install -y \\ build-essential \\ libbz2-dev \\ libcurl4-gnutls-dev wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz tar -xzf R-3.6.0.tar.gz cd R-3.6.0 ./configure make make install See the Singularity documentation for a full reference on how to specify build specs. Note that the %runscript section is ignored when the container is executed on OSG. The final image.def looks like: Bootstrap: docker From: opensciencegrid/osgvo-ubuntu-20.04:latest %post apt-get update -y apt-get install -y \\ build-essential \\ libbz2-dev \\ libcurl4-gnutls-dev wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz tar -xzf R-3.6.0.tar.gz cd R-3.6.0 ./configure make make install Once your build spec is ready, you can \"build\" the container image by running this command: $ sudo singularity build my-container.sif image.def Note that sudo here is currently required. Once the image is built, you can upload it to Stash, test it on OSGConenct, and use it in your HTCondor jobs. This is all described in the container guide .","title":"Create a Singularity Container Image"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-singularity/#create-a-singularity-container-image","text":"NOTE: Building Singularity containers are currently not supported on the OSGConnect access point. The guide assumes that you have your own Linux machine where you can install Singularity and execute commands via sudo. This guide is meant to accompany the instructions for using containers in the Open Science Pool. You can use your own custom container to run jobs in the Open Science Pool. This guide describes how to create your own Singularity container \"image\" (the blueprint for the container). For an overview and how to execute images on OSG, please see Use Containers on the OSG","title":"Create a Singularity Container Image"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-singularity/#install-singularity","text":"Install Singularity to your computer using the appropriate version for your operating system. Note that OSG does not provide any container build hosts. sudo is required to build the images.","title":"Install Singularity"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-singularity/#identify-components","text":"What software do you want to install? Make sure that you have either the source code or a command that can be used to install it through Linux (like apt-get or yum ). You'll also need to choose a \"base\" container, on which to add your particular software or tools. We strongly recommend using one of the OSG's published containers as your starting point. See the available containers on Docker Hub here: OSG Docker Containers The best candidates for you will be containers that have \"osgvo\" in the name.","title":"Identify Components"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers-singularity/#editing-the-build-spec","text":"Create a folder on your computer and inside it, create a blank text file called image.def . The first lines of this file should include where to get the base image from. If using the OSG's Ubuntu 20.04 image that would look like this: Bootstrap: docker From: opensciencegrid/osgvo-ubuntu-20.04:latest Then there is a section called %post where you put the additional commands to make the image just like you need it. For example: %post apt-get update -y apt-get install -y \\ build-essential \\ libbz2-dev \\ libcurl4-gnutls-dev wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz tar -xzf R-3.6.0.tar.gz cd R-3.6.0 ./configure make make install See the Singularity documentation for a full reference on how to specify build specs. Note that the %runscript section is ignored when the container is executed on OSG. The final image.def looks like: Bootstrap: docker From: opensciencegrid/osgvo-ubuntu-20.04:latest %post apt-get update -y apt-get install -y \\ build-essential \\ libbz2-dev \\ libcurl4-gnutls-dev wget https://cran.r-project.org/src/base/R-3/R-3.6.0.tar.gz tar -xzf R-3.6.0.tar.gz cd R-3.6.0 ./configure make make install Once your build spec is ready, you can \"build\" the container image by running this command: $ sudo singularity build my-container.sif image.def Note that sudo here is currently required. Once the image is built, you can upload it to Stash, test it on OSGConenct, and use it in your HTCondor jobs. This is all described in the container guide .","title":"Editing the Build Spec"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/","text":"Use Containers on the OSG \u00b6 Docker and Singularity are container systems that allow users full control over their software environment. You can create your own container image or choose from a set of pre-defined images, and specify that your submitted jobs run within one of these. For jobs on OSG, it does not matter whether you provide a Docker or Singularity image. Either is compatible with our system and can be used with little to no modification. Determining factors on when to use Singularity images over Docker images include if an image already exists, external to OSG distribution preferences, and if you have experience building images in one for format and not the other. Because OSG is a distributed infrastructure and workloads consists of a large number jobs (and there container executions), it is important to consider how the container image is transferred to the execution nodes. The instructions below contain best practices when it comes to access both Singularity and Docker images. When using a container for your jobs, the container image is automatically started up when HTCondor matches your job to a slot. The executable provided in the submit script will be run within the context of the container image, having access to software and libraries that were installed to the image, as if they were already on the server where the job is running. Job executables need not (and should not) run any commands to start the container. Nor should the container image contain any entrypoint/cmd - the job is the command to be run in the container. Exploring Images on the Access Points \u00b6 Just like it is important to test your codes and jobs at a small scale, you should make sure that your container is working correctly. One way to explore how OSG sees your container images, is to explore them on the OSG Connect access points. Start an interactive session with the Singularity \"shell\" mode. The recommended command line, similar to how containers are started for jobs, is: singularity shell \\ --home $PWD:/srv \\ --pwd /srv \\ --bind /cvmfs \\ --scratch /var/tmp \\ --scratch /tmp \\ --contain --ipc --pid \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest/ This will give you an interactive shell in an Ubuntu 20.04 container, with your current working directory mounted under /srv. You can explore the container and test your code with for example your own inputs from your home directory. Once you are down exploring, exit the container by running exit or with CTRL+D OSG-Provided Images \u00b6 The OSG Team maintains a set of images that are already in the OSG Singularity repository. A list of available containers can be found on this page . If the software you need isn't already supported in a listed container, you can use your own container or any container image in Docker Hub (see sections further below). Once the container you need is in the OSG Singularity repository, your can submit jobs that run within a particular container by listing the container image in the submit file. For example, this is what a submit file might look like to run your job within our EL8 container: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest\" <other usual submit file lines> queue Custom Singularity Images \u00b6 If you already have software in the form of a .sif Singuilarity file, and that file is within the supported data sizes , you can stage the .sif file with your job. The image will be resused for each job, and thus the preferred transfer method is Stash . Store the .sif file under /public/$USERNAME/ , and then use the stash url directly in the +SingularityImage attribute. Note that you can not use shell variable expansion in the submit file - be sure to replace the username with your actual OSG Connect username. Example: +SingularityImage = \"stash:///osgconnect/public/USERNAME/my-custom-image-v1.sif\" <other usual submit file lines> queue Be aware that Stash aggressively caches the image based on file naming. If you need to do quick changes, please use versioning of the .sif file so that the caches see a \"new\" name. In this example, replacing my-custom-image-v1.sif with new content will probably mean that some nodes get the old version and some nodes the new version. Prevent this by creating a new file named with v2. More information on how to create Singularity images can be found in the Singularity Images Guide . Custom Docker Images \u00b6 If you would prefer to create or use an existing Docker Hub container, for example an authoritative container for your software which already exists in Docker Hub, OSG can distribute the image for you via CVMFS. The result is a synchronized copy of the image under /cvmfs/singularity.opensciencegrid.org/ which is cached and available to the execution nodes. Creating and/or registering a Docker image is described in the Docker Images Guide . To run a job with a Docker image, use the +SingularityImage to specify the image the job should be using. Example: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest\" <other usual submit file lines> queue Another example would be if your Docker Hub username is alice and you created a container called ncbi-blast added to the OSG Singularity repository, your submit file will include: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/alice/ncbi-blast\" <other usual submit file lines> queue Using Containers from Non-OSG Connect Access Points \u00b6 Users on non-OSG Connect access points can use all the container functionality described above, but will have to use slightly more complex job submit files. This is because the OSG Connect access points uses job transforms to update the jobs based on the +SingularityImage attribute, and OSG Connect users also have direct access to Stash. To run a Singularity image from a non-OSG Connect access point, include a job requirements , and specify a method for image transfer. For example: Requirements = HAS_SINGULARITY == TRUE && SINGULARITY_CAN_USE_SIF = TRUE transfer_input_files = http://datastore.host/mycontainer.sif +SingularityImage = \"./mycontainer.sif\" <other usual submit file lines> queue For images available on CVMFS, just add job requirements : Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest\" <other usual submit file lines> queue Frequently Asked Questions / Common Issues \u00b6 FATAL: kernel too old \u00b6 If you get a FATAL: kernel too old error, it means that the glibc version in the image is too new for the kernel on the host. You can work around this problem by specifying the minimum host kernel. For example, if you want to run the Ubuntu 18.04 image, specfy a minimum host kernel of 3.10.0, formatted as 31000 (major * 10000 + minor * 100 + patch): Requirements = HAS_SINGULARITY == True && OSG_HOST_KERNEL_VERSION >= 31000 Learning More \u00b6 For more information about Docker, please see: Docker Home Page and Singularity, please see: Singularity Home Page Singularity has become the preferred containerization method in scientific computing. The following talk describes Singularity for scientific computing:","title":"Use Containers on the OSG"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#use-containers-on-the-osg","text":"Docker and Singularity are container systems that allow users full control over their software environment. You can create your own container image or choose from a set of pre-defined images, and specify that your submitted jobs run within one of these. For jobs on OSG, it does not matter whether you provide a Docker or Singularity image. Either is compatible with our system and can be used with little to no modification. Determining factors on when to use Singularity images over Docker images include if an image already exists, external to OSG distribution preferences, and if you have experience building images in one for format and not the other. Because OSG is a distributed infrastructure and workloads consists of a large number jobs (and there container executions), it is important to consider how the container image is transferred to the execution nodes. The instructions below contain best practices when it comes to access both Singularity and Docker images. When using a container for your jobs, the container image is automatically started up when HTCondor matches your job to a slot. The executable provided in the submit script will be run within the context of the container image, having access to software and libraries that were installed to the image, as if they were already on the server where the job is running. Job executables need not (and should not) run any commands to start the container. Nor should the container image contain any entrypoint/cmd - the job is the command to be run in the container.","title":"Use Containers on the OSG"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#exploring-images-on-the-access-points","text":"Just like it is important to test your codes and jobs at a small scale, you should make sure that your container is working correctly. One way to explore how OSG sees your container images, is to explore them on the OSG Connect access points. Start an interactive session with the Singularity \"shell\" mode. The recommended command line, similar to how containers are started for jobs, is: singularity shell \\ --home $PWD:/srv \\ --pwd /srv \\ --bind /cvmfs \\ --scratch /var/tmp \\ --scratch /tmp \\ --contain --ipc --pid \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest/ This will give you an interactive shell in an Ubuntu 20.04 container, with your current working directory mounted under /srv. You can explore the container and test your code with for example your own inputs from your home directory. Once you are down exploring, exit the container by running exit or with CTRL+D","title":"Exploring Images on the Access Points"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#osg-provided-images","text":"The OSG Team maintains a set of images that are already in the OSG Singularity repository. A list of available containers can be found on this page . If the software you need isn't already supported in a listed container, you can use your own container or any container image in Docker Hub (see sections further below). Once the container you need is in the OSG Singularity repository, your can submit jobs that run within a particular container by listing the container image in the submit file. For example, this is what a submit file might look like to run your job within our EL8 container: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest\" <other usual submit file lines> queue","title":"OSG-Provided Images"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#custom-singularity-images","text":"If you already have software in the form of a .sif Singuilarity file, and that file is within the supported data sizes , you can stage the .sif file with your job. The image will be resused for each job, and thus the preferred transfer method is Stash . Store the .sif file under /public/$USERNAME/ , and then use the stash url directly in the +SingularityImage attribute. Note that you can not use shell variable expansion in the submit file - be sure to replace the username with your actual OSG Connect username. Example: +SingularityImage = \"stash:///osgconnect/public/USERNAME/my-custom-image-v1.sif\" <other usual submit file lines> queue Be aware that Stash aggressively caches the image based on file naming. If you need to do quick changes, please use versioning of the .sif file so that the caches see a \"new\" name. In this example, replacing my-custom-image-v1.sif with new content will probably mean that some nodes get the old version and some nodes the new version. Prevent this by creating a new file named with v2. More information on how to create Singularity images can be found in the Singularity Images Guide .","title":"Custom Singularity Images"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#custom-docker-images","text":"If you would prefer to create or use an existing Docker Hub container, for example an authoritative container for your software which already exists in Docker Hub, OSG can distribute the image for you via CVMFS. The result is a synchronized copy of the image under /cvmfs/singularity.opensciencegrid.org/ which is cached and available to the execution nodes. Creating and/or registering a Docker image is described in the Docker Images Guide . To run a job with a Docker image, use the +SingularityImage to specify the image the job should be using. Example: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest\" <other usual submit file lines> queue Another example would be if your Docker Hub username is alice and you created a container called ncbi-blast added to the OSG Singularity repository, your submit file will include: +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/alice/ncbi-blast\" <other usual submit file lines> queue","title":"Custom Docker Images"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#using-containers-from-non-osg-connect-access-points","text":"Users on non-OSG Connect access points can use all the container functionality described above, but will have to use slightly more complex job submit files. This is because the OSG Connect access points uses job transforms to update the jobs based on the +SingularityImage attribute, and OSG Connect users also have direct access to Stash. To run a Singularity image from a non-OSG Connect access point, include a job requirements , and specify a method for image transfer. For example: Requirements = HAS_SINGULARITY == TRUE && SINGULARITY_CAN_USE_SIF = TRUE transfer_input_files = http://datastore.host/mycontainer.sif +SingularityImage = \"./mycontainer.sif\" <other usual submit file lines> queue For images available on CVMFS, just add job requirements : Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest\" <other usual submit file lines> queue","title":"Using Containers from Non-OSG Connect Access Points"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#frequently-asked-questions-common-issues","text":"","title":"Frequently Asked Questions / Common Issues"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#fatal-kernel-too-old","text":"If you get a FATAL: kernel too old error, it means that the glibc version in the image is too new for the kernel on the host. You can work around this problem by specifying the minimum host kernel. For example, if you want to run the Ubuntu 18.04 image, specfy a minimum host kernel of 3.10.0, formatted as 31000 (major * 10000 + minor * 100 + patch): Requirements = HAS_SINGULARITY == True && OSG_HOST_KERNEL_VERSION >= 31000","title":"FATAL: kernel too old"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/containers/#learning-more","text":"For more information about Docker, please see: Docker Home Page and Singularity, please see: Singularity Home Page Singularity has become the preferred containerization method in scientific computing. The following talk describes Singularity for scientific computing:","title":"Learning More"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/","text":"Example Software Compilation \u00b6 Example of Compilng Software For Use In OSG Connect \u00b6 Introduction \u00b6 Due to the distributed nature of the Open Science Pool, you will always need to ensure that your jobs have access to the software that will be executed. This guide provides a detailed example of compiling software for use in OSG Connect. For this example, we will be compiling Samtools which is a very common bioinformatics software for working with aligned sequencing data. However, the steps described in this example are applicable to many other software. For a general introduction to software compilation in OSG Connect, please see Compiling Software for OSG Connect . Specifically, this guide provides two examples of compiling Samtools, one without CRAM file support and one with CRAM file support . Why two examples? Currently, to install Samtools with CRAM support requires additional dependencies (aka libraries) that will also need to be installed and most Samtools users are only working with BAM files which does not require CRAM support. Do I need CRAM support for my work? CRAM is an alternative compressed sequence alignment file format to BAM. Learn more at https://www.sanger.ac.uk/tool/cram/ . Compile Samtools Without CRAM Support \u00b6 Step 1. Acquire Samtools source code \u00b6 Samtools source code is available at http://www.htslib.org/download/ . The development code is also available via GitHub at https://github.com/samtools/samtools . On the download page is some important information to make note of: \"[Samtools] uses HTSlib internally [and] these source packages contain their own copies of htslib\" What this means is 1.) HTSlib is a dependency of Samtools and 2.) the HTSlib source code is included with the Samtools source code. Either download the Samtools source code to your computer and upload to your login node, or right-click on the Samtools source code link and copy the link location. Login in to your OSG Connect login node and use wget to download the source code directly and extract the tarball: [user@login ~]$ wget https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2 [user@login ~]$ tar -xjf samtools-1.10.tar.bz2 The above two commands will create a directory named samtools-1.10 which contains all the code and instructions needed for compiling Samtools and HTSlib. Take a moment to look at the content available in this new directory. Step 2. Read through installation instructions \u00b6 What steps need to be performed for our compilation? What system dependencies exist for our software? Answers to these questions, and other important information, should be available in the installation instructions for your software which will be available online and/or included in the source code. The HTSlib website where the Samtools source code is hosted provides basic installation instructions and refers users to INSTALL (which is a plain text file that can be found in samtools-1.10/ ) for more information. You will also see a README file in the source code directory which will provide important information. README files will always be included with your source code and we recommend reviewing before compiling software. There is also a README and INSTALL file available for HTSlib in the source code directory samtools-1.10/htslib-1.10/ . cd to samtools-1.10 and read through README and INSTALL . As described in INSTALL , the Samtools installation will follow the common configure , make , make install process: Basic Installation ================== To build and install Samtools, 'cd' to the samtools-1.x directory containing the package's source and type the following commands: ./configure make make install The './configure' command checks your build environment and allows various optional functionality to be enabled (see Configuration below). Also described in INSTALL are a number of required and optional system dependencies for installing Samtools and HTSlib (which is itself a dependency of Samtools): System Requirements =================== Samtools and HTSlib depend on the following libraries: Samtools: zlib <http://zlib.net> curses or GNU ncurses (optional, for the 'tview' command) <http://www.gnu.org/software/ncurses/> HTSlib: zlib <http://zlib.net> libbz2 <http://bzip.org/> liblzma <http://tukaani.org/xz/> libcurl <https://curl.haxx.se/> (optional but strongly recommended, for network access) libcrypto <https://www.openssl.org/> (optional, for Amazon S3 support; not needed on MacOS) ... The bzip2 and liblzma dependencies can be removed if full CRAM support is not needed - see HTSlib's INSTALL file for details. Some dependencies are needed to support certain features from Samtools (such as tview and CRAM compression). You will not need tview as this is intended for interactive work which is not currently supported in OSG Connect. For this specific compilation example, we will disable both tview and CRAM support - see below for our compilation example that will provide CRAM file support. Following the suggestion in the Samtools INSTALL file, we can view the HTSlib INSTALL file at samtools-1.10/htslib-1.10/INSTALL . Here we will find the necessary information for disabling bzip2 and liblzma dependencies: --disable-bz2 Bzip2 is an optional compression codec format for CRAM, included in HTSlib by default. It can be disabled with --disable-bz2, but be aware that not all CRAM files may be possible to decode. --disable-lzma LZMA is an optional compression codec for CRAM, included in HTSlib by default. It can be disabled with --disable-lzma, but be aware that not all CRAM files may be possible to decode. These are two flags that will need to be used when performing our installation. To determine what libraries are available on our OSG Connect login node, we can look at /usr/lib and /usr/lib64 for the various Samtools library dependencies, for example: [user@login ~]$ ls /usr/lib* | grep libcurl [user@login ~]$ ls /usr/lib* | grep htslib Although we will find matches for libcurl , we will not find any htslib files meaning that HTSlib is not currently installed on the login node, nor is it currently available as a module. This means that HTSlib will also need to be compiled. Luckly, the Samtools developers have conveniently included the HTSlib source code with the Samtools source code and have made it possible to compile both Samtools and HTSlib at the same time. From the Samtools INSTALL file, is the following: By default, configure looks for an HTSlib source tree within or alongside the samtools source directory; if there are several likely candidates, you will have to choose one via this option. This mean that we don't have to do anything extra to get HTSlib installed because the Samtools installation will do it by default. When performing your compilation, if your compiler is unable to locate the necessary libraries, or if newer versions of libraries are needed, it will result in an error - this makes for an alternative method of determining whether your system has the appropriate libraries for your software and more often than not, installation by trial and error is a common approach. However, taking a little bit of time before hand and looking for library files can save you time and frustration during software compilation. Step 3. Perform Samtools compilation \u00b6 We now have all of the information needed to start our compilation of Samtools without CRAM support. First, we will create a new directory in our home directory that will store the Samtools compiled software. The example here will use a directory, called my-software , for organizing all compiled software in the home directory: [user@login ~]$ mkdir $HOME/my-software [user@login ~]$ mkdir $HOME/my-software/samtools-1.10 As a best practice, always include the version name of your software in the directory name. Next we'll change to the Samtools source code direcory that was created in Step 1 . You should see the INSTALL and README files as well as a file called configure . The first command we will run is ./configure - this step will execute the configure script and allows us to modify various details about our Samtools installation. We will be executing configure with several flags: [user@login samtools-1.10]$ ./configure --prefix=$HOME/my-software/samtools-1.10 --disable-bz2 --disable-lzma --without-curses Here we used --prefix to specify where we would like the final Samtools software to be installed, --disable-bz2 and --disable-lzma to disable lzma and bzip2 dependencies for CRAM, and --without-curses to disable tview support. Next run the final two commands: [user@login samtools-1.10]$ make [user@login samtools-1.10]$ make install Once make install has finished running, the compilation is complete. We can also confirm this by looking at the content of ~/my-software/samtools-1.10/ where we had Samtools installed: [user@login samtools-1.10]$ cd ~ [user@login ~]$ ls -F my-software/samtools-1.10/ bin/ share/ There will be two directories present in my-software/samtools-1.10 , one named bin and another named share . The Samtools executable will be located in bin and we can give it a quick test to make sure it runs as expected: [user@login ~]$ ./my-software/samtools-1.10/bin/samtools view which will return the Samtools view usage statement. Step 4. Make our software portable \u00b6 Our subsequent job submissions on OSG Connect will need a copy of our software. For convenience, we recommend converting your software directory to a tar archive. First move to my-software/ , then create the tar archive: [user@login ~]$ mv my-software/ [user@login my-software]$ tar -czf samtools-1.10.tar.gz samtools-1.10/ [user@login my-software]$ ls samtools-1.10* samtools-1.10/ samtools-1.10.tar.gz [user@login my-software]$ du -h samtools-1.10.tar.gz 2.0M samtools-1.10.tar.gz The last command in the above example returns the size of our tar archive. This is important for determine the appropriate method that we should use for transferring this file along with our subsequent jobs. To learn more, please see Introduction to Data Management on OSG Connect . To clean up and clear out space in your home directory, we recommend deleting the Samtools source code directory. Step 5. Use Samtools in our jobs \u00b6 Now that Samtools has been compiled we can submit jobs that use this software. Below is an example submit file for a job that will use Samtools with a BAM file named my-sample.bam which is <100MB in size: #samtools.sub log = samtools.$(Cluster).log error = samtools.$(Cluster)_$(Process).err output = samtools.$(Cluster)_$(Process).out executable = samtools.sh transfer_input_files = /home/username/my-software/samtools-1.10.tar.gz, my-sample.bam should_transfer_files = YES when_to_transfer_output = ON_EXIT requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_memory = 1.3GB request_disk = 1.5GB request_cpus = 1 queue 1 The above submit file will transfer a complete copy of the Samtools tar archive created in Step 4 and also includes an important requirements attribute which tells HTCondor to run our job on execute nodes running Red Hat Linux version 7 operating system. The resource requests for your jobs may differ from what is shown in the above example. Always run tests to determine the appropriate requests for your jobs. Some additional steps are then needed in the executable bash script used by this job to \"untar\" the Samtools and add this software to the PATH enviroment variable: 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # samtools.sh # untar software tar -xzf samtools-1.10.tar.gz # modify environment variables export PATH = $_CONDOR_SCRATCH_DIR /samtools-1.10/bin: $PATH # run samtools commands ... Compile Samtools With CRAM Support \u00b6 This example includes steps to install and use a library and to use a module, which are both currently needed for compiling Samtools with CRAM support. The steps in this example assume that you have performed Step 1 and Step 2 in the above example for compiling Samtools without CRAM support. Step 2. Read through installation instructions, continued \u00b6 From both the Samtools and HTSlib INSTALL files, we know that both bzip2 and libzlma are required for CRAM support. We can check our system for these libraries: [user@login ~]$ ls /usr/lib* | grep libz [user@login ~]$ ls /usr/lib* | grep libbz2 which will reveal that both sets of libraries are available on the login. However if we were to attempt Samtools installation with CRAM support right now we would find that this results in an error when performing the configure step. If the libraries are present, why do we get this error? This error is due to differences between types of library files. For example, running ls /usr/lib* | grep libbz2 will return two matches, libbz2.so.1 and libbz2.so.1.0.6 . But running ls /usr/lib* | grep liblz will return four matches including three .so and one .a files. Our Samtools compilation specifically requires the .a type of library file for both libbz2 and liblzma and the absence of this type of library file in /usr/lib64 is why compilation will fail without additional steps. Luckily for us, bzip2 version 1.0.6 is available as a module and this module includes access to a .a library file. We will use this module for our Samtools compilation. To learn more about using modules, please see Accessing Software using Distributed Environment Modules . liblzma however is not currently available as a module and our next step will be to install liblzma . Step 3. Compile liblzma \u00b6 To compile Samtools with CRAM support requires that we first compile liblzma . Following the same approach as we did for Samtools, first we acquire a copy of the the latest liblzma source code, then review the installation instructions. From our online search we will that liblzma is availble from the XZ Utils library package. [user@login ~]$ wget https://tukaani.org/xz/xz-5.2.5.tar.gz [user@login ~]$ tar -xzf xz-5.2.5.tar.gz Then review the installation instructions and check for dependencies. Everything that is needed for the default installation of XZ utils is currently available on the login node. [user@login ~]$ cd xz-5.2.5/ [user@login xz-5.2.5]$ less INSTALL Perform the XZ Utils compilation: [user@login xz-5.2.5]$ mkdir $HOME/my-software/xz-5.2.5 [user@login xz-5.2.5]$ ./configure --prefix=$HOME/my-software/xz-5.2.5 [user@login xz-5.2.5]$ make [user@login xz-5.2.5]$ make install [user@login xz-5.2.5]$ ls -F $HOME/my-software/xz-5.2.5 /bin /include /lib /share Success! Lastly we need to set some environment variables so that Samtools knows where to find this library: [user@login xz-5.2.5]$ export PATH=$HOME/my-software/xz-5.2.5/bin:$PATH [user@login xz-5.2.5]$ export LIBRARY_PATH=$HOME/my-software/xz-5.2.5/lib:$LIBRARY_PATH [user@login xz-5.2.5]$ export LD_LIBRARY_PATH=$LIBRARY_PATH Step 4. Load bzip2 module \u00b6 After installing XZ Utils and setting our environment variable, next we will load the bzip2 module: [user@login xz-5.2.5]$ module load bzip2/1.0.6 Loading this module will further modify some of your environment variables so that Samtools is able to locate the bzip2 library files. To learn more about using modules, please see Accessing Software using Distributed Environment Modules . Step 5. Compile Samtools \u00b6 After compiling XZ Utils (which provides liblzma ) and loading the bzip2 1.0.6 module, we are now ready to compile Samtools with CRAM support. First, we will create a new directory in our home directory that will store the Samtools compiled software. The example here will use a common directory, called my-software , for organizing all compiled software in the home directory: [user@login ~]$ mkdir $HOME/my-software [user@login ~]$ mkdir $HOME/my-software/samtools-1.10 As a best practice, always include the version name of your software in the directory name. Next, we will change our directory to the Samtools source code direcory that was created in Step 1 . You should see the INSTALL and README files as well as a file called configure . The first command we will run is ./configure - this file is a script that allows us to modify various details about our Samtools installation and we will be executing configure with a flag that disables tview : [user@login samtools-1.10]$ ./configure --prefix=$HOME/my-software/samtools-1.10 --without-curses Here we used --prefix to specify where we would like the final Samtools software to be installed and --without-curses to disable tview support. Next run the final two commands: [user@login samtools-1.10]$ make [user@login samtools-1.10]$ make install Once make install has finished running, the compilation is complete. We can also confirm this by looking at the content of ~/my-software/samtools-1.10/ where we had Samtools installed: [user@login samtools-1.10]$ cd ~ [user@login ~]$ ls -F my-software/samtools-1.10/ bin/ share/ There will be two directories present in my-software/samtools-1.10 , one named bin and another named share . The Samtools executable will be located in bin and we can give it a quick test to make sure it runs as expected: [user@login ~]$ ./my-software/samtools-1.10/bin/samtools view which will return the Samtools view usage statement. Step 6. Make our software portable \u00b6 Our subsequent job submissions on OSG Connect will need a copy of our software. For convenience, we recommend converting your software directory to a tar archive. First move to my-software/ , then create the tar archive: [user@login ~]$ cd my-software/ [user@login my-software]$ tar -czf samtools-1.10.tar.gz samtools-1.10/ [user@login my-software]$ ls samtools-1.10* samtools-1.10/ samtools-1.10.tar.gz [user@login my-software]$ du -h samtools-1.10.tar.gz 2.0M samtools-1.10.tar.gz The last command in the above example returns the size of our tar archive. This is important for determine the appropriate method that we should use for transferring this file along with our subsequent jobs. To learn more, please see Introduction to Data Management on OSG Connect . Follow the these same steps for creating a tar archive of the xz-5.2.5 library as well. To clean up and clear out space in your home directory, we recommend deleting the Samtools source code directory. Step 7. Use Samtools in our jobs \u00b6 Now that Samtools has been compiled we can submit jobs that use this software. For Samtools with CRAM we will also need to bring along a copy of XZ Utils (which includes the liblzma library) and ensure that our jobs have access to the bzip2 1.0.6 module. Below is an example submit file for a job that will use Samtools with a Fasta file genome.fa' and CRAM file named my-sample.cram` which is <100MB in size: #samtools-cram.sub log = samtools-cram.$(Cluster).log error = samtools-cram.$(Cluster)_$(Process).err output = samtools-cram.$(Cluster)_$(Process).out executable = samtools-cram.sh transfer_input_files = /home/username/my-software/samtools-1.10.tar.gz, /home/username/my-software/xz-5.2.5.tar.gz, genome.fa, my-sample.cram should_transfer_files = YES when_to_transfer_output = ON_EXIT requirements = (OSGVO_OS_STRING == \"RHEL 7\") && (HAS_MODULES =?= TRUE) request_memory = 1.3GB request_disk = 1.5GB request_cpus = 1 queue 1 The above submit file will transfer a complete copy of the Samtools tar archive created in Step 6 as well as a copy of XZ Utils installation from Step 3 . This submit file also includes two important requirements which tell HTCondor to run our job on execute nodes running Red Hat Linux version 7 operating system and which has access to OSG Connect software modules. The resource requests for your jobs may differ from what is shown in the above example. Always run tests to determine the appropriate requests for your jobs. Some additional steps are then needed in the executable bash script used by this job to \"untar\" the Samtools and XZ Util tar archives, modify the PATH and LD_LIBRARY_PATH enviroments of our job, and load the bzip2 module: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash # samtools-cram.sh # untar software and libraries tar -xzf samtools-1.10.tar.gz tar -xzf xz-5.2.5.tar.gz # modify environment variables export LD_LIBRARY_PATH = $_CONDOR_SCRATCH_DIR /xz-5.2.5/lib: $LD_LIBRARY_PATH export PATH = $_CONDOR_SCRATCH_DIR /samtools-1.10/bin: $_CONDOR_SCRATCH_DIR /xz-5.2.5/bin: $PATH # load bzip2 module module load bzip2/1.0.6 # run samtools commands ...","title":"Example Software Compilation"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#example-software-compilation","text":"","title":"Example Software Compilation"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#example-of-compilng-software-for-use-in-osg-connect","text":"","title":"Example of Compilng Software For Use In OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#introduction","text":"Due to the distributed nature of the Open Science Pool, you will always need to ensure that your jobs have access to the software that will be executed. This guide provides a detailed example of compiling software for use in OSG Connect. For this example, we will be compiling Samtools which is a very common bioinformatics software for working with aligned sequencing data. However, the steps described in this example are applicable to many other software. For a general introduction to software compilation in OSG Connect, please see Compiling Software for OSG Connect . Specifically, this guide provides two examples of compiling Samtools, one without CRAM file support and one with CRAM file support . Why two examples? Currently, to install Samtools with CRAM support requires additional dependencies (aka libraries) that will also need to be installed and most Samtools users are only working with BAM files which does not require CRAM support. Do I need CRAM support for my work? CRAM is an alternative compressed sequence alignment file format to BAM. Learn more at https://www.sanger.ac.uk/tool/cram/ .","title":"Introduction"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#compile-samtools-without-cram-support","text":"","title":"Compile Samtools Without CRAM Support"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-1-acquire-samtools-source-code","text":"Samtools source code is available at http://www.htslib.org/download/ . The development code is also available via GitHub at https://github.com/samtools/samtools . On the download page is some important information to make note of: \"[Samtools] uses HTSlib internally [and] these source packages contain their own copies of htslib\" What this means is 1.) HTSlib is a dependency of Samtools and 2.) the HTSlib source code is included with the Samtools source code. Either download the Samtools source code to your computer and upload to your login node, or right-click on the Samtools source code link and copy the link location. Login in to your OSG Connect login node and use wget to download the source code directly and extract the tarball: [user@login ~]$ wget https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2 [user@login ~]$ tar -xjf samtools-1.10.tar.bz2 The above two commands will create a directory named samtools-1.10 which contains all the code and instructions needed for compiling Samtools and HTSlib. Take a moment to look at the content available in this new directory.","title":"Step 1. Acquire Samtools source code"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-2-read-through-installation-instructions","text":"What steps need to be performed for our compilation? What system dependencies exist for our software? Answers to these questions, and other important information, should be available in the installation instructions for your software which will be available online and/or included in the source code. The HTSlib website where the Samtools source code is hosted provides basic installation instructions and refers users to INSTALL (which is a plain text file that can be found in samtools-1.10/ ) for more information. You will also see a README file in the source code directory which will provide important information. README files will always be included with your source code and we recommend reviewing before compiling software. There is also a README and INSTALL file available for HTSlib in the source code directory samtools-1.10/htslib-1.10/ . cd to samtools-1.10 and read through README and INSTALL . As described in INSTALL , the Samtools installation will follow the common configure , make , make install process: Basic Installation ================== To build and install Samtools, 'cd' to the samtools-1.x directory containing the package's source and type the following commands: ./configure make make install The './configure' command checks your build environment and allows various optional functionality to be enabled (see Configuration below). Also described in INSTALL are a number of required and optional system dependencies for installing Samtools and HTSlib (which is itself a dependency of Samtools): System Requirements =================== Samtools and HTSlib depend on the following libraries: Samtools: zlib <http://zlib.net> curses or GNU ncurses (optional, for the 'tview' command) <http://www.gnu.org/software/ncurses/> HTSlib: zlib <http://zlib.net> libbz2 <http://bzip.org/> liblzma <http://tukaani.org/xz/> libcurl <https://curl.haxx.se/> (optional but strongly recommended, for network access) libcrypto <https://www.openssl.org/> (optional, for Amazon S3 support; not needed on MacOS) ... The bzip2 and liblzma dependencies can be removed if full CRAM support is not needed - see HTSlib's INSTALL file for details. Some dependencies are needed to support certain features from Samtools (such as tview and CRAM compression). You will not need tview as this is intended for interactive work which is not currently supported in OSG Connect. For this specific compilation example, we will disable both tview and CRAM support - see below for our compilation example that will provide CRAM file support. Following the suggestion in the Samtools INSTALL file, we can view the HTSlib INSTALL file at samtools-1.10/htslib-1.10/INSTALL . Here we will find the necessary information for disabling bzip2 and liblzma dependencies: --disable-bz2 Bzip2 is an optional compression codec format for CRAM, included in HTSlib by default. It can be disabled with --disable-bz2, but be aware that not all CRAM files may be possible to decode. --disable-lzma LZMA is an optional compression codec for CRAM, included in HTSlib by default. It can be disabled with --disable-lzma, but be aware that not all CRAM files may be possible to decode. These are two flags that will need to be used when performing our installation. To determine what libraries are available on our OSG Connect login node, we can look at /usr/lib and /usr/lib64 for the various Samtools library dependencies, for example: [user@login ~]$ ls /usr/lib* | grep libcurl [user@login ~]$ ls /usr/lib* | grep htslib Although we will find matches for libcurl , we will not find any htslib files meaning that HTSlib is not currently installed on the login node, nor is it currently available as a module. This means that HTSlib will also need to be compiled. Luckly, the Samtools developers have conveniently included the HTSlib source code with the Samtools source code and have made it possible to compile both Samtools and HTSlib at the same time. From the Samtools INSTALL file, is the following: By default, configure looks for an HTSlib source tree within or alongside the samtools source directory; if there are several likely candidates, you will have to choose one via this option. This mean that we don't have to do anything extra to get HTSlib installed because the Samtools installation will do it by default. When performing your compilation, if your compiler is unable to locate the necessary libraries, or if newer versions of libraries are needed, it will result in an error - this makes for an alternative method of determining whether your system has the appropriate libraries for your software and more often than not, installation by trial and error is a common approach. However, taking a little bit of time before hand and looking for library files can save you time and frustration during software compilation.","title":"Step 2. Read through installation instructions"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-3-perform-samtools-compilation","text":"We now have all of the information needed to start our compilation of Samtools without CRAM support. First, we will create a new directory in our home directory that will store the Samtools compiled software. The example here will use a directory, called my-software , for organizing all compiled software in the home directory: [user@login ~]$ mkdir $HOME/my-software [user@login ~]$ mkdir $HOME/my-software/samtools-1.10 As a best practice, always include the version name of your software in the directory name. Next we'll change to the Samtools source code direcory that was created in Step 1 . You should see the INSTALL and README files as well as a file called configure . The first command we will run is ./configure - this step will execute the configure script and allows us to modify various details about our Samtools installation. We will be executing configure with several flags: [user@login samtools-1.10]$ ./configure --prefix=$HOME/my-software/samtools-1.10 --disable-bz2 --disable-lzma --without-curses Here we used --prefix to specify where we would like the final Samtools software to be installed, --disable-bz2 and --disable-lzma to disable lzma and bzip2 dependencies for CRAM, and --without-curses to disable tview support. Next run the final two commands: [user@login samtools-1.10]$ make [user@login samtools-1.10]$ make install Once make install has finished running, the compilation is complete. We can also confirm this by looking at the content of ~/my-software/samtools-1.10/ where we had Samtools installed: [user@login samtools-1.10]$ cd ~ [user@login ~]$ ls -F my-software/samtools-1.10/ bin/ share/ There will be two directories present in my-software/samtools-1.10 , one named bin and another named share . The Samtools executable will be located in bin and we can give it a quick test to make sure it runs as expected: [user@login ~]$ ./my-software/samtools-1.10/bin/samtools view which will return the Samtools view usage statement.","title":"Step 3. Perform Samtools compilation"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-4-make-our-software-portable","text":"Our subsequent job submissions on OSG Connect will need a copy of our software. For convenience, we recommend converting your software directory to a tar archive. First move to my-software/ , then create the tar archive: [user@login ~]$ mv my-software/ [user@login my-software]$ tar -czf samtools-1.10.tar.gz samtools-1.10/ [user@login my-software]$ ls samtools-1.10* samtools-1.10/ samtools-1.10.tar.gz [user@login my-software]$ du -h samtools-1.10.tar.gz 2.0M samtools-1.10.tar.gz The last command in the above example returns the size of our tar archive. This is important for determine the appropriate method that we should use for transferring this file along with our subsequent jobs. To learn more, please see Introduction to Data Management on OSG Connect . To clean up and clear out space in your home directory, we recommend deleting the Samtools source code directory.","title":"Step 4. Make our software portable"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-5-use-samtools-in-our-jobs","text":"Now that Samtools has been compiled we can submit jobs that use this software. Below is an example submit file for a job that will use Samtools with a BAM file named my-sample.bam which is <100MB in size: #samtools.sub log = samtools.$(Cluster).log error = samtools.$(Cluster)_$(Process).err output = samtools.$(Cluster)_$(Process).out executable = samtools.sh transfer_input_files = /home/username/my-software/samtools-1.10.tar.gz, my-sample.bam should_transfer_files = YES when_to_transfer_output = ON_EXIT requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_memory = 1.3GB request_disk = 1.5GB request_cpus = 1 queue 1 The above submit file will transfer a complete copy of the Samtools tar archive created in Step 4 and also includes an important requirements attribute which tells HTCondor to run our job on execute nodes running Red Hat Linux version 7 operating system. The resource requests for your jobs may differ from what is shown in the above example. Always run tests to determine the appropriate requests for your jobs. Some additional steps are then needed in the executable bash script used by this job to \"untar\" the Samtools and add this software to the PATH enviroment variable: 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # samtools.sh # untar software tar -xzf samtools-1.10.tar.gz # modify environment variables export PATH = $_CONDOR_SCRATCH_DIR /samtools-1.10/bin: $PATH # run samtools commands ...","title":"Step 5. Use Samtools in our jobs"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#compile-samtools-with-cram-support","text":"This example includes steps to install and use a library and to use a module, which are both currently needed for compiling Samtools with CRAM support. The steps in this example assume that you have performed Step 1 and Step 2 in the above example for compiling Samtools without CRAM support.","title":"Compile Samtools With CRAM Support"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-2-read-through-installation-instructions-continued","text":"From both the Samtools and HTSlib INSTALL files, we know that both bzip2 and libzlma are required for CRAM support. We can check our system for these libraries: [user@login ~]$ ls /usr/lib* | grep libz [user@login ~]$ ls /usr/lib* | grep libbz2 which will reveal that both sets of libraries are available on the login. However if we were to attempt Samtools installation with CRAM support right now we would find that this results in an error when performing the configure step. If the libraries are present, why do we get this error? This error is due to differences between types of library files. For example, running ls /usr/lib* | grep libbz2 will return two matches, libbz2.so.1 and libbz2.so.1.0.6 . But running ls /usr/lib* | grep liblz will return four matches including three .so and one .a files. Our Samtools compilation specifically requires the .a type of library file for both libbz2 and liblzma and the absence of this type of library file in /usr/lib64 is why compilation will fail without additional steps. Luckily for us, bzip2 version 1.0.6 is available as a module and this module includes access to a .a library file. We will use this module for our Samtools compilation. To learn more about using modules, please see Accessing Software using Distributed Environment Modules . liblzma however is not currently available as a module and our next step will be to install liblzma .","title":"Step 2. Read through installation instructions, continued"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-3-compile-liblzma","text":"To compile Samtools with CRAM support requires that we first compile liblzma . Following the same approach as we did for Samtools, first we acquire a copy of the the latest liblzma source code, then review the installation instructions. From our online search we will that liblzma is availble from the XZ Utils library package. [user@login ~]$ wget https://tukaani.org/xz/xz-5.2.5.tar.gz [user@login ~]$ tar -xzf xz-5.2.5.tar.gz Then review the installation instructions and check for dependencies. Everything that is needed for the default installation of XZ utils is currently available on the login node. [user@login ~]$ cd xz-5.2.5/ [user@login xz-5.2.5]$ less INSTALL Perform the XZ Utils compilation: [user@login xz-5.2.5]$ mkdir $HOME/my-software/xz-5.2.5 [user@login xz-5.2.5]$ ./configure --prefix=$HOME/my-software/xz-5.2.5 [user@login xz-5.2.5]$ make [user@login xz-5.2.5]$ make install [user@login xz-5.2.5]$ ls -F $HOME/my-software/xz-5.2.5 /bin /include /lib /share Success! Lastly we need to set some environment variables so that Samtools knows where to find this library: [user@login xz-5.2.5]$ export PATH=$HOME/my-software/xz-5.2.5/bin:$PATH [user@login xz-5.2.5]$ export LIBRARY_PATH=$HOME/my-software/xz-5.2.5/lib:$LIBRARY_PATH [user@login xz-5.2.5]$ export LD_LIBRARY_PATH=$LIBRARY_PATH","title":"Step 3. Compile liblzma"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-4-load-bzip2-module","text":"After installing XZ Utils and setting our environment variable, next we will load the bzip2 module: [user@login xz-5.2.5]$ module load bzip2/1.0.6 Loading this module will further modify some of your environment variables so that Samtools is able to locate the bzip2 library files. To learn more about using modules, please see Accessing Software using Distributed Environment Modules .","title":"Step 4. Load bzip2 module"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-5-compile-samtools","text":"After compiling XZ Utils (which provides liblzma ) and loading the bzip2 1.0.6 module, we are now ready to compile Samtools with CRAM support. First, we will create a new directory in our home directory that will store the Samtools compiled software. The example here will use a common directory, called my-software , for organizing all compiled software in the home directory: [user@login ~]$ mkdir $HOME/my-software [user@login ~]$ mkdir $HOME/my-software/samtools-1.10 As a best practice, always include the version name of your software in the directory name. Next, we will change our directory to the Samtools source code direcory that was created in Step 1 . You should see the INSTALL and README files as well as a file called configure . The first command we will run is ./configure - this file is a script that allows us to modify various details about our Samtools installation and we will be executing configure with a flag that disables tview : [user@login samtools-1.10]$ ./configure --prefix=$HOME/my-software/samtools-1.10 --without-curses Here we used --prefix to specify where we would like the final Samtools software to be installed and --without-curses to disable tview support. Next run the final two commands: [user@login samtools-1.10]$ make [user@login samtools-1.10]$ make install Once make install has finished running, the compilation is complete. We can also confirm this by looking at the content of ~/my-software/samtools-1.10/ where we had Samtools installed: [user@login samtools-1.10]$ cd ~ [user@login ~]$ ls -F my-software/samtools-1.10/ bin/ share/ There will be two directories present in my-software/samtools-1.10 , one named bin and another named share . The Samtools executable will be located in bin and we can give it a quick test to make sure it runs as expected: [user@login ~]$ ./my-software/samtools-1.10/bin/samtools view which will return the Samtools view usage statement.","title":"Step 5. Compile Samtools"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-6-make-our-software-portable","text":"Our subsequent job submissions on OSG Connect will need a copy of our software. For convenience, we recommend converting your software directory to a tar archive. First move to my-software/ , then create the tar archive: [user@login ~]$ cd my-software/ [user@login my-software]$ tar -czf samtools-1.10.tar.gz samtools-1.10/ [user@login my-software]$ ls samtools-1.10* samtools-1.10/ samtools-1.10.tar.gz [user@login my-software]$ du -h samtools-1.10.tar.gz 2.0M samtools-1.10.tar.gz The last command in the above example returns the size of our tar archive. This is important for determine the appropriate method that we should use for transferring this file along with our subsequent jobs. To learn more, please see Introduction to Data Management on OSG Connect . Follow the these same steps for creating a tar archive of the xz-5.2.5 library as well. To clean up and clear out space in your home directory, we recommend deleting the Samtools source code directory.","title":"Step 6. Make our software portable"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/example-compilation/#step-7-use-samtools-in-our-jobs","text":"Now that Samtools has been compiled we can submit jobs that use this software. For Samtools with CRAM we will also need to bring along a copy of XZ Utils (which includes the liblzma library) and ensure that our jobs have access to the bzip2 1.0.6 module. Below is an example submit file for a job that will use Samtools with a Fasta file genome.fa' and CRAM file named my-sample.cram` which is <100MB in size: #samtools-cram.sub log = samtools-cram.$(Cluster).log error = samtools-cram.$(Cluster)_$(Process).err output = samtools-cram.$(Cluster)_$(Process).out executable = samtools-cram.sh transfer_input_files = /home/username/my-software/samtools-1.10.tar.gz, /home/username/my-software/xz-5.2.5.tar.gz, genome.fa, my-sample.cram should_transfer_files = YES when_to_transfer_output = ON_EXIT requirements = (OSGVO_OS_STRING == \"RHEL 7\") && (HAS_MODULES =?= TRUE) request_memory = 1.3GB request_disk = 1.5GB request_cpus = 1 queue 1 The above submit file will transfer a complete copy of the Samtools tar archive created in Step 6 as well as a copy of XZ Utils installation from Step 3 . This submit file also includes two important requirements which tell HTCondor to run our job on execute nodes running Red Hat Linux version 7 operating system and which has access to OSG Connect software modules. The resource requests for your jobs may differ from what is shown in the above example. Always run tests to determine the appropriate requests for your jobs. Some additional steps are then needed in the executable bash script used by this job to \"untar\" the Samtools and XZ Util tar archives, modify the PATH and LD_LIBRARY_PATH enviroments of our job, and load the bzip2 module: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash # samtools-cram.sh # untar software and libraries tar -xzf samtools-1.10.tar.gz tar -xzf xz-5.2.5.tar.gz # modify environment variables export LD_LIBRARY_PATH = $_CONDOR_SCRATCH_DIR /xz-5.2.5/lib: $LD_LIBRARY_PATH export PATH = $_CONDOR_SCRATCH_DIR /samtools-1.10/bin: $_CONDOR_SCRATCH_DIR /xz-5.2.5/bin: $PATH # load bzip2 module module load bzip2/1.0.6 # run samtools commands ...","title":"Step 7. Use Samtools in our jobs"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/","text":"Access Software using Distributed Environment Modules \u00b6 Introduction \u00b6 This page covers the use of distributed environment modules on RHEL7 compute nodes in the OSG computing environment. Environment modules provide users with an easy way to access different versions of software, libraries, and compilers. Currently, OSG Connect login nodes and a majority of OSG compute nodes use the RHEL7 operating system. However, given the distributed nature of the OSG, there are at times a number of RHEL6 and RHEL8 compute nodes also available to users. For users that require, or who can also use, RHEL6 compute nodes for their applications, please contact us to learn more about modules specifically available to RHEL6 compute nodes. List Available Modules on OSG Connect \u00b6 First sign in to your OSG Connect login node. Then use module avail to see all available software applications and libraries: $ module avail ------------ /cvmfs/connect.opensciencegrid.org/modules/modulefiles/linux-rhel7-x86_64/Core ------------ autoconf/2.69 libiconv/1.15 py-idna/2.5-py3.7 (D) automake/1.16.1 libjpeg-turbo/1.5.3 py-ipaddress/1.0.18-py2.7 binutils/2.31.1-py2.7 libjpeg-turbo/1.5.90 (D) py-kiwisolver/1.0.1-py2.7 binutils/2.31.1-py3.7 (D) libpciaccess/0.13.5 py-kiwisolver/1.0.1-py3.7 (D) bison/3.0.5-py2.7 libpng/1.6.34 py-lit/0.5.0-py2.7 bison/3.0.5 (D) libpthread-stubs/0.4 py-mako/1.0.4-py2.7 boost/1.68.0-py2.7 libsigsegv/2.11 py-markupsafe/1.0-py2.7 boost/1.68.0-py3.7 (D) libsm/1.2.2 py-matplotlib/2.2.3-py2.7 bowtie2/2.3.4.1-py2.7 libtiff/4.0.9 py-matplotlib/3.0.0-py3.7 (D) bullet3/2.87 libtool/2.4.6 py-nose/1.3.7-py2.7 bwa/0.7.17 libx11/1.6.5 py-numexpr/2.6.5-py2.7 bzip2/1.0.6 libxau/1.0.8 py-numexpr/2.6.5-py3.7 (D) cairo/1.14.12-py2.7 libxcb/1.13 py-numpy/1.15.2-py2.7 cctools/7.0.8-py2.7 libxdamage/1.1.4 py-numpy/1.15.2-py3.7 (D) cfitsio/3.450 libxdmcp/1.1.2 py-pandas/0.23.4-py2.7 charmpp/6.8.2 libxext/1.3.3 py-pandas/0.23.4-py3.7 (D) ... more modules ... Where: D: Default Module Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Please know, there are more than 200 modules available to OSG Connect users. The above is not a complete list of all available modules as the list is quite long and subject to change as new sofware and libraries get added. How To Use Environment Modules \u00b6 In order to use the software or libraries provided in a module, you will need to 'load' that particular module. To load a particular module, use module load [modulename] . For example: $ module load python/2.7.14-k To see currently loaded modules, use module list . For example: $ module list Currently Loaded Modules: 1) python/2.7.14-k To unload a package and remove a module from your environment, use module unload [modulename] . For example: $ module unload python/2.7.14-k Finally, module help will give you more detailed information. Use Environment Modules in Jobs \u00b6 Add module commands to executable script \u00b6 To use environment modules within a job, use the same module load command described above inside your job's main \"executable\" to load your software and then run it. For example: 1 2 3 4 #!/bin/bash module load python/2.7.14-k python myscript.py Set appropriate submit file requirements \u00b6 Not all resources available through OSG Connect support distributed environment modules. In order to ensure that your jobs will have access to OSG Connect modules, you will need to add the following to your HTCondor submit file: Requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") && (OpSys == \"LINUX\") or append the above requirements if you already have other requirements specified: Requirements = [Other requirements ] && (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") && (OpSys == \"LINUX\")","title":"Access Software using Distributed Environment Modules"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#access-software-using-distributed-environment-modules","text":"","title":"Access Software using Distributed Environment Modules"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#introduction","text":"This page covers the use of distributed environment modules on RHEL7 compute nodes in the OSG computing environment. Environment modules provide users with an easy way to access different versions of software, libraries, and compilers. Currently, OSG Connect login nodes and a majority of OSG compute nodes use the RHEL7 operating system. However, given the distributed nature of the OSG, there are at times a number of RHEL6 and RHEL8 compute nodes also available to users. For users that require, or who can also use, RHEL6 compute nodes for their applications, please contact us to learn more about modules specifically available to RHEL6 compute nodes.","title":"Introduction"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#list-available-modules-on-osg-connect","text":"First sign in to your OSG Connect login node. Then use module avail to see all available software applications and libraries: $ module avail ------------ /cvmfs/connect.opensciencegrid.org/modules/modulefiles/linux-rhel7-x86_64/Core ------------ autoconf/2.69 libiconv/1.15 py-idna/2.5-py3.7 (D) automake/1.16.1 libjpeg-turbo/1.5.3 py-ipaddress/1.0.18-py2.7 binutils/2.31.1-py2.7 libjpeg-turbo/1.5.90 (D) py-kiwisolver/1.0.1-py2.7 binutils/2.31.1-py3.7 (D) libpciaccess/0.13.5 py-kiwisolver/1.0.1-py3.7 (D) bison/3.0.5-py2.7 libpng/1.6.34 py-lit/0.5.0-py2.7 bison/3.0.5 (D) libpthread-stubs/0.4 py-mako/1.0.4-py2.7 boost/1.68.0-py2.7 libsigsegv/2.11 py-markupsafe/1.0-py2.7 boost/1.68.0-py3.7 (D) libsm/1.2.2 py-matplotlib/2.2.3-py2.7 bowtie2/2.3.4.1-py2.7 libtiff/4.0.9 py-matplotlib/3.0.0-py3.7 (D) bullet3/2.87 libtool/2.4.6 py-nose/1.3.7-py2.7 bwa/0.7.17 libx11/1.6.5 py-numexpr/2.6.5-py2.7 bzip2/1.0.6 libxau/1.0.8 py-numexpr/2.6.5-py3.7 (D) cairo/1.14.12-py2.7 libxcb/1.13 py-numpy/1.15.2-py2.7 cctools/7.0.8-py2.7 libxdamage/1.1.4 py-numpy/1.15.2-py3.7 (D) cfitsio/3.450 libxdmcp/1.1.2 py-pandas/0.23.4-py2.7 charmpp/6.8.2 libxext/1.3.3 py-pandas/0.23.4-py3.7 (D) ... more modules ... Where: D: Default Module Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Please know, there are more than 200 modules available to OSG Connect users. The above is not a complete list of all available modules as the list is quite long and subject to change as new sofware and libraries get added.","title":"List Available Modules on OSG Connect"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#how-to-use-environment-modules","text":"In order to use the software or libraries provided in a module, you will need to 'load' that particular module. To load a particular module, use module load [modulename] . For example: $ module load python/2.7.14-k To see currently loaded modules, use module list . For example: $ module list Currently Loaded Modules: 1) python/2.7.14-k To unload a package and remove a module from your environment, use module unload [modulename] . For example: $ module unload python/2.7.14-k Finally, module help will give you more detailed information.","title":"How To Use Environment Modules"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#use-environment-modules-in-jobs","text":"","title":"Use Environment Modules in Jobs"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#add-module-commands-to-executable-script","text":"To use environment modules within a job, use the same module load command described above inside your job's main \"executable\" to load your software and then run it. For example: 1 2 3 4 #!/bin/bash module load python/2.7.14-k python myscript.py","title":"Add module commands to executable script"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/new_modules_list/#set-appropriate-submit-file-requirements","text":"Not all resources available through OSG Connect support distributed environment modules. In order to ensure that your jobs will have access to OSG Connect modules, you will need to add the following to your HTCondor submit file: Requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") && (OpSys == \"LINUX\") or append the above requirements if you already have other requirements specified: Requirements = [Other requirements ] && (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 7\") && (OpSys == \"LINUX\")","title":"Set appropriate submit file requirements"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/requirements/","text":"Control Where Your Jobs Run / Job Requirements \u00b6 By default, your jobs will match any available spot in the OSG. This is fine for very generic jobs. However, in some cases a job may have one or more system requirements in order to complete successfully. For instance, your job may need to run on a node with a specific operating system. HTCondor provides several options for \"steering\" your jobs to appropriate nodes and system environments. The request_cpus , request_gpus , request_memory , and request_disk submit file attributes should be used to specify the hardware needs of your jobs. Please see our guides Multicore Jobs and Large Memory Jobs for more details. HTCondor also provides a requirements attribute and feature-specific attributes that can be added to your submit files to target specific environments in which to run your jobs. Lastly, there are some custom attributes you can add to your submit file to either focus on, or avoid, certain execution sites. Requirements \u00b6 The requirements attribute is formatted as an expression, so you can use logical operators to combine multiple requirements where && is used for AND and || used for OR. For example, the following requirements statement will direct jobs only to 64 bit RHEL (Red Hat Enterprise Linux) 8 nodes. requirements = OSGVO_OS_STRING == \"RHEL 8\" && Arch == \"X86_64\" Alternatively, if you have code which can run on either RHEL 7 or 8, you can use OR: requirements = (OSGVO_OS_STRING == \"RHEL 7\" || OSGVO_OS_STRING == \"RHEL 8\") && Arch == \"X86_64\" Note that parentheses placement is important for controling how the logical operations are interpreted by HTCondor. Another common requirement is to land on a node which has CVMFS. Then the requirements would be: requirements = HAS_oasis_opensciencegrid_org == True Additional Feature-Specific Attributes \u00b6 There are many attributes that you can use with requirements . To see what values you can specify for a given attribute you can run the following command while connected to your login node: $ condor_status -af {ATTR_NAME} | sort -u For example, to see what values you can specify for the OSGVO_OS_STRING attribute run: $ condor_status -af OSGVO_OS_STRING | sort -u RHEL 7 RHEL 8 This means that we can specify an OS version of RHEL 7 or RHEL 8 . Alternatively you will find many attributes will take the boolean values true or false . Below is a list of common attributes that you can include in your submit file requirements statement. HAS_SINGULARITY - Boolean specifying the need to use Singularity containers in your job. HAS_MODULES - Boolean specifying the need to use modules in your job. module load ... or not. OSGVO_OS_NAME - The name of the operating system of the compute node. The most common name is RHEL OSGVO_OS_VERSION - Version of the operating system OSGVO_OS_STRING - Combined OS name and version. Common values are RHEL 7 and RHEL 8 . Please see the requirements string above on the recommended setup. OSGVO_CPU_MODEL - The CPU model identifier string as presented in /proc/cpuinfo HAS_CVMFS_oasis_opensciencegrid_org - Attribute specifying the need to access specific oasis /cvmfs file system repositories. CUDACapability - For GPU jobs, specifies the CUDA compute capability. See our GPU guide for more details. Specifying Sites / Avoiding Sites \u00b6 To run your jobs on a list of specific execution sites, or avoid a set of sites, use the +DESIRED_Sites / +UNDESIRED_Sites attributes in your job submit file. These attributes should only be used as a last resort. For example, it is much better to use feature attributes (see above) to make your job go to nodes matching what you really require, than to broadly allow/block whole sites. We encourage you to contact the facilitation team before taking this action, to make sure it is right for you. To avoid certain sites, first find the site names. You can find a current list by querying the pool: condor_status -af GLIDEIN_Site | sort -u In your submit file, add a comma separated list of sites like: +UNDESIRED_Sites = \"ISI,SU-ITS\" Those sites will now be exluded from the set of sites your job can run at. Similarly, you can use +DESIRED_Sites to list a subset of sites you want to target. For example, to run your jobs at the SU-ITS site, and only at that site, use: +DESIRED_Sites = \"ISI,SU-ITS\" Note that you should only specify one of +DESIRED_Sites / +UNDESIRED_Sites in the submit file. Using both at the same time will prevent the job from running.","title":"Control Where Your Jobs Run/Job Requirements"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/requirements/#control-where-your-jobs-run-job-requirements","text":"By default, your jobs will match any available spot in the OSG. This is fine for very generic jobs. However, in some cases a job may have one or more system requirements in order to complete successfully. For instance, your job may need to run on a node with a specific operating system. HTCondor provides several options for \"steering\" your jobs to appropriate nodes and system environments. The request_cpus , request_gpus , request_memory , and request_disk submit file attributes should be used to specify the hardware needs of your jobs. Please see our guides Multicore Jobs and Large Memory Jobs for more details. HTCondor also provides a requirements attribute and feature-specific attributes that can be added to your submit files to target specific environments in which to run your jobs. Lastly, there are some custom attributes you can add to your submit file to either focus on, or avoid, certain execution sites.","title":"Control Where Your Jobs Run / Job Requirements"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/requirements/#requirements","text":"The requirements attribute is formatted as an expression, so you can use logical operators to combine multiple requirements where && is used for AND and || used for OR. For example, the following requirements statement will direct jobs only to 64 bit RHEL (Red Hat Enterprise Linux) 8 nodes. requirements = OSGVO_OS_STRING == \"RHEL 8\" && Arch == \"X86_64\" Alternatively, if you have code which can run on either RHEL 7 or 8, you can use OR: requirements = (OSGVO_OS_STRING == \"RHEL 7\" || OSGVO_OS_STRING == \"RHEL 8\") && Arch == \"X86_64\" Note that parentheses placement is important for controling how the logical operations are interpreted by HTCondor. Another common requirement is to land on a node which has CVMFS. Then the requirements would be: requirements = HAS_oasis_opensciencegrid_org == True","title":"Requirements"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/requirements/#additional-feature-specific-attributes","text":"There are many attributes that you can use with requirements . To see what values you can specify for a given attribute you can run the following command while connected to your login node: $ condor_status -af {ATTR_NAME} | sort -u For example, to see what values you can specify for the OSGVO_OS_STRING attribute run: $ condor_status -af OSGVO_OS_STRING | sort -u RHEL 7 RHEL 8 This means that we can specify an OS version of RHEL 7 or RHEL 8 . Alternatively you will find many attributes will take the boolean values true or false . Below is a list of common attributes that you can include in your submit file requirements statement. HAS_SINGULARITY - Boolean specifying the need to use Singularity containers in your job. HAS_MODULES - Boolean specifying the need to use modules in your job. module load ... or not. OSGVO_OS_NAME - The name of the operating system of the compute node. The most common name is RHEL OSGVO_OS_VERSION - Version of the operating system OSGVO_OS_STRING - Combined OS name and version. Common values are RHEL 7 and RHEL 8 . Please see the requirements string above on the recommended setup. OSGVO_CPU_MODEL - The CPU model identifier string as presented in /proc/cpuinfo HAS_CVMFS_oasis_opensciencegrid_org - Attribute specifying the need to access specific oasis /cvmfs file system repositories. CUDACapability - For GPU jobs, specifies the CUDA compute capability. See our GPU guide for more details.","title":"Additional Feature-Specific Attributes"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/requirements/#specifying-sites-avoiding-sites","text":"To run your jobs on a list of specific execution sites, or avoid a set of sites, use the +DESIRED_Sites / +UNDESIRED_Sites attributes in your job submit file. These attributes should only be used as a last resort. For example, it is much better to use feature attributes (see above) to make your job go to nodes matching what you really require, than to broadly allow/block whole sites. We encourage you to contact the facilitation team before taking this action, to make sure it is right for you. To avoid certain sites, first find the site names. You can find a current list by querying the pool: condor_status -af GLIDEIN_Site | sort -u In your submit file, add a comma separated list of sites like: +UNDESIRED_Sites = \"ISI,SU-ITS\" Those sites will now be exluded from the set of sites your job can run at. Similarly, you can use +DESIRED_Sites to list a subset of sites you want to target. For example, to run your jobs at the SU-ITS site, and only at that site, use: +DESIRED_Sites = \"ISI,SU-ITS\" Note that you should only specify one of +DESIRED_Sites / +UNDESIRED_Sites in the submit file. Using both at the same time will prevent the job from running.","title":"Specifying Sites / Avoiding Sites"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/","text":"Using Software on the Open Science Pool \u00b6 Overview of Software Options \u00b6 There are several options available for managing the software needs of your work within the Open Science Pool (OSPool). In general, the OSPool can support most popular, open source software that fit the distributed high throughput computing model. At present, we do not have or support most commercial software due to licensing issues. Here we review options, and provide links to additonal information, for using software installed by users, software available as precompiled binaries or via containers, and preinstalled software via modules. Install Your Own Software \u00b6 For most cases, it will be advantageous for you to install the software needed for your jobs. This not only gives you the greatest control over your computing environment, but will also make your jobs more distributable, allowing you to run jobs at more locations. Installing your own software can be performed interactively on your assigned login server. More information about how to install your own software from source code can be found at Compiling Software for the OSPool . When installing software on an OSG Connect login node, your software will be specifically compiled against the Red Hat Enterprise Linux (RHEL) 7 OS used on these nodes. In most cases, subsequent jobs that use this software will also need to run on a RHEL 7 OS, which can be specified by the requirements attribute of your HTCondor submit files - an example is provided in the software compilation guide linked above. Be sure to also review our Introduction to Data Management on OSG Connect guide to determine the appropriate method for transferring software with your jobs. Use Precompiled Binaries and Prebuilt Executables \u00b6 Some software may be available as a precompiled binary or prebuilt executable which provides a quick and easy way to run a program without the need for installation from source code. Binaries and executables are software files that are ready to run as is, however binaries should always be tested beforehand. There are several important considerations for using precompiled binaries on the OSPool: 1.) only binary files compiled against a Linux operating system are suitable for use on the OSPool, 2.) some softwares have system and hardware dependencies that must be met in order to run properly, and 3.) the available binaries may not have been compiled with the feaures or configuration needed for your work. Use Docker and Singularity Containers \u00b6 Container systems provide users with customizable and reproducable computing and software environments. The Open Science Pool is compatible with both Singularity and Docker containers - the latter will be converted to a Singularity image and added to the OSG container image repository. Users can choose from a set of pre-defined containers already available within OSG, or can use published or custom made containers. More details on how to use containers on the OSPool can be found in our Docker and Singularity Containers guide. Access Software In Distributed Modules \u00b6 Currently, the OSPool provides over 200 preinstalled software, libraries, and compiliers via distributed environment modules. Modules provide a streamlined option for working with different software versions as well as managing library dependencies for software. To run jobs that use modules, your HTCondor submit files must include additional requirements to direct your jobs to appropriate compute nodes within OSG. More details about using modules can be found here . Ask for Helpk \u00b6 If you are not sure which of the above options might be best for your software, we're happy to help! Just contact us at support@opensciencegrid.org . Watch this video from the 2021 OSG Virtual School for more information about using software on OSG:","title":"Using Software on the Open Science Pool"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#using-software-on-the-open-science-pool","text":"","title":"Using Software on the Open Science Pool"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#overview-of-software-options","text":"There are several options available for managing the software needs of your work within the Open Science Pool (OSPool). In general, the OSPool can support most popular, open source software that fit the distributed high throughput computing model. At present, we do not have or support most commercial software due to licensing issues. Here we review options, and provide links to additonal information, for using software installed by users, software available as precompiled binaries or via containers, and preinstalled software via modules.","title":"Overview of Software Options"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#install-your-own-software","text":"For most cases, it will be advantageous for you to install the software needed for your jobs. This not only gives you the greatest control over your computing environment, but will also make your jobs more distributable, allowing you to run jobs at more locations. Installing your own software can be performed interactively on your assigned login server. More information about how to install your own software from source code can be found at Compiling Software for the OSPool . When installing software on an OSG Connect login node, your software will be specifically compiled against the Red Hat Enterprise Linux (RHEL) 7 OS used on these nodes. In most cases, subsequent jobs that use this software will also need to run on a RHEL 7 OS, which can be specified by the requirements attribute of your HTCondor submit files - an example is provided in the software compilation guide linked above. Be sure to also review our Introduction to Data Management on OSG Connect guide to determine the appropriate method for transferring software with your jobs.","title":"Install Your Own Software"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#use-precompiled-binaries-and-prebuilt-executables","text":"Some software may be available as a precompiled binary or prebuilt executable which provides a quick and easy way to run a program without the need for installation from source code. Binaries and executables are software files that are ready to run as is, however binaries should always be tested beforehand. There are several important considerations for using precompiled binaries on the OSPool: 1.) only binary files compiled against a Linux operating system are suitable for use on the OSPool, 2.) some softwares have system and hardware dependencies that must be met in order to run properly, and 3.) the available binaries may not have been compiled with the feaures or configuration needed for your work.","title":"Use Precompiled Binaries and Prebuilt Executables"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#use-docker-and-singularity-containers","text":"Container systems provide users with customizable and reproducable computing and software environments. The Open Science Pool is compatible with both Singularity and Docker containers - the latter will be converted to a Singularity image and added to the OSG container image repository. Users can choose from a set of pre-defined containers already available within OSG, or can use published or custom made containers. More details on how to use containers on the OSPool can be found in our Docker and Singularity Containers guide.","title":"Use Docker and Singularity Containers"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#access-software-in-distributed-modules","text":"Currently, the OSPool provides over 200 preinstalled software, libraries, and compiliers via distributed environment modules. Modules provide a streamlined option for working with different software versions as well as managing library dependencies for software. To run jobs that use modules, your HTCondor submit files must include additional requirements to direct your jobs to appropriate compute nodes within OSG. More details about using modules can be found here .","title":"Access Software In Distributed Modules"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-overview/#ask-for-helpk","text":"If you are not sure which of the above options might be best for your software, we're happy to help! Just contact us at support@opensciencegrid.org . Watch this video from the 2021 OSG Virtual School for more information about using software on OSG:","title":"Ask for Helpk"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-request/","text":"Request Help with Your Software \u00b6 A large number of software packages can be used by compiling a portable installation or using a container (many community sofwares are already available in authoritative containers). If you believe none of these options ( described here ) are applicable for your software, please get in touch with a simple email to [support@opensciencegrid.org][support] that describes: 1. the software name, version, and/or website with download and install instructions 2. what science each job does, using the software 3. what you've tried so far (if anything), and what indications of issues you've experienced As long as this code is: available to the public in source form (e.g. open source) licensed to all users, and does not require a license key would not be better supported by another approach (which are usually preferable) we should be able to help you create a portable installation with the 'right' solution.","title":"Request Help with Your Software"},{"location":"managing_htc_workloads_on_osg_connect/using_software_on_the_osg/software-request/#request-help-with-your-software","text":"A large number of software packages can be used by compiling a portable installation or using a container (many community sofwares are already available in authoritative containers). If you believe none of these options ( described here ) are applicable for your software, please get in touch with a simple email to [support@opensciencegrid.org][support] that describes: 1. the software name, version, and/or website with download and install instructions 2. what science each job does, using the software 3. what you've tried so far (if anything), and what indications of issues you've experienced As long as this code is: available to the public in source form (e.g. open source) licensed to all users, and does not require a license key would not be better supported by another approach (which are usually preferable) we should be able to help you create a portable installation with the 'right' solution.","title":"Request Help with Your Software"},{"location":"overview/references/acknowledgeOSG/","text":"Acknowledge the OSG \u00b6 This page has been moved to the OSG Website .","title":"Acknowledge the OSG"},{"location":"overview/references/acknowledgeOSG/#acknowledge-the-osg","text":"This page has been moved to the OSG Website .","title":"Acknowledge the OSG"},{"location":"overview/references/contact-information/","text":"Contact OSG for non-Support Inquiries \u00b6 For media contact, leadership, or general questions about OSG, please see our main website To request a user account on OSG Connect, please visit the website . For any assistance or technical questions regarding jobs or data, please see our page on how to Get Help and/or contact the OSG Research Facilitation team at support@opensciencegrid.org","title":"Contact OSG for non-Support Inquiries"},{"location":"overview/references/contact-information/#contact-osg-for-non-support-inquiries","text":"For media contact, leadership, or general questions about OSG, please see our main website To request a user account on OSG Connect, please visit the website . For any assistance or technical questions regarding jobs or data, please see our page on how to Get Help and/or contact the OSG Research Facilitation team at support@opensciencegrid.org","title":"Contact OSG for non-Support Inquiries"},{"location":"overview/references/frequently-asked-questions-faq-/","text":"Frequently Asked Questions \u00b6 Getting Started \u00b6 Who is eligible to become the user of OSG Connect? Any researcher affiliated with a U.S. institution (college, university, national laboratory or research foundation) is eligible to become an OSG Connect user. Researchers outside of the U.S. with affiliations to U.S. groups may be eligible for membership if they are sponsored by a collaborator within the U.S. Researchers outside of the U.S. are asked to first contact us directly to discuss membership. How do I become an user of OSG Connect? Please follow the steps outlined in the Sign Up process . Software \u00b6 What software packages are available? In general, we support most software that fit the distributed high throughput computing model. Users are encouraged to download and install their own software. For some software, we support distributed software modules listed here . Software can be added to the modules upon request. Additionally, users may install their software into a Docker container which can run on OSG as a Singularity image. See this guide for more information. How do I access a specific software application? We have implemented modules within OSG Connect to manage the software that is available to users. Modules allow for easy access to a number of software and version options. Our Accessing Software using Distributed Environment Modules page provides more details on how to use modules in OSG Connect. Are there any restrictions on installing commercial softwares? We can only directly support software that is freely distributable. At present, we do not have or support most commercial software due to licensing issues. (One exception is running MATLAB standalone executables which have been compiled with the MATLAB Compiler Runtime). Software that is licensed to individual users (and not to be shared between users) can be staged within the user's /home directory with HTCondor transferring to jobs, but should not be staged in OSG's public data staging locations (see ../../managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/-data-management-and-policies). Please get in touch with any questions about licensed software. Can I request for system wide installation of the open source software useful for my research? Yes. Please contact support@opensciencegrid.org . Running Jobs \u00b6 What type of computation is a good match or NOT a good match for OSG Connect? OSG Connect is a high throughput computing system. You can get the most of out OSG Connect resources by breaking up a single large computational task into many smaller tasks for the fastest overall turnaround. This approach can be invaluable in accelerating your computational work and thus your research. Please see our \"Is OSG for You?\" page for more details on how to determine if your work matches up well with OSG Connect's model. What job scheduler is being used on OSG Connect? We use use the task scheduling software called HTCondor to schedule and run jobs. How do I submit a computing job? Jobs are submitted via HTCondor scheduler. Please see our QuickStart guide for more details on submitting and managing jobs. How many jobs can I have in the queue? The number of jobs that are submitted to the queue by any one user should not exceed 10,000. If you have more jobs than that, we ask that you include the following statement in your submit file: max_idle = 2000 This is the maximum number of jobs that you will have in the \"Idle\" or \"Held\" state for the submitted batch of jobs at any given time. Using a value of 2000 will ensure that your jobs continue to apply a constant pressure on the queue, but will not fill up the queue unnecessarily (which helps the scheduler to perform optimally). Data Storage and Transfer \u00b6 What is the best way to process large volume of data? Use the Stash data system to stage large volumes of data. Please refer the section Data Solutions for more details. How do I transfer my data to and from OSG Connect? You can transfer data using scp or rsync. See Using scp To Transfer Files To OSG Connect for more details. How public is /public? The data under your /public location is discoverable and readable by anyone in the world. Data in /public is made public over http/https (via https://stash.osgconnect.net/public/ ) and mirrored to /cvmfs/stash.osgstorage.org/osgconnect/public/ (for use with stashcp ) which is mounted on a large number of systems around the world. Is there any support for private data? OSG currently has no storage appropriate for HIPAA data. If you do not want your data to be downloadable by anyone, and it\u2019s small enough for HTCondor file transfer (i.e. <100MB per file and <500MB total per job), then it should be staged in your /home directory and transferred to jobs with HTCondor file transfer ( transfer_input_files , in the submit file). If your data must remain private and is too large for HTCondor file transfer, then it\u2019s not a good fit for the \u201copen\u201d environment of the Open Science Pool, and another resource will likely be more appropriate. As a reminder, if the data is not being used for active computing work on OSG Connect, it should not be stored on OSG Connect systems. Lastly, our data storage locations are not backed up nor are they intended for long-term storage. Can I get a quota increase? Contact us if you think you'll need a quota increase for /home or /public to accommodate a set of concurrently-running jobs. We can suppport very large amounts of data, the default quotas are just a starting point. Will I get notified about hitting quota limits? The only place you can currently see your quota status is in the login messages. Workflow Management \u00b6 How do I run and manage complex workflows? For workflows that have multiple steps and/or multiple files to, we advise using a workflow management system. A workflow management system allows you to define different computational steps in your workflow and indicate how inputs and outputs should be transferred between these steps. Once you define a workflow, the workflow management system will then run your workflow, automatically retrying failed jobs and transferrring files between different steps. What workflow management systems are recommended on OSG? We support and distribute DAGMan, Pegasus, and Swift for workflow management. Workshops and Training \u00b6 Do you plan to offer training sessions and workshop? We plan to offer workshops for the researchers on multiple locations, including an annual, week-long summer school for OSG users. Please check our events page for further information about workshop dates and locations. Who may attend OSG workshops? Workshops are typically open to students, post docs, staff and faculty. What are the topics covered in a typical workshop? We typically cover shell scripting, python (or R) programming, version control with git and distributed high throughout computing. How to cite or acknowledge OSG? Whenever you make use of OSG resources, services or tools, we would be grateful to have you acknowledge OSG in your presentations and publications. For example, you can add the following in your acknowledgements section: \"This research was done using resources provided by the OSG, which is supported by the National Science Foundation and the U.S. Department of Energy's Office of Science.\" We recommend the following references for citations 1) Pordes, R. et al. (2007). \"The Open Science Grid\", J. Phys. Conf. Ser. 78, 012057.doi:10.1088/1742-6596/78/1/012057. 2) Sfiligoi, I., Bradley, D. C., Holzman, B., Mhashilkar, P., Padhi, S. and Wurthwein, F. (2009). \"The Pilot Way to Grid Resources Using glideinWMS\", 2009 WRI World Congress on Computer Science and Information Engineering, Vol. 2, pp. 428\u2013432. doi:10.1109/CSIE.2009.950.","title":"Frequently Asked Questions"},{"location":"overview/references/frequently-asked-questions-faq-/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"overview/references/frequently-asked-questions-faq-/#getting-started","text":"Who is eligible to become the user of OSG Connect? Any researcher affiliated with a U.S. institution (college, university, national laboratory or research foundation) is eligible to become an OSG Connect user. Researchers outside of the U.S. with affiliations to U.S. groups may be eligible for membership if they are sponsored by a collaborator within the U.S. Researchers outside of the U.S. are asked to first contact us directly to discuss membership. How do I become an user of OSG Connect? Please follow the steps outlined in the Sign Up process .","title":"Getting Started"},{"location":"overview/references/frequently-asked-questions-faq-/#software","text":"What software packages are available? In general, we support most software that fit the distributed high throughput computing model. Users are encouraged to download and install their own software. For some software, we support distributed software modules listed here . Software can be added to the modules upon request. Additionally, users may install their software into a Docker container which can run on OSG as a Singularity image. See this guide for more information. How do I access a specific software application? We have implemented modules within OSG Connect to manage the software that is available to users. Modules allow for easy access to a number of software and version options. Our Accessing Software using Distributed Environment Modules page provides more details on how to use modules in OSG Connect. Are there any restrictions on installing commercial softwares? We can only directly support software that is freely distributable. At present, we do not have or support most commercial software due to licensing issues. (One exception is running MATLAB standalone executables which have been compiled with the MATLAB Compiler Runtime). Software that is licensed to individual users (and not to be shared between users) can be staged within the user's /home directory with HTCondor transferring to jobs, but should not be staged in OSG's public data staging locations (see ../../managing_htc_workloads_on_osg_connect/managing_data_for_jobs/osgconnect-storage/-data-management-and-policies). Please get in touch with any questions about licensed software. Can I request for system wide installation of the open source software useful for my research? Yes. Please contact support@opensciencegrid.org .","title":"Software"},{"location":"overview/references/frequently-asked-questions-faq-/#running-jobs","text":"What type of computation is a good match or NOT a good match for OSG Connect? OSG Connect is a high throughput computing system. You can get the most of out OSG Connect resources by breaking up a single large computational task into many smaller tasks for the fastest overall turnaround. This approach can be invaluable in accelerating your computational work and thus your research. Please see our \"Is OSG for You?\" page for more details on how to determine if your work matches up well with OSG Connect's model. What job scheduler is being used on OSG Connect? We use use the task scheduling software called HTCondor to schedule and run jobs. How do I submit a computing job? Jobs are submitted via HTCondor scheduler. Please see our QuickStart guide for more details on submitting and managing jobs. How many jobs can I have in the queue? The number of jobs that are submitted to the queue by any one user should not exceed 10,000. If you have more jobs than that, we ask that you include the following statement in your submit file: max_idle = 2000 This is the maximum number of jobs that you will have in the \"Idle\" or \"Held\" state for the submitted batch of jobs at any given time. Using a value of 2000 will ensure that your jobs continue to apply a constant pressure on the queue, but will not fill up the queue unnecessarily (which helps the scheduler to perform optimally).","title":"Running Jobs"},{"location":"overview/references/frequently-asked-questions-faq-/#data-storage-and-transfer","text":"What is the best way to process large volume of data? Use the Stash data system to stage large volumes of data. Please refer the section Data Solutions for more details. How do I transfer my data to and from OSG Connect? You can transfer data using scp or rsync. See Using scp To Transfer Files To OSG Connect for more details. How public is /public? The data under your /public location is discoverable and readable by anyone in the world. Data in /public is made public over http/https (via https://stash.osgconnect.net/public/ ) and mirrored to /cvmfs/stash.osgstorage.org/osgconnect/public/ (for use with stashcp ) which is mounted on a large number of systems around the world. Is there any support for private data? OSG currently has no storage appropriate for HIPAA data. If you do not want your data to be downloadable by anyone, and it\u2019s small enough for HTCondor file transfer (i.e. <100MB per file and <500MB total per job), then it should be staged in your /home directory and transferred to jobs with HTCondor file transfer ( transfer_input_files , in the submit file). If your data must remain private and is too large for HTCondor file transfer, then it\u2019s not a good fit for the \u201copen\u201d environment of the Open Science Pool, and another resource will likely be more appropriate. As a reminder, if the data is not being used for active computing work on OSG Connect, it should not be stored on OSG Connect systems. Lastly, our data storage locations are not backed up nor are they intended for long-term storage. Can I get a quota increase? Contact us if you think you'll need a quota increase for /home or /public to accommodate a set of concurrently-running jobs. We can suppport very large amounts of data, the default quotas are just a starting point. Will I get notified about hitting quota limits? The only place you can currently see your quota status is in the login messages.","title":"Data Storage and Transfer"},{"location":"overview/references/frequently-asked-questions-faq-/#workflow-management","text":"How do I run and manage complex workflows? For workflows that have multiple steps and/or multiple files to, we advise using a workflow management system. A workflow management system allows you to define different computational steps in your workflow and indicate how inputs and outputs should be transferred between these steps. Once you define a workflow, the workflow management system will then run your workflow, automatically retrying failed jobs and transferrring files between different steps. What workflow management systems are recommended on OSG? We support and distribute DAGMan, Pegasus, and Swift for workflow management.","title":"Workflow Management"},{"location":"overview/references/frequently-asked-questions-faq-/#workshops-and-training","text":"Do you plan to offer training sessions and workshop? We plan to offer workshops for the researchers on multiple locations, including an annual, week-long summer school for OSG users. Please check our events page for further information about workshop dates and locations. Who may attend OSG workshops? Workshops are typically open to students, post docs, staff and faculty. What are the topics covered in a typical workshop? We typically cover shell scripting, python (or R) programming, version control with git and distributed high throughout computing. How to cite or acknowledge OSG? Whenever you make use of OSG resources, services or tools, we would be grateful to have you acknowledge OSG in your presentations and publications. For example, you can add the following in your acknowledgements section: \"This research was done using resources provided by the OSG, which is supported by the National Science Foundation and the U.S. Department of Energy's Office of Science.\" We recommend the following references for citations 1) Pordes, R. et al. (2007). \"The Open Science Grid\", J. Phys. Conf. Ser. 78, 012057.doi:10.1088/1742-6596/78/1/012057. 2) Sfiligoi, I., Bradley, D. C., Holzman, B., Mhashilkar, P., Padhi, S. and Wurthwein, F. (2009). \"The Pilot Way to Grid Resources Using glideinWMS\", 2009 WRI World Congress on Computer Science and Information Engineering, Vol. 2, pp. 428\u2013432. doi:10.1109/CSIE.2009.950.","title":"Workshops and Training"},{"location":"overview/references/gracc/","text":"OSG Accounting (GRACC) \u00b6 GRACC is the Open Science Pool's accounting system. If you need graphs or high level statistics on your OSG usage, please go to: https://gracc.opensciencegrid.org/ To drill down on your own project and/or user, go to Payload Jobs . Under the Project or User drop-downs, find your project/user. You can select multiple ones. In the upper right corner, you can select a different time period. You can then select a different Group By time range. For example, if you want data for the last year grouped monthly, select \"Last 1 year\" for the Period , and \"1M\" for the Group By .","title":"OSG Accounting (GRACC)"},{"location":"overview/references/gracc/#osg-accounting-gracc","text":"GRACC is the Open Science Pool's accounting system. If you need graphs or high level statistics on your OSG usage, please go to: https://gracc.opensciencegrid.org/ To drill down on your own project and/or user, go to Payload Jobs . Under the Project or User drop-downs, find your project/user. You can select multiple ones. In the upper right corner, you can select a different time period. You can then select a different Group By time range. For example, if you want data for the last year grouped monthly, select \"Last 1 year\" for the Period , and \"1M\" for the Group By .","title":"OSG Accounting (GRACC)"},{"location":"overview/references/policy/","text":"Policies for Using OSG Connect and the OSPool \u00b6 Access to OSG Connect and the Open Science Pool (OSPool) is contingent on compliance with the below and with any requests from OSG staff to change practices that cause issues for OSG systems and/or users. Please contact us if you have any questions! We can often help with exceptions to default policies and/or identify available alternative approaches to help you with a perceived barrier. As the below do not cover every possible scenario of potentially disruptive practices, OSG staff reserve the right to take any necessary corrective actions to ensure performance and resource availability for all OSG Connect users. This may include the hold or removal of jobs, deletion of user data, deactivation of accounts, etc. In some cases, these actions may need to be taken without notifying the user. By using the OSG Connect service, users are expected to follow the Open Science Pool acceptable use policy , which includes appropriate scope of use and common user security practices. OSG Connect is only available to individuals affiliated with a US-based academic, government, or non-profit organization, or with a research project led by an affiliated sponsor. Users can have up to 10,000 jobs queued, without taking additional steps , and should submit multiple jobs via a single submit file, according to our online guides. Please write to us if you\u2019d like to easily submit more! Do not run computationally-intensive or persistent processes on the access points (login nodes). Exceptions include single-threaded software compilation and data management tasks (transfer to/from the login nodes, directory creation, file moving/renaming, untar-ing, etc.). The execution of multi-threaded tasks for job setup or post-processing or software testing will almost certainly cause performance issues and may result in loss of access. Software testing should be executed from within submitted jobs, where job scheduling also provides a more accurate test environment to the user without compromising performance of the login nodes. OSG staff reserve the right to kill any tasks running on the login nodes, in order to ensure performance for all users. Similarly, please contact us to discuss appropriate features and options, rather than running scripts (including cron ) to automate job submission , throttling, resubmission, or ordered execution (e.g. workflows), even if these are executed remotely to coordinate work on the OSG Connect login nodes. These almost always end up causing significant issues and/or wasted computing capacity, and we're happy to help you to implement automation tools the integrate with HTCondor. Data Policies : OSG Connect filesystems are not backed up and should be treated as temporary (\u201cscratch\u201d-like) space for active work, only , following OSG Connect policies for data storage and per-job transfers . Some OSG Connect storage spaces are truly \u2018open\u2019 with data available to be downloaded publicly. Of note: Users should keep copies of essential data and software in non-OSG locations, as OSG staff reserve the right to remove data at any time in order to ensure and/or restore system availability, and without prior notice to users. Proprietary data, HIPAA, and data with any other privacy concerns should not be stored on any OSG Connect filesystems or computed in OSG. Similarly, users should follow all licensing requirements when storing and executing software via OSG Connect submit servers. (See \u201cSoftware in OSG Connect\u201d.) Users should keep their /home directory privileges restricted to their user or group, and should not add \u2018global\u2019 permissions, which will allow other users to potentially make your data public. User-created \u2018open\u2019 network ports are disallowed , unless explicitly permitted following an accepted justification to support@opensciencegrid.org. (If you\u2019re not sure whether something you want to do will open a port, just get in touch!) The following actions may be taken automatedly or by OSG staff to stop or prevent jobs from causing problems. Please contact us if you\u2019d like help understanding why your jobs were held or removed, and so we can help you avoid problems in the future. Jobs using more memory or disk than requested may be automatically held (see Scaling Up after Test Jobs for tips on requesting the \u2018right\u2019 amount of job resources in your submit file). Jobs running longer than their JobDurationCategory allows for will be held (see Indicate the Job Duration Category of Your Jobs ). Jobs that have executed more than 30 times without completing may be automatically held (likely because they\u2019re too long for OSG). Jobs that have been held more than 14 days may be automatically removed. Jobs queued for more than three months may be automatically removed. Jobs otherwise causing known problems may be held or removed, without prior notification to the user. Held jobs may also be edited to prevent automated release/retry NOTE: in order to respect user email clients, job holds and removals do not come with specific notification to the user, unless configured by the user at the time of submission using HTCondor\u2019s \u2018notification\u2019 feature.","title":"Policies for Using OSG Connect and the OSPool "},{"location":"overview/references/policy/#policies-for-using-osg-connect-and-the-ospool","text":"Access to OSG Connect and the Open Science Pool (OSPool) is contingent on compliance with the below and with any requests from OSG staff to change practices that cause issues for OSG systems and/or users. Please contact us if you have any questions! We can often help with exceptions to default policies and/or identify available alternative approaches to help you with a perceived barrier. As the below do not cover every possible scenario of potentially disruptive practices, OSG staff reserve the right to take any necessary corrective actions to ensure performance and resource availability for all OSG Connect users. This may include the hold or removal of jobs, deletion of user data, deactivation of accounts, etc. In some cases, these actions may need to be taken without notifying the user. By using the OSG Connect service, users are expected to follow the Open Science Pool acceptable use policy , which includes appropriate scope of use and common user security practices. OSG Connect is only available to individuals affiliated with a US-based academic, government, or non-profit organization, or with a research project led by an affiliated sponsor. Users can have up to 10,000 jobs queued, without taking additional steps , and should submit multiple jobs via a single submit file, according to our online guides. Please write to us if you\u2019d like to easily submit more! Do not run computationally-intensive or persistent processes on the access points (login nodes). Exceptions include single-threaded software compilation and data management tasks (transfer to/from the login nodes, directory creation, file moving/renaming, untar-ing, etc.). The execution of multi-threaded tasks for job setup or post-processing or software testing will almost certainly cause performance issues and may result in loss of access. Software testing should be executed from within submitted jobs, where job scheduling also provides a more accurate test environment to the user without compromising performance of the login nodes. OSG staff reserve the right to kill any tasks running on the login nodes, in order to ensure performance for all users. Similarly, please contact us to discuss appropriate features and options, rather than running scripts (including cron ) to automate job submission , throttling, resubmission, or ordered execution (e.g. workflows), even if these are executed remotely to coordinate work on the OSG Connect login nodes. These almost always end up causing significant issues and/or wasted computing capacity, and we're happy to help you to implement automation tools the integrate with HTCondor. Data Policies : OSG Connect filesystems are not backed up and should be treated as temporary (\u201cscratch\u201d-like) space for active work, only , following OSG Connect policies for data storage and per-job transfers . Some OSG Connect storage spaces are truly \u2018open\u2019 with data available to be downloaded publicly. Of note: Users should keep copies of essential data and software in non-OSG locations, as OSG staff reserve the right to remove data at any time in order to ensure and/or restore system availability, and without prior notice to users. Proprietary data, HIPAA, and data with any other privacy concerns should not be stored on any OSG Connect filesystems or computed in OSG. Similarly, users should follow all licensing requirements when storing and executing software via OSG Connect submit servers. (See \u201cSoftware in OSG Connect\u201d.) Users should keep their /home directory privileges restricted to their user or group, and should not add \u2018global\u2019 permissions, which will allow other users to potentially make your data public. User-created \u2018open\u2019 network ports are disallowed , unless explicitly permitted following an accepted justification to support@opensciencegrid.org. (If you\u2019re not sure whether something you want to do will open a port, just get in touch!) The following actions may be taken automatedly or by OSG staff to stop or prevent jobs from causing problems. Please contact us if you\u2019d like help understanding why your jobs were held or removed, and so we can help you avoid problems in the future. Jobs using more memory or disk than requested may be automatically held (see Scaling Up after Test Jobs for tips on requesting the \u2018right\u2019 amount of job resources in your submit file). Jobs running longer than their JobDurationCategory allows for will be held (see Indicate the Job Duration Category of Your Jobs ). Jobs that have executed more than 30 times without completing may be automatically held (likely because they\u2019re too long for OSG). Jobs that have been held more than 14 days may be automatically removed. Jobs queued for more than three months may be automatically removed. Jobs otherwise causing known problems may be held or removed, without prior notification to the user. Held jobs may also be edited to prevent automated release/retry NOTE: in order to respect user email clients, job holds and removals do not come with specific notification to the user, unless configured by the user at the time of submission using HTCondor\u2019s \u2018notification\u2019 feature.","title":"Policies for Using OSG Connect and the OSPool"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/","text":"Generate SSH Keys and Activate Your OSG Login \u00b6 Overview \u00b6 OSG Connect requires SSH-key-based logins. You need to follow a two-step process to set up the SSH key to your account. Generate a SSH key pair. Add your public key to the submit host by uploading it to your OSG Connect user profile (via the OSG Connect website). After completing the process, you can log in from a local computer (your laptop or desktop) to the OSG Connect login node assigned using either ssh or an ssh program like Putty -- see below for more details on logging in. NOTE: Please do not edit the authorized keys file on the login node. Step 1: Generate SSH Keys \u00b6 We will discuss how to generate a SSH key pair for two cases: \"Unix\" systems (Linux, Mac) and certain, latest versions of Windows Older Windows systems Please note: The key pair consist of a private key and a public key. You will upload the public key to OSG Connect, but you also need to keep a copy of the private key to log in! You should keep the private key on machines that you have direct access to, i.e. your local computer (your laptop or desktop). Unix-based operating system (Linux/Mac) or latest Windows 10 versions \u00b6 We will create a key in the .ssh directory of your computer. Open a terminal on your local computer and run the following commands: mkdir ~/.ssh chmod 700 ~/.ssh ssh-keygen -t rsa For the newer OS versions the .ssh directory is already created and the first command is redundant. The last command will produce a prompt similar to Generating public/private rsa key pair. Enter file in which to save the key (/home/<local_user_name>/.ssh/id_rsa): Unless you want to change the location of the key, continue by pressing enter. Now you will be asked for a passphrase. Enter a passphrase that you will be able to remember and which is secure: Enter passphrase (empty for no passphrase): Enter same passphrase again: When everything has successfully completed, the output should resemble the following: Your identification has been saved in /home/<local_user_name>/.ssh/id_rsa. Your public key has been saved in /home/<local_user_name>/.ssh/id_rsa.pub. The key fingerprint is: ... The part you want to upload is the content of the .pub file (~/.ssh/id_rsa.pub) Windows, using Putty to log in \u00b6 If you can connect using the ssh command within the Command Prompt (Windows 10 build version 1803 and later), please follow the Mac/Linux directions above. If not, continue with the directions below. Open the PuTTYgen program. You can download PuttyGen here: PuttyGen Download Page , scroll down until you see the puttygen.exe file. For Type of key to generate, select RSA or SSH-2 RSA. Click the \"Generate\" button. Move your mouse in the area below the progress bar. When the progress bar is full, PuTTYgen generates your key pair. Type a passphrase in the \"Key passphrase\" field. Type the same passphrase in the \"Confirm passphrase\" field. You can use a key without a passphrase, but this is not recommended. Click the \"Save private key\" button to save the private key. You must save the private key. You will need it to connect to your machine. Right-click in the text field labeled \"Public key for pasting into OpenSSH authorized_keys file\" and choose Select All. Right-click again in the same text field and choose Copy. Step 2: Add the public SSH key to login node \u00b6 To add your public key to the OSG Connect log in node: Go to www.osgconnect.net and sign in with the institutional identity you used when requesting an OSG Connect account. Click \"Profile\" in the top right corner. Click the \"Edit Profile\" button located after the user information in the left hand box. Copy/paste the public key which is found in the .pub file into the \"SSH Public Key\" text box. The expected key is a single line, with three fields looking something like ssh-rsa ASSFFSAF... user@host . If you used the first set of key-generating instructions it is the content of ~/.ssh/id_rsa.pub and for the second (using PuTTYgen), it is the content from step 7 above. Click \"Update Profile\" The key is now added to your profile in the OSG Connect website. This will automatically be added to the login nodes within a couple hours. Can I Use Multiple Keys? \u00b6 Yes! If you want to log into OSG Connect from multiple computers, you can do so by generating a keypair on each computer you want to use, and then adding the public key to your OSG Connect profile. Logging In \u00b6 After following the steps above to upload your key and it's been a few hours, you should be able to log in to OSG Connect. Determine which login node to use \u00b6 Before you can connect, you will need to know which login node your account is assigned to. You can find this information on your profile from the OSG Connect website. Go to www.osgconnect.net and sign in with your CILogin. Click \"Profile\" in the top right corner. The assigned login nodes are listed in the left side box. Make note of the address of your assigned login node as you will use this to connect to OSG Connect. For Mac, Linux, or newer versions of Windows \u00b6 Open a terminal and type in: ssh <your_osg_connect_username>@<your_osg_login_node> It will ask for the passphrase for your ssh key (if you set one) and then you should be logged in. For older versions of Windows \u00b6 On older versions of Windows, you can use the Putty program to log in. Open the PutTTY program. If necessary, you can download PuTTY from the website here PuTTY download page . Type the address of your assigned login node as the hostname (see \"Determine which login node to use\" above). In the left hand menu, click the \"+\" next to \"SSH\" to expand the menu. Click \"Auth\" in the \"SSH\" menu. Click \"Browse\" and specify the private key file you saved in step 5 above. Return to \"Session\". a. Name your session b. Save session for future use Click \"Open\" to launch shell. Provide your ssh-key passphrase (created at Step 4 in PuTTYgen) when prompted to do so. Getting Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Generate SSH Keys and Activate Your OSG Login "},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#generate-ssh-keys-and-activate-your-osg-login","text":"","title":"Generate SSH Keys and Activate Your OSG Login"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#overview","text":"OSG Connect requires SSH-key-based logins. You need to follow a two-step process to set up the SSH key to your account. Generate a SSH key pair. Add your public key to the submit host by uploading it to your OSG Connect user profile (via the OSG Connect website). After completing the process, you can log in from a local computer (your laptop or desktop) to the OSG Connect login node assigned using either ssh or an ssh program like Putty -- see below for more details on logging in. NOTE: Please do not edit the authorized keys file on the login node.","title":"Overview"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#step-1-generate-ssh-keys","text":"We will discuss how to generate a SSH key pair for two cases: \"Unix\" systems (Linux, Mac) and certain, latest versions of Windows Older Windows systems Please note: The key pair consist of a private key and a public key. You will upload the public key to OSG Connect, but you also need to keep a copy of the private key to log in! You should keep the private key on machines that you have direct access to, i.e. your local computer (your laptop or desktop).","title":"Step 1: Generate SSH Keys"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#unix-based-operating-system-linuxmac-or-latest-windows-10-versions","text":"We will create a key in the .ssh directory of your computer. Open a terminal on your local computer and run the following commands: mkdir ~/.ssh chmod 700 ~/.ssh ssh-keygen -t rsa For the newer OS versions the .ssh directory is already created and the first command is redundant. The last command will produce a prompt similar to Generating public/private rsa key pair. Enter file in which to save the key (/home/<local_user_name>/.ssh/id_rsa): Unless you want to change the location of the key, continue by pressing enter. Now you will be asked for a passphrase. Enter a passphrase that you will be able to remember and which is secure: Enter passphrase (empty for no passphrase): Enter same passphrase again: When everything has successfully completed, the output should resemble the following: Your identification has been saved in /home/<local_user_name>/.ssh/id_rsa. Your public key has been saved in /home/<local_user_name>/.ssh/id_rsa.pub. The key fingerprint is: ... The part you want to upload is the content of the .pub file (~/.ssh/id_rsa.pub)","title":"Unix-based operating system (Linux/Mac) or latest Windows 10 versions"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#windows-using-putty-to-log-in","text":"If you can connect using the ssh command within the Command Prompt (Windows 10 build version 1803 and later), please follow the Mac/Linux directions above. If not, continue with the directions below. Open the PuTTYgen program. You can download PuttyGen here: PuttyGen Download Page , scroll down until you see the puttygen.exe file. For Type of key to generate, select RSA or SSH-2 RSA. Click the \"Generate\" button. Move your mouse in the area below the progress bar. When the progress bar is full, PuTTYgen generates your key pair. Type a passphrase in the \"Key passphrase\" field. Type the same passphrase in the \"Confirm passphrase\" field. You can use a key without a passphrase, but this is not recommended. Click the \"Save private key\" button to save the private key. You must save the private key. You will need it to connect to your machine. Right-click in the text field labeled \"Public key for pasting into OpenSSH authorized_keys file\" and choose Select All. Right-click again in the same text field and choose Copy.","title":"Windows, using Putty to log in"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#step-2-add-the-public-ssh-key-to-login-node","text":"To add your public key to the OSG Connect log in node: Go to www.osgconnect.net and sign in with the institutional identity you used when requesting an OSG Connect account. Click \"Profile\" in the top right corner. Click the \"Edit Profile\" button located after the user information in the left hand box. Copy/paste the public key which is found in the .pub file into the \"SSH Public Key\" text box. The expected key is a single line, with three fields looking something like ssh-rsa ASSFFSAF... user@host . If you used the first set of key-generating instructions it is the content of ~/.ssh/id_rsa.pub and for the second (using PuTTYgen), it is the content from step 7 above. Click \"Update Profile\" The key is now added to your profile in the OSG Connect website. This will automatically be added to the login nodes within a couple hours.","title":"Step 2: Add the public SSH key to login node"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#can-i-use-multiple-keys","text":"Yes! If you want to log into OSG Connect from multiple computers, you can do so by generating a keypair on each computer you want to use, and then adding the public key to your OSG Connect profile.","title":"Can I Use Multiple Keys?"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#logging-in","text":"After following the steps above to upload your key and it's been a few hours, you should be able to log in to OSG Connect.","title":"Logging In"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#determine-which-login-node-to-use","text":"Before you can connect, you will need to know which login node your account is assigned to. You can find this information on your profile from the OSG Connect website. Go to www.osgconnect.net and sign in with your CILogin. Click \"Profile\" in the top right corner. The assigned login nodes are listed in the left side box. Make note of the address of your assigned login node as you will use this to connect to OSG Connect.","title":"Determine which login node to use"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#for-mac-linux-or-newer-versions-of-windows","text":"Open a terminal and type in: ssh <your_osg_connect_username>@<your_osg_login_node> It will ask for the passphrase for your ssh key (if you set one) and then you should be logged in.","title":"For Mac, Linux, or newer versions of Windows"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#for-older-versions-of-windows","text":"On older versions of Windows, you can use the Putty program to log in. Open the PutTTY program. If necessary, you can download PuTTY from the website here PuTTY download page . Type the address of your assigned login node as the hostname (see \"Determine which login node to use\" above). In the left hand menu, click the \"+\" next to \"SSH\" to expand the menu. Click \"Auth\" in the \"SSH\" menu. Click \"Browse\" and specify the private key file you saved in step 5 above. Return to \"Session\". a. Name your session b. Save session for future use Click \"Open\" to launch shell. Provide your ssh-key passphrase (created at Step 4 in PuTTYgen) when prompted to do so.","title":"For older versions of Windows"},{"location":"overview/welcome_and_account_setup/generate-add-sshkey/#getting-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Getting Help"},{"location":"overview/welcome_and_account_setup/is-it-for-you/","text":"Computation on the Open Science Pool \u00b6 The OSG is a nationally-funded consortium of computing resources at more than one hundred institutional partners that, together, offer a strategic advantage for computing work that can be run as numerous short tasks that can execute independent of one another. For researchers who are not part of an organization with their own pool in the OSG, we offer the Open Science Pool (OSPool) , with dozens of campuses contributing excess computing capacity in support of open science. The OSPool is available for US-affiliated research projects and groups via the OSG Connect service, which the documentation on this site is specific to. Learn more about the services provided by the OSG that can support your HTC workload: For problems that can be run as numerous independent jobs (a high-throughput approach) and have requirements represented in the first two columns of the table below, the significant capacity of the OSPool can transform the types of questions that researchers are able to tackle. Importantly, many compute tasks that may appear to not be a good fit can be modified in simple ways to take advantage, and we'd love to discuss options with you! Ideal jobs! Still advantageous Maybe not, but get in touch! Expected Throughput: 1000s concurrent jobs 100s concurrent jobs let's discuss! Per-Job Requirements CPU cores 1 < 8 > 8 (or MPI) GPUs 0 1 > 1 Walltime < 10 hrs* < 20 hrs* > 20 hrs (not a good fit) RAM < few GB < 40 GB > 40 GB Input < 500 MB < 10 GB > 10 GB** Output < 1GB < 10 GB > 10 GB** Software pre-compiled binaries, containers Most other than ---> Licensed software, non-Linux * or checkpointable ** per job; you can work with a multi-TB dataset on the OSPool if it can be split into pieces! Some examples of work that have been a good fit for the OSPool and benefited from using its resources include: image analysis (including MRI, GIS, etc.) text-based analysis, including DNA read mapping and other bioinformatics hyper/parameter sweeps Monte Carlo methods and other model optimization Learn more and chat with a Research Computing Facilitator by signing up for OSG Connect Resources to Quickly Learn More \u00b6 Introduction to OSG the Distributed High Throughput Computing framework from the annual OSG User School : Full OSG User Documentation including our Roadmap to HTC Workload Submission OSG User Training materials . Live training sessions are advertised/open to those with active accounts on OSG Connect. Learn more and chat with a Research Computing Facilitator by signing up for OSG Connect","title":"Computation on the Open Science Pool "},{"location":"overview/welcome_and_account_setup/is-it-for-you/#computation-on-the-open-science-pool","text":"The OSG is a nationally-funded consortium of computing resources at more than one hundred institutional partners that, together, offer a strategic advantage for computing work that can be run as numerous short tasks that can execute independent of one another. For researchers who are not part of an organization with their own pool in the OSG, we offer the Open Science Pool (OSPool) , with dozens of campuses contributing excess computing capacity in support of open science. The OSPool is available for US-affiliated research projects and groups via the OSG Connect service, which the documentation on this site is specific to. Learn more about the services provided by the OSG that can support your HTC workload: For problems that can be run as numerous independent jobs (a high-throughput approach) and have requirements represented in the first two columns of the table below, the significant capacity of the OSPool can transform the types of questions that researchers are able to tackle. Importantly, many compute tasks that may appear to not be a good fit can be modified in simple ways to take advantage, and we'd love to discuss options with you! Ideal jobs! Still advantageous Maybe not, but get in touch! Expected Throughput: 1000s concurrent jobs 100s concurrent jobs let's discuss! Per-Job Requirements CPU cores 1 < 8 > 8 (or MPI) GPUs 0 1 > 1 Walltime < 10 hrs* < 20 hrs* > 20 hrs (not a good fit) RAM < few GB < 40 GB > 40 GB Input < 500 MB < 10 GB > 10 GB** Output < 1GB < 10 GB > 10 GB** Software pre-compiled binaries, containers Most other than ---> Licensed software, non-Linux * or checkpointable ** per job; you can work with a multi-TB dataset on the OSPool if it can be split into pieces! Some examples of work that have been a good fit for the OSPool and benefited from using its resources include: image analysis (including MRI, GIS, etc.) text-based analysis, including DNA read mapping and other bioinformatics hyper/parameter sweeps Monte Carlo methods and other model optimization Learn more and chat with a Research Computing Facilitator by signing up for OSG Connect","title":"Computation on the Open Science Pool"},{"location":"overview/welcome_and_account_setup/is-it-for-you/#resources-to-quickly-learn-more","text":"Introduction to OSG the Distributed High Throughput Computing framework from the annual OSG User School : Full OSG User Documentation including our Roadmap to HTC Workload Submission OSG User Training materials . Live training sessions are advertised/open to those with active accounts on OSG Connect. Learn more and chat with a Research Computing Facilitator by signing up for OSG Connect","title":"Resources to Quickly Learn More"},{"location":"overview/welcome_and_account_setup/registration-and-login/","text":"Registration and Login for OSG Connect \u00b6 Registration and Login for OSG Connect \u00b6 The major steps to getting started on OSG Connect are: apply for an OSG Connect account meet with an OSG Connect staff member for an short consultation and orientation. join and set your default \"project\" upload .ssh keys to the OSG Connect website Each of these is detailed in the guide below. Once you've gone through these steps, you should be able to login to the OSG Connect submit node. Account Creation \u00b6 Sign in to or create an account \u00b6 Start by creating an OSG Connect account. Visit the OSG Connect web site , then click on the Sign Up button. You will need to agree to our Acceptable Use Policy in order to get to the main log in screen. The main log in screen will prompt you to sign in via your primary institutional affiliation. You'll be directed to a discovery service which asks you what your home institution is. (If you've used CILogon before it may already know your home institution and skip this step.) Locate your institution in the list, or type its name to find matches. No Institutional Identity? If you don't have an institutional identity or can't find your institution on the provided list, either (separately) sign up for a Globus ID and follow the link for that option on this page, or contact the OSG Connect support team for guidance at support@opensciencegrid.org Note that this is the identity that will get linked to your OSG Connect account, so be sure to pick the institution (if you have multiple affiliations) that you would like to associate with your OSG Connect account. After selecting your institution in the discovery service, you'll be taken to your own institution's local sign-in screen. You've probably used it before, and if it looks familiar that's because it's exactly the same web site. Sign in using your campus credentials. When done, you'll return automatically to the OSG Connect portal and can carry on with signup. Returning to OSG Connect ? If you already have an OSG Connect account and are not being taken to your profile page after logging in with your institution's credentials, see our transition guide for how to proceed. After continuing, and allowing certain permissions in the next screen, you'll be asked to create a profile and save changes. If this works successfully, you should see that your membership to OSG is \"pending\" on the right hand side of the screen. Orientation Meeting \u00b6 Once you've applied to join OSG Connect as described above, an OSG Connect support team member will contact you to arrange an initial orientation meeting. This meeting generally takes about 20-30 minutes and is a chance to talk about your work, how it will fit on the OSG, and some practical next steps for getting started. Some of these next steps are also listed below. Join a Project \u00b6 As part of the sign up and meeting process, you'll be asked for information related to your research group so that you can be assigned to an accounting project. For more information about this process, see this guide: Start or Join a Project in OSG Connect Generate and Add an SSH key \u00b6 Once your account is created and you're approved, you can generate and upload an SSH key to the OSG Connect website; this key will be duplicated on the OSG Connect login node so that you're able to log in there and submit jobs. To see how to generate and add an SSH key, please visit this page: Step by step instructions to generate and adding an SSH key Log In \u00b6 Once you've gone through the steps above, you should be able to log in to the OSG Connect login node. See the second half of the SSH key guide for details: How to Log Into the OSG Connect Login Node Overview of access procedure and accounting \u00b6 For those interested in details, this section describes some of the background information behind projects. The OSG governs access to grid resources through an accounting framework that assigns each user's jobs to an accounting group or project . As a new user of the OSG, one of the first things to iron out is what project or projects best describe your work. This is more a matter of accountability than of entitlement: it concerns how organizations report to their sponsors and funding agencies on the utilization of resources placed under their administration. To assist in this, OSG Connect uses a group management tool that places users into one or more groups with names such as osg.RDCEP or osg.Extenci . The osg portion of this name differentiates our groups from those of other organizations in the same group management facility. The latter portion identifies the specific project, each with a Principal Investigator or other administrator, that oversees access to resources. Within our web tools, these names are often in mixed case, though you may see them uppercased in some reporting/accounting software. The first step in registration is to create a user account and bind it to other identity information. After that you will enroll in a project group. Once you've enrolled in a group, you'll have the requisite rights to log in to the submit node for the OSG Connect job scheduler, or to transfer data in and out of Stash. Submit node logins are typically via Secure Shell (SSH) using a password or a public key. We'll discuss how to connect further on.","title":"Registration and Login for OSG Connect "},{"location":"overview/welcome_and_account_setup/registration-and-login/#registration-and-login-for-osg-connect","text":"","title":"Registration and Login for OSG Connect"},{"location":"overview/welcome_and_account_setup/registration-and-login/#registration-and-login-for-osg-connect_1","text":"The major steps to getting started on OSG Connect are: apply for an OSG Connect account meet with an OSG Connect staff member for an short consultation and orientation. join and set your default \"project\" upload .ssh keys to the OSG Connect website Each of these is detailed in the guide below. Once you've gone through these steps, you should be able to login to the OSG Connect submit node.","title":"Registration and Login for OSG Connect"},{"location":"overview/welcome_and_account_setup/registration-and-login/#account-creation","text":"","title":"Account Creation"},{"location":"overview/welcome_and_account_setup/registration-and-login/#sign-in-to-or-create-an-account","text":"Start by creating an OSG Connect account. Visit the OSG Connect web site , then click on the Sign Up button. You will need to agree to our Acceptable Use Policy in order to get to the main log in screen. The main log in screen will prompt you to sign in via your primary institutional affiliation. You'll be directed to a discovery service which asks you what your home institution is. (If you've used CILogon before it may already know your home institution and skip this step.) Locate your institution in the list, or type its name to find matches. No Institutional Identity? If you don't have an institutional identity or can't find your institution on the provided list, either (separately) sign up for a Globus ID and follow the link for that option on this page, or contact the OSG Connect support team for guidance at support@opensciencegrid.org Note that this is the identity that will get linked to your OSG Connect account, so be sure to pick the institution (if you have multiple affiliations) that you would like to associate with your OSG Connect account. After selecting your institution in the discovery service, you'll be taken to your own institution's local sign-in screen. You've probably used it before, and if it looks familiar that's because it's exactly the same web site. Sign in using your campus credentials. When done, you'll return automatically to the OSG Connect portal and can carry on with signup. Returning to OSG Connect ? If you already have an OSG Connect account and are not being taken to your profile page after logging in with your institution's credentials, see our transition guide for how to proceed. After continuing, and allowing certain permissions in the next screen, you'll be asked to create a profile and save changes. If this works successfully, you should see that your membership to OSG is \"pending\" on the right hand side of the screen.","title":"Sign in to or create an account"},{"location":"overview/welcome_and_account_setup/registration-and-login/#orientation-meeting","text":"Once you've applied to join OSG Connect as described above, an OSG Connect support team member will contact you to arrange an initial orientation meeting. This meeting generally takes about 20-30 minutes and is a chance to talk about your work, how it will fit on the OSG, and some practical next steps for getting started. Some of these next steps are also listed below.","title":"Orientation Meeting"},{"location":"overview/welcome_and_account_setup/registration-and-login/#join-a-project","text":"As part of the sign up and meeting process, you'll be asked for information related to your research group so that you can be assigned to an accounting project. For more information about this process, see this guide: Start or Join a Project in OSG Connect","title":"Join a Project"},{"location":"overview/welcome_and_account_setup/registration-and-login/#generate-and-add-an-ssh-key","text":"Once your account is created and you're approved, you can generate and upload an SSH key to the OSG Connect website; this key will be duplicated on the OSG Connect login node so that you're able to log in there and submit jobs. To see how to generate and add an SSH key, please visit this page: Step by step instructions to generate and adding an SSH key","title":"Generate and Add an SSH key"},{"location":"overview/welcome_and_account_setup/registration-and-login/#log-in","text":"Once you've gone through the steps above, you should be able to log in to the OSG Connect login node. See the second half of the SSH key guide for details: How to Log Into the OSG Connect Login Node","title":"Log In"},{"location":"overview/welcome_and_account_setup/registration-and-login/#overview-of-access-procedure-and-accounting","text":"For those interested in details, this section describes some of the background information behind projects. The OSG governs access to grid resources through an accounting framework that assigns each user's jobs to an accounting group or project . As a new user of the OSG, one of the first things to iron out is what project or projects best describe your work. This is more a matter of accountability than of entitlement: it concerns how organizations report to their sponsors and funding agencies on the utilization of resources placed under their administration. To assist in this, OSG Connect uses a group management tool that places users into one or more groups with names such as osg.RDCEP or osg.Extenci . The osg portion of this name differentiates our groups from those of other organizations in the same group management facility. The latter portion identifies the specific project, each with a Principal Investigator or other administrator, that oversees access to resources. Within our web tools, these names are often in mixed case, though you may see them uppercased in some reporting/accounting software. The first step in registration is to create a user account and bind it to other identity information. After that you will enroll in a project group. Once you've enrolled in a group, you'll have the requisite rights to log in to the submit node for the OSG Connect job scheduler, or to transfer data in and out of Stash. Submit node logins are typically via Secure Shell (SSH) using a password or a public key. We'll discuss how to connect further on.","title":"Overview of access procedure and accounting"},{"location":"overview/welcome_and_account_setup/starting-project/","text":"Join and Use a Project in OSG Connect \u00b6 Background \u00b6 The OSG Connect team assigns individual user accounts to \"projects\". These projects are a way to track usage hours and capture information about the types of research using OSG Connect. A project typically corresponds to a research group headed by a single PI, but can sometimes represent a long-term multi-institutional project or some other grouping. You must be a member of a project before you can use OSG Connect to submit jobs. The next section of this guide describes the process for joining an OSG Connect project. Joining a Project \u00b6 Project Membership via Account Creation Process (Default) \u00b6 You will be added to a project when going through the typical OSG Connect account setup process. After applying for an OSG Connect account, you will receive an email to set up a consultation meeting and confirm which 'OSG Project' your usage should be associated with. You will be prompted to provide information based on the following two scenarios: If you are the first member of your research group / team to use the OSG through OSG Connect , a new project will be created for you. You will need to provide the following information to do so: Project Name PI Name PI Email PI Organization PI Department Field of Science: (out of https://osp.unm.edu/pi-resources/nsf-research-classifications.html) Project Description If you know that other members of your research group have used OSG Connect in the past, you can likely join a pre-existing group. Provide the name of your institution and PI to the OSG Connect team (if you haven't already) and we can confirm. Based on this information, OSG Connect support staff will either create a project and add you to it, or add you to an existing project when your account is approved. Join a Project \u00b6 If you need to join an existing project (you can be a member of more than one), please email the OSG team (support@opensciencegrid.org) with your name and the project you wish to join, with PI in CC to confirm. \"Set\" your OSG Connect project \u00b6 Job submission on OSG Connect requires a project be assigned to your account on the login node. This can be set after you have been added to a project as described above. Option 1 (preferred) : To set your default project, sign in to your login node and type $ connect project You should see a list of projects that you have joined. Most often there will only be one option! Make sure the right project is highlighted and press \"enter\" to save that choice. Option 2 : If need to run jobs under a different project you are a member of (not your default), you can manually set the project for those jobs by putting this option in the submit file: +ProjectName=\"ProjectName\" View Metrics For Your Project \u00b6 The project's resource usage appears in the OSG accounting system, GRACC . You can see the main OSG Connect dashboard here: Link to OSG Connect Dashboard At the top of that dashboard, there is a set of filters that you can use to examine the number of hours used by your project, specific users, or your institution.","title":"Join and Use a Project in OSG Connect "},{"location":"overview/welcome_and_account_setup/starting-project/#join-and-use-a-project-in-osg-connect","text":"","title":"Join and Use a Project in OSG Connect"},{"location":"overview/welcome_and_account_setup/starting-project/#background","text":"The OSG Connect team assigns individual user accounts to \"projects\". These projects are a way to track usage hours and capture information about the types of research using OSG Connect. A project typically corresponds to a research group headed by a single PI, but can sometimes represent a long-term multi-institutional project or some other grouping. You must be a member of a project before you can use OSG Connect to submit jobs. The next section of this guide describes the process for joining an OSG Connect project.","title":"Background"},{"location":"overview/welcome_and_account_setup/starting-project/#joining-a-project","text":"","title":"Joining a Project"},{"location":"overview/welcome_and_account_setup/starting-project/#project-membership-via-account-creation-process-default","text":"You will be added to a project when going through the typical OSG Connect account setup process. After applying for an OSG Connect account, you will receive an email to set up a consultation meeting and confirm which 'OSG Project' your usage should be associated with. You will be prompted to provide information based on the following two scenarios: If you are the first member of your research group / team to use the OSG through OSG Connect , a new project will be created for you. You will need to provide the following information to do so: Project Name PI Name PI Email PI Organization PI Department Field of Science: (out of https://osp.unm.edu/pi-resources/nsf-research-classifications.html) Project Description If you know that other members of your research group have used OSG Connect in the past, you can likely join a pre-existing group. Provide the name of your institution and PI to the OSG Connect team (if you haven't already) and we can confirm. Based on this information, OSG Connect support staff will either create a project and add you to it, or add you to an existing project when your account is approved.","title":"Project Membership via Account Creation Process (Default)"},{"location":"overview/welcome_and_account_setup/starting-project/#join-a-project","text":"If you need to join an existing project (you can be a member of more than one), please email the OSG team (support@opensciencegrid.org) with your name and the project you wish to join, with PI in CC to confirm.","title":"Join a Project"},{"location":"overview/welcome_and_account_setup/starting-project/#set-your-osg-connect-project","text":"Job submission on OSG Connect requires a project be assigned to your account on the login node. This can be set after you have been added to a project as described above. Option 1 (preferred) : To set your default project, sign in to your login node and type $ connect project You should see a list of projects that you have joined. Most often there will only be one option! Make sure the right project is highlighted and press \"enter\" to save that choice. Option 2 : If need to run jobs under a different project you are a member of (not your default), you can manually set the project for those jobs by putting this option in the submit file: +ProjectName=\"ProjectName\"","title":"\"Set\" your OSG Connect project"},{"location":"overview/welcome_and_account_setup/starting-project/#view-metrics-for-your-project","text":"The project's resource usage appears in the OSG accounting system, GRACC . You can see the main OSG Connect dashboard here: Link to OSG Connect Dashboard At the top of that dashboard, there is a set of filters that you can use to examine the number of hours used by your project, specific users, or your institution.","title":"View Metrics For Your Project"},{"location":"software_examples_for_osg/bioinformatics/tutorial-blast-split/","text":"Job components and plan Get materials and set up files Examine the submit file Examine the wrapper script Submit the jobs Bonus: a BLAST workflow This tutorial will put together several OSG tools and ideas - handling a larger data file, splitting a large file into smaller pieces, and transferring a portable software program. Job components and plan \u00b6 To run BLAST, we need three things: 1. the BLAST program (specifically the blastx binary) 2. a reference database (this is usually a larger file) 3. the file we want to query against the database The database and the input file will each get special treatment. The database we are using is large enough that we will want to use OSG Connect's stashcache capability (more information about that here ). The input file is large enough that a) it is near the upper limit of what is practical to transfer, b) it would take hours to complete a single blastx analysis for it, and c) the resulting output file would be huge. Because the BLAST process is run over the input file line by line, it is scientifically valid to split up the input query file, analyze the pieces, and then put the results back together at the end! By splitting the input query file into smaller pieces, each of the queries can be run as separate jobs. On the other hand, BLAST databases should not be split, because the blast output includes a score value for each sequence that is calculated relative to the entire length of the database. Get materials and set up files \u00b6 Run the tutorial command: tutorial blast-split Once the tutorial has downloaded, move into the folder and run the download_files.sh script to download the remaining files: cd tutorial-blast-split ./download_files.sh This command will have downloaded and unzipped the BLAST program ( ncbi-blast-2.9.0+ ), the file we want to query ( mouse_rna.fa ) and a set of tools that will split the file into smaller pieces ( gt-1.5.10-Linux_x86_64-64bit-complete ). Next, we will use the command gt from the genome tools package to split our input query file into 2 MB chunks as indicated by the -targetsize flag. To split the file, run this command: ./gt-1.5.10-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize 2 mouse_rna.fa Later, we'll need a list of the split files, so run this command to generate that list: ls mouse_rna.fa.* > list.txt Examine the submit file \u00b6 The submit file, blast.submit looks like this: executable = run_blast.sh arguments = $(inputfile) transfer_input_files = ncbi-blast-2.9.0+/bin/blastx, $(inputfile), stash:///osgconnect/public/osg/BlastTutorial/pdbaa.tar.gz output = logs/job_$(process).out error = logs/job_$(process).err log = logs/job_$(process).log requirements = OSGVO_OS_STRING == \"RHEL 7\" && Arch == \"X86_64\" request_memory = 2GB request_disk = 1GB request_cpus = 1 queue inputfile from list.txt The executable run_blast.sh is a script that runs blast and takes in a file to query as its argument. We'll look at this script in more detail in a minute. Our job will need to transfer the blastx executable and the input file being used for queries, shown in the transfer_input_files line. Because of the size of our database, we'll be using stash:/// to transfer the database to our job. Note on stash:/// : In this job, we're copying the file from a particular /public folder ( osg/BlastTutorialV1 ), but you have your own /public folder that you could use for the database. If you wanted to try this, you would want to navigate to your /public folder, download the pdbaa.tar.gz file, return to your /home folder, and change the path in the stash:/// command above. This might look like: cd /public/username wget http://stash.osgconnect.net/public/osg/BlastTutorialV1/pdbaa.tar.gz cd /home/username Finally, you may have already noticed that instead of listing the individual input file by name, we've used the following syntax: $(inputfile) . This is a variable that represents the name of an individual input file. We've done this so that we can set the variable as a different file name for each job. We can set the variable by using the queue syntax shown at the bottom of the file: queue inputfile from list.txt This command will pull file names from the list.txt file that we created earlier, and submit one job per file and set the \"inputfile\" variable to that file name. Examine the wrapper script \u00b6 The submit file had a script called run_blast.sh : 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # get input file from arguments inputfile = $1 # Prepare our database and unzip into new dir tar -xzvf pdbaa.tar.gz rm pdbaa.tar.gz # run blast query on input file ./blastx -db pdbaa/pdbaa -query $inputfile -out $inputfile .result It saves the name of the input file, unpacks our database, and then runs the BLAST query from the input file we transferred and used as the argument. Submit the jobs \u00b6 Our jobs should be set and ready to go. To submit them, run this command: condor_submit blast.submit And you should see that 51 jobs have been submitted: Submitting job(s)................................................ 51 job(s) submitted to cluster 90363. You can check on your jobs' progress using condor_q Bonus: a BLAST workflow \u00b6 We had to go through multiple steps to run the jobs above. There was an initial step to split the files and generate a list of them; then we submitted the jobs. These two steps can be tied together in a workflow using the HTCondor DAGMan workflow tool. First, we would create a script ( split_files.sh ) that does the file splitting steps: 1 2 3 4 5 #!/bin/bash filesize = $1 ./gt-1.5.10-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize $filesize mouse_rna.fa ls mouse_rna.fa.* > list.txt This script will need executable permissions: chmod +x split_files.sh Then, we create a DAG workflow file that ties the two steps together: ## DAG: blastrun.dag JOB blast blast.submit SCRIPT PRE blast split_files.sh 2 To submit this DAG, we use this command: condor_submit_dag blastrun.dag","title":"High-Throughput BLAST"},{"location":"software_examples_for_osg/bioinformatics/tutorial-blast-split/#job-components-and-plan","text":"To run BLAST, we need three things: 1. the BLAST program (specifically the blastx binary) 2. a reference database (this is usually a larger file) 3. the file we want to query against the database The database and the input file will each get special treatment. The database we are using is large enough that we will want to use OSG Connect's stashcache capability (more information about that here ). The input file is large enough that a) it is near the upper limit of what is practical to transfer, b) it would take hours to complete a single blastx analysis for it, and c) the resulting output file would be huge. Because the BLAST process is run over the input file line by line, it is scientifically valid to split up the input query file, analyze the pieces, and then put the results back together at the end! By splitting the input query file into smaller pieces, each of the queries can be run as separate jobs. On the other hand, BLAST databases should not be split, because the blast output includes a score value for each sequence that is calculated relative to the entire length of the database.","title":"Job components and plan"},{"location":"software_examples_for_osg/bioinformatics/tutorial-blast-split/#get-materials-and-set-up-files","text":"Run the tutorial command: tutorial blast-split Once the tutorial has downloaded, move into the folder and run the download_files.sh script to download the remaining files: cd tutorial-blast-split ./download_files.sh This command will have downloaded and unzipped the BLAST program ( ncbi-blast-2.9.0+ ), the file we want to query ( mouse_rna.fa ) and a set of tools that will split the file into smaller pieces ( gt-1.5.10-Linux_x86_64-64bit-complete ). Next, we will use the command gt from the genome tools package to split our input query file into 2 MB chunks as indicated by the -targetsize flag. To split the file, run this command: ./gt-1.5.10-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize 2 mouse_rna.fa Later, we'll need a list of the split files, so run this command to generate that list: ls mouse_rna.fa.* > list.txt","title":"Get materials and set up files"},{"location":"software_examples_for_osg/bioinformatics/tutorial-blast-split/#examine-the-submit-file","text":"The submit file, blast.submit looks like this: executable = run_blast.sh arguments = $(inputfile) transfer_input_files = ncbi-blast-2.9.0+/bin/blastx, $(inputfile), stash:///osgconnect/public/osg/BlastTutorial/pdbaa.tar.gz output = logs/job_$(process).out error = logs/job_$(process).err log = logs/job_$(process).log requirements = OSGVO_OS_STRING == \"RHEL 7\" && Arch == \"X86_64\" request_memory = 2GB request_disk = 1GB request_cpus = 1 queue inputfile from list.txt The executable run_blast.sh is a script that runs blast and takes in a file to query as its argument. We'll look at this script in more detail in a minute. Our job will need to transfer the blastx executable and the input file being used for queries, shown in the transfer_input_files line. Because of the size of our database, we'll be using stash:/// to transfer the database to our job. Note on stash:/// : In this job, we're copying the file from a particular /public folder ( osg/BlastTutorialV1 ), but you have your own /public folder that you could use for the database. If you wanted to try this, you would want to navigate to your /public folder, download the pdbaa.tar.gz file, return to your /home folder, and change the path in the stash:/// command above. This might look like: cd /public/username wget http://stash.osgconnect.net/public/osg/BlastTutorialV1/pdbaa.tar.gz cd /home/username Finally, you may have already noticed that instead of listing the individual input file by name, we've used the following syntax: $(inputfile) . This is a variable that represents the name of an individual input file. We've done this so that we can set the variable as a different file name for each job. We can set the variable by using the queue syntax shown at the bottom of the file: queue inputfile from list.txt This command will pull file names from the list.txt file that we created earlier, and submit one job per file and set the \"inputfile\" variable to that file name.","title":"Examine the submit file"},{"location":"software_examples_for_osg/bioinformatics/tutorial-blast-split/#examine-the-wrapper-script","text":"The submit file had a script called run_blast.sh : 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # get input file from arguments inputfile = $1 # Prepare our database and unzip into new dir tar -xzvf pdbaa.tar.gz rm pdbaa.tar.gz # run blast query on input file ./blastx -db pdbaa/pdbaa -query $inputfile -out $inputfile .result It saves the name of the input file, unpacks our database, and then runs the BLAST query from the input file we transferred and used as the argument.","title":"Examine the wrapper script"},{"location":"software_examples_for_osg/bioinformatics/tutorial-blast-split/#submit-the-jobs","text":"Our jobs should be set and ready to go. To submit them, run this command: condor_submit blast.submit And you should see that 51 jobs have been submitted: Submitting job(s)................................................ 51 job(s) submitted to cluster 90363. You can check on your jobs' progress using condor_q","title":"Submit the jobs"},{"location":"software_examples_for_osg/bioinformatics/tutorial-blast-split/#bonus-a-blast-workflow","text":"We had to go through multiple steps to run the jobs above. There was an initial step to split the files and generate a list of them; then we submitted the jobs. These two steps can be tied together in a workflow using the HTCondor DAGMan workflow tool. First, we would create a script ( split_files.sh ) that does the file splitting steps: 1 2 3 4 5 #!/bin/bash filesize = $1 ./gt-1.5.10-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize $filesize mouse_rna.fa ls mouse_rna.fa.* > list.txt This script will need executable permissions: chmod +x split_files.sh Then, we create a DAG workflow file that ties the two steps together: ## DAG: blastrun.dag JOB blast blast.submit SCRIPT PRE blast split_files.sh 2 To submit this DAG, we use this command: condor_submit_dag blastrun.dag","title":"Bonus: a BLAST workflow"},{"location":"software_examples_for_osg/bioinformatics/tutorial-bwa/","text":"Introduction Get Tutorial Files Install and Prepare BWA Download Data to Analyze Run a Single Test Job Scaling Up to Analyze Multiple Samples Introduction \u00b6 This tutorial focuses on a subset of the Data Carpentry Genomics workshop curriculum - specifically, this page cover's how to run a BWA workflow on OSG resources. It will use the same general flow as the BWA segment of the Data Carpentry workshop with minor adjustments. The goal of this tutorial is to learn how to convert an existing BWA workflow to run on the OSPool. Get Tutorial Files \u00b6 Logged into the submit node, we will run the tutorial command, that will create a folder for our analysis, as well as some sample files. tutorial bwa Install and Prepare BWA \u00b6 First, we need to install BWA, also called Burrows-Wheeler Aligner. To do this, we will create and navigate to a new folder in our /home directory called software . We will then follow the developer's instructions (https://github.com/lh3/bwa) for using git clone to clone the software and then build the tool using make . cd ~/tutorial-bwa cd software git clone https://github.com/lh3/bwa.git cd bwa make Next, BWA needs to be added to our PATH variables, to test if the installation worked: export PATH=$PATH:/home/$USER/tutorial-bwa/software/bwa/ To check that BWA has been installed correctly, type bwa . You should receive output similar to the following: Program: bwa (alignment via Burrows-Wheeler transformation) Version: 0.7.17-r1198-dirty Contact: Heng Li <hli@ds.dfci.harvard.edu> Usage: bwa <command> [options] Command: index index sequences in the FASTA format mem BWA-MEM algorithm fastmap identify super-maximal exact matches ... Now that we have successfully installed bwa , we will create a portable compressed tarball of this software so that it is smaller and quicker to transport when we submit our jobs to the OSPool. cd ~/tutorial-bwa/software tar -czvf bwa.tar.gz bwa Checking the size of this compressed tarball using ls -lh bwa.tar.gz reveals the file is approximately 4MB. The tarball should stay in /home. Download Data to Analyze \u00b6 Now that we have installed BWA, we need to download data to analyze. For this tutorial, we will be downloading data used in the Data Carpentry workshop. This data includes both the genome of Escherichia coli (E. coli) and paired-end RNA sequencing reads obtained from a study carried out by Blount et al. published in PNAS . Additional information about how the data was modified in preparation for this analysis can be found on the Data Carpentry's workshop website . cd ~/tutorial-bwa ./download_data.sh Investigating the size of the downloaded genome by typing: ls -lh data/ref_genome/ reveals the file is 1.4 MB. Therefore, this file should remain in /home and does not need to be moved to /public. We should also check the trimmed fastq paired-end read files: ls -lh data/trimmed_fastq_small Once everything is downloaded, make sure you're still in the tutorial-bwa directory. cd ~/tutorial-bwa Run a Single Test Job \u00b6 Now that we have all items in our analysis ready, it is time to submit a single test job to map our RNA reads to the E. coli genome. For a single test job, we will choose a single sample to analyze. In the following example, we will align both the forward and reverse reads of SRR2584863 to the E. coli genome. Using a text editor such as nano or vim , we can create an example submit file for this test job called bwa-test.sub containing the following information: universe = vanilla executable = bwa-test.sh # arguments = # need to transfer bwa.tar.gz file, the reference # genome, and the trimmed fastq files transfer_input_files = software/bwa.tar.gz, data/ref_genome/ecoli_rel606.fasta.gz, data/trimmed_fastq_small/SRR2584863_1.trim.sub.fastq, data/trimmed_fastq_small/SRR2584863_2.trim.sub.fastq log = TestJobOutput/bwa_test_job.log output = TestJobOutput/bwa_test_job.out error = TestJobOutput/bwa_test_job.error +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 2GB request_disk = 1GB should_transfer_files = YES when_to_transfer_output = ON_EXIT requirements = (OSGVO_OS_STRING == \"RHEL 7\") queue 1 You will notice that the .log, .out, and .error files will be saved to a folder called TestJobOutput . We need to create this folder using mkdir TestJobOutput before we submit our job. We will call the script for this analysis bwa-test.sh and it should contain the following information: #!/bin/bash # Script name: bwa-test.sh echo \"Unpacking software\" tar -xzf bwa.tar.gz echo \"Setting PATH for bwa\" export PATH=$_CONDOR_SCRATCH_DIR/bwa/:$PATH echo \"Indexing E. coli genome\" bwa index ecoli_rel606.fasta.gz echo \"Starting bwa alignment for SRR2584863\" bwa mem ecoli_rel606.fasta.gz SRR2584863_1.trim.sub.fastq SRR2584863_2.trim.sub.fastq > SRR2584863.aligned.sam echo \"Done with bwa alignment for SRR2584863!\" echo \"Cleaning up files generated from genome indexing\" rm ecoli_rel606.fasta.gz.amb rm ecoli_rel606.fasta.gz.ann rm ecoli_rel606.fasta.gz.bwt rm ecoli_rel606.fasta.gz.pac rm ecoli_rel606.fasta.gz.sa We can submit this single test job to HTCondor by typing: condor_submit bwa-test.sub To check the status of the job, we can use condor_q . Upon the completion of the test job, we should investigate the output to ensure that it is what we expected and also review the .log file to help optimize future resource requests in preparation for scaling up. For example, when we investigate the bwa_test_job.log file created in this analysis, at the bottom of the file we see a resource table: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 253770 1048576 27945123 Memory (MB) : 144 2048 2500 Here we see that we used less than half of both the disk space and memory we requested. In future jobs, we should request a smaller amount of each resource, such as 0.5 GB of disk space and 0.5 GB of memory. Prior to scaling up our analysis, we should run additional test jobs using these resource requests to ensure that they are sufficient to allow our job to complete successfully. Scaling Up to Analyze Multiple Samples \u00b6 In preparation for scaling up, please review our guide on how to scale up after a successful test job and how to easily submit multiple jobs with a single submit file . After reviewing how to submit multiple jobs with a single submit file, it is possible to determine that the most appropriate way to submit multiple jobs for this analysis is to use queue <var> from <list.txt> . To use this option, we first need to create a file with just the sample names/IDs that we want to analyze. To do this, we want to cut all information after the \"_\" symbol to remove the forward/reverse read information and file extensions. For example, we want SRR2584863_1.trim.sub.fastq to become just SRR2584863. We will save the sample names in a file called samples.txt : cd ~/tutorial-bwa cd data/trimmed_fastq_small/ ls *.fastq | cut -f 1 -d '_' | uniq > samples.txt cd ~/tutorial-bwa Now, we can create a new submit file called bwa-alignment.sub to queue a new job for each sample. To make it simpler to start, you can copy the bwa-test.sub file ( cp bwa-test.sub bwa-alignment.sub ) and modify it. universe = vanilla executable = bwa-alignment.sh arguments = $(sample) transfer_input_files = software/bwa.tar.gz, data/ref_genome/ecoli_rel606.fasta.gz, data/trimmed_fastq_small/$(sample)_1.trim.sub.fastq, data/trimmed_fastq_small/$(sample)_2.trim.sub.fastq transfer_output_remaps = \"$(sample).aligned.sam=results/$(sample).aligned.sam\" log = log/bwa_$(sample)_job.log output = output/bwa_$(sample)_job.out error = error/bwa_$(sample)_job.error +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 0.5GB request_disk = 0.5GB should_transfer_files = YES when_to_transfer_output = ON_EXIT requirements = (OSGVO_OS_STRING == \"RHEL 7\") queue sample from data/trimmed_fastq_small/samples.txt In addition to restructuring our submit file to queue a new job for each sample, it is also advantageous to have our standard output, log, and error files saved to dedicated folders called \"log\", \"output\", and \"error\" to help keep our output files organized. Therefore, we need to make these folders in our /home directory prior to submitting our job. We will also create an additional folder to store our aligned sequencing files in a folder called results : mkdir log mkdir output mkdir error mkdir results To store the aligned sequencing files in the results folder, we can add the transfer_output_remaps feature to our submit file. This feature allows us to specify a name and a path to save our output files in the format of \"file1 = path/to/save/file2\", where file1 is the origional name of the document and file2 is the name that we want to save the file using. In the example above, we do not change the name of the resulting output files. This feature also helps us keep an organized working space, rather than having all of our resulting sequencing files be saved to our /home directory. Once our submit file has been updated, we can update our script to look like and call it something like bwa-alignment.sh : #!/bin/bash # Script name: bwa-alignment.sh echo \"Unpackage software\" tar -xzf bwa.tar.gz echo \"Set PATH for bwa\" export PATH=$_CONDOR_SCRATCH_DIR/bwa/:$PATH # Renaming first argument sample=$1 echo \"Index E.coli genome\" bwa index ecoli_rel606.fasta.gz echo \"Starting bwa alignment for ${sample}\" bwa mem ecoli_rel606.fasta.gz ${sample}_1.trim.sub.fastq ${sample}_2.trim.sub.fastq > ${sample}.aligned.sam echo \"Done with bwa alignment for ${sample}!\" echo \"Cleaning up workspace\" rm ecoli_rel606.fasta.gz.amb rm ecoli_rel606.fasta.gz.ann rm ecoli_rel606.fasta.gz.bwt rm ecoli_rel606.fasta.gz.pac rm ecoli_rel606.fasta.gz.sa Once ready, we can submit our job to HTCondor by using condor_submit bwa-alignment.sub . When we type condor_q , we see that three jobs have entered the queue (one for each of our three experimental samples). When our jobs are completed, we can confirm that our alignment output results files were created by typing: ls -lh results/* We can also investigate our log, error, and output files in their respective folders to ensure we obtained the resulting output of these files that we expected. For more information about running bioinformatics workflows on the OSG, we recommend our BLAST tutorial as well as our Samtools instillation guide.","title":"High-Throughput BWA Read Mapping"},{"location":"software_examples_for_osg/bioinformatics/tutorial-bwa/#introduction","text":"This tutorial focuses on a subset of the Data Carpentry Genomics workshop curriculum - specifically, this page cover's how to run a BWA workflow on OSG resources. It will use the same general flow as the BWA segment of the Data Carpentry workshop with minor adjustments. The goal of this tutorial is to learn how to convert an existing BWA workflow to run on the OSPool.","title":"Introduction"},{"location":"software_examples_for_osg/bioinformatics/tutorial-bwa/#get-tutorial-files","text":"Logged into the submit node, we will run the tutorial command, that will create a folder for our analysis, as well as some sample files. tutorial bwa","title":"Get Tutorial Files"},{"location":"software_examples_for_osg/bioinformatics/tutorial-bwa/#install-and-prepare-bwa","text":"First, we need to install BWA, also called Burrows-Wheeler Aligner. To do this, we will create and navigate to a new folder in our /home directory called software . We will then follow the developer's instructions (https://github.com/lh3/bwa) for using git clone to clone the software and then build the tool using make . cd ~/tutorial-bwa cd software git clone https://github.com/lh3/bwa.git cd bwa make Next, BWA needs to be added to our PATH variables, to test if the installation worked: export PATH=$PATH:/home/$USER/tutorial-bwa/software/bwa/ To check that BWA has been installed correctly, type bwa . You should receive output similar to the following: Program: bwa (alignment via Burrows-Wheeler transformation) Version: 0.7.17-r1198-dirty Contact: Heng Li <hli@ds.dfci.harvard.edu> Usage: bwa <command> [options] Command: index index sequences in the FASTA format mem BWA-MEM algorithm fastmap identify super-maximal exact matches ... Now that we have successfully installed bwa , we will create a portable compressed tarball of this software so that it is smaller and quicker to transport when we submit our jobs to the OSPool. cd ~/tutorial-bwa/software tar -czvf bwa.tar.gz bwa Checking the size of this compressed tarball using ls -lh bwa.tar.gz reveals the file is approximately 4MB. The tarball should stay in /home.","title":"Install and Prepare BWA"},{"location":"software_examples_for_osg/bioinformatics/tutorial-bwa/#download-data-to-analyze","text":"Now that we have installed BWA, we need to download data to analyze. For this tutorial, we will be downloading data used in the Data Carpentry workshop. This data includes both the genome of Escherichia coli (E. coli) and paired-end RNA sequencing reads obtained from a study carried out by Blount et al. published in PNAS . Additional information about how the data was modified in preparation for this analysis can be found on the Data Carpentry's workshop website . cd ~/tutorial-bwa ./download_data.sh Investigating the size of the downloaded genome by typing: ls -lh data/ref_genome/ reveals the file is 1.4 MB. Therefore, this file should remain in /home and does not need to be moved to /public. We should also check the trimmed fastq paired-end read files: ls -lh data/trimmed_fastq_small Once everything is downloaded, make sure you're still in the tutorial-bwa directory. cd ~/tutorial-bwa","title":"Download Data to Analyze"},{"location":"software_examples_for_osg/bioinformatics/tutorial-bwa/#run-a-single-test-job","text":"Now that we have all items in our analysis ready, it is time to submit a single test job to map our RNA reads to the E. coli genome. For a single test job, we will choose a single sample to analyze. In the following example, we will align both the forward and reverse reads of SRR2584863 to the E. coli genome. Using a text editor such as nano or vim , we can create an example submit file for this test job called bwa-test.sub containing the following information: universe = vanilla executable = bwa-test.sh # arguments = # need to transfer bwa.tar.gz file, the reference # genome, and the trimmed fastq files transfer_input_files = software/bwa.tar.gz, data/ref_genome/ecoli_rel606.fasta.gz, data/trimmed_fastq_small/SRR2584863_1.trim.sub.fastq, data/trimmed_fastq_small/SRR2584863_2.trim.sub.fastq log = TestJobOutput/bwa_test_job.log output = TestJobOutput/bwa_test_job.out error = TestJobOutput/bwa_test_job.error +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 2GB request_disk = 1GB should_transfer_files = YES when_to_transfer_output = ON_EXIT requirements = (OSGVO_OS_STRING == \"RHEL 7\") queue 1 You will notice that the .log, .out, and .error files will be saved to a folder called TestJobOutput . We need to create this folder using mkdir TestJobOutput before we submit our job. We will call the script for this analysis bwa-test.sh and it should contain the following information: #!/bin/bash # Script name: bwa-test.sh echo \"Unpacking software\" tar -xzf bwa.tar.gz echo \"Setting PATH for bwa\" export PATH=$_CONDOR_SCRATCH_DIR/bwa/:$PATH echo \"Indexing E. coli genome\" bwa index ecoli_rel606.fasta.gz echo \"Starting bwa alignment for SRR2584863\" bwa mem ecoli_rel606.fasta.gz SRR2584863_1.trim.sub.fastq SRR2584863_2.trim.sub.fastq > SRR2584863.aligned.sam echo \"Done with bwa alignment for SRR2584863!\" echo \"Cleaning up files generated from genome indexing\" rm ecoli_rel606.fasta.gz.amb rm ecoli_rel606.fasta.gz.ann rm ecoli_rel606.fasta.gz.bwt rm ecoli_rel606.fasta.gz.pac rm ecoli_rel606.fasta.gz.sa We can submit this single test job to HTCondor by typing: condor_submit bwa-test.sub To check the status of the job, we can use condor_q . Upon the completion of the test job, we should investigate the output to ensure that it is what we expected and also review the .log file to help optimize future resource requests in preparation for scaling up. For example, when we investigate the bwa_test_job.log file created in this analysis, at the bottom of the file we see a resource table: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 253770 1048576 27945123 Memory (MB) : 144 2048 2500 Here we see that we used less than half of both the disk space and memory we requested. In future jobs, we should request a smaller amount of each resource, such as 0.5 GB of disk space and 0.5 GB of memory. Prior to scaling up our analysis, we should run additional test jobs using these resource requests to ensure that they are sufficient to allow our job to complete successfully.","title":"Run a Single Test Job"},{"location":"software_examples_for_osg/bioinformatics/tutorial-bwa/#scaling-up-to-analyze-multiple-samples","text":"In preparation for scaling up, please review our guide on how to scale up after a successful test job and how to easily submit multiple jobs with a single submit file . After reviewing how to submit multiple jobs with a single submit file, it is possible to determine that the most appropriate way to submit multiple jobs for this analysis is to use queue <var> from <list.txt> . To use this option, we first need to create a file with just the sample names/IDs that we want to analyze. To do this, we want to cut all information after the \"_\" symbol to remove the forward/reverse read information and file extensions. For example, we want SRR2584863_1.trim.sub.fastq to become just SRR2584863. We will save the sample names in a file called samples.txt : cd ~/tutorial-bwa cd data/trimmed_fastq_small/ ls *.fastq | cut -f 1 -d '_' | uniq > samples.txt cd ~/tutorial-bwa Now, we can create a new submit file called bwa-alignment.sub to queue a new job for each sample. To make it simpler to start, you can copy the bwa-test.sub file ( cp bwa-test.sub bwa-alignment.sub ) and modify it. universe = vanilla executable = bwa-alignment.sh arguments = $(sample) transfer_input_files = software/bwa.tar.gz, data/ref_genome/ecoli_rel606.fasta.gz, data/trimmed_fastq_small/$(sample)_1.trim.sub.fastq, data/trimmed_fastq_small/$(sample)_2.trim.sub.fastq transfer_output_remaps = \"$(sample).aligned.sam=results/$(sample).aligned.sam\" log = log/bwa_$(sample)_job.log output = output/bwa_$(sample)_job.out error = error/bwa_$(sample)_job.error +JobDurationCategory = \"Medium\" request_cpus = 1 request_memory = 0.5GB request_disk = 0.5GB should_transfer_files = YES when_to_transfer_output = ON_EXIT requirements = (OSGVO_OS_STRING == \"RHEL 7\") queue sample from data/trimmed_fastq_small/samples.txt In addition to restructuring our submit file to queue a new job for each sample, it is also advantageous to have our standard output, log, and error files saved to dedicated folders called \"log\", \"output\", and \"error\" to help keep our output files organized. Therefore, we need to make these folders in our /home directory prior to submitting our job. We will also create an additional folder to store our aligned sequencing files in a folder called results : mkdir log mkdir output mkdir error mkdir results To store the aligned sequencing files in the results folder, we can add the transfer_output_remaps feature to our submit file. This feature allows us to specify a name and a path to save our output files in the format of \"file1 = path/to/save/file2\", where file1 is the origional name of the document and file2 is the name that we want to save the file using. In the example above, we do not change the name of the resulting output files. This feature also helps us keep an organized working space, rather than having all of our resulting sequencing files be saved to our /home directory. Once our submit file has been updated, we can update our script to look like and call it something like bwa-alignment.sh : #!/bin/bash # Script name: bwa-alignment.sh echo \"Unpackage software\" tar -xzf bwa.tar.gz echo \"Set PATH for bwa\" export PATH=$_CONDOR_SCRATCH_DIR/bwa/:$PATH # Renaming first argument sample=$1 echo \"Index E.coli genome\" bwa index ecoli_rel606.fasta.gz echo \"Starting bwa alignment for ${sample}\" bwa mem ecoli_rel606.fasta.gz ${sample}_1.trim.sub.fastq ${sample}_2.trim.sub.fastq > ${sample}.aligned.sam echo \"Done with bwa alignment for ${sample}!\" echo \"Cleaning up workspace\" rm ecoli_rel606.fasta.gz.amb rm ecoli_rel606.fasta.gz.ann rm ecoli_rel606.fasta.gz.bwt rm ecoli_rel606.fasta.gz.pac rm ecoli_rel606.fasta.gz.sa Once ready, we can submit our job to HTCondor by using condor_submit bwa-alignment.sub . When we type condor_q , we see that three jobs have entered the queue (one for each of our three experimental samples). When our jobs are completed, we can confirm that our alignment output results files were created by typing: ls -lh results/* We can also investigate our log, error, and output files in their respective folders to ensure we obtained the resulting output of these files that we expected. For more information about running bioinformatics workflows on the OSG, we recommend our BLAST tutorial as well as our Samtools instillation guide.","title":"Scaling Up to Analyze Multiple Samples"},{"location":"software_examples_for_osg/drug_discovery/tutorial-AutoDockVina/","text":"Overview Tutorial Files Files Need to Submit the Job Running the simulation Next Steps Getting Help Overview \u00b6 AutoDock Vina is a molecular docking program useful for computer aided drug design. In this tutorial, we will learn how to run AutoDock Vina on OSG. Tutorial Files \u00b6 It is easiest to start with the tutorial command. Type: $ tutorial AutoDockVina This will create a directory tutorial-AutodockVina . Change into the directory and look at the available files: $ cd tutorial-AutoDockVina $ ls You should see the following: receptor_config.txt # Configuration file (input) data/ receptor.pdbqt # Receptor coordinates and atomic charges (input) ligand.pdbqt # Ligand coordinates and atomic charges (input) vina_job.submit # Job submission file vina_wrapper.bash # Execution script We need to download the AutoDock program separately into the this directory as well. Go to the AutoDock Vina website and click on the Download link at the top of the page. Download the Linux version of the program; you can do this directly to the current directory by using the wget command and the download link. If you use the -O option shown below, it will rename the tar file to match what is used in the rest of the guide. $ wget http://vina.scripps.edu/download/autodock_vina_1_1_2_linux_x86.tgz -O autodock_vina.tar.gz Files Need to Submit the Job \u00b6 The file vina_job.submit is the job submission file and contains the description of the job in HTCondor language. Specifically, it includes an \"executable\" (the script HTCondor will use in the job to run vina), a list of the files needed to run the job (shown in \"transfer_input_files\"), and indications of where to write logging information and what resources and requirements the job needs. Change needed: If your download software tar.gz file has a different name, change the name in the transfer_input_files line below. executable = vina_wrapper.bash transfer_input_files = data/, receptor_config.txt, autodock_vina.tar.gz should_transfer_files = Yes when_to_transfer_output = ON_EXIT output = job.$(Cluster).$(Process).out error = job.$(Cluster).$(Process).error log = job.$(Cluster).$(Process).log requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 512MB request_disk = 512MB queue 1 Next we see the execution wrapper vina_wrapper.bash . The execution wrapper and its inside content are executed on a worker node out in the Open Science Pool. The first two commands will unzip the file containing the AutoDock Vina program and make it accessible on the command line; we can then run a typical vina command. Change needed: If your download tar.gz file has a different name, change it in the script below, in the tar command. You will also need to change the name of the AUTODOCK_FOLDER to match whatever folder appears when you un-tar the downloaded tar.gz file. 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash # set TMPDIR variable export TMPDIR = $_CONDOR_SCRATCH_DIR # Unzip autodock vina software tar -xzf autodock_vina.tar.gz export PATH = $PWD /AUTODOCK_FOLDER/bin: $PATH # Run vina vina --config receptor_config.txt \\ --ligand ligand.pdbqt --out receptor-ligand.pdbqt \\ --log receptor-ligand.log Running the simulation \u00b6 We submit the job using condor_submit command as follows $ condor_submit vina_job.submit Now you have submitted the AutoDock Vina job on the OSG. The present job should be finished quickly (less than 10 mins). You can check the status of the submitted job by using condor_q command as follows: $ condor_q username After job completion, you will see the output files receptor-ligand_output.pdbqt and receptor-ligand.log in your work directory. Next Steps \u00b6 After running this example, you may want to scale up to testing multiple molecules or ligands. Some good starting points for this include: Decide how many docking runs you want to try per job. If one molecule can be tested in a few seconds, you can probably run a few hundred in a job that runs in about an hour. How should you divide up the input data in this case? Do you need individual input files for each molecule, or can you use one to share? Should the molecule files all get copied to every job or just the jobs where they're needed? You can separate groups of files by putting them in separate directories or tar.gz files to help with this. Look at this guide to see different ways that you can use HTCondor to submit multiple jobs at once. If you want to use a different (or additional) docking programs, you can include them in the same job by downloading and including those software files in your job submission. Getting Help \u00b6 For assistance or questions, please email the OSG User Support team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Running a Molecule Docking Job with AutoDock Vina"},{"location":"software_examples_for_osg/drug_discovery/tutorial-AutoDockVina/#overview","text":"AutoDock Vina is a molecular docking program useful for computer aided drug design. In this tutorial, we will learn how to run AutoDock Vina on OSG.","title":"Overview"},{"location":"software_examples_for_osg/drug_discovery/tutorial-AutoDockVina/#tutorial-files","text":"It is easiest to start with the tutorial command. Type: $ tutorial AutoDockVina This will create a directory tutorial-AutodockVina . Change into the directory and look at the available files: $ cd tutorial-AutoDockVina $ ls You should see the following: receptor_config.txt # Configuration file (input) data/ receptor.pdbqt # Receptor coordinates and atomic charges (input) ligand.pdbqt # Ligand coordinates and atomic charges (input) vina_job.submit # Job submission file vina_wrapper.bash # Execution script We need to download the AutoDock program separately into the this directory as well. Go to the AutoDock Vina website and click on the Download link at the top of the page. Download the Linux version of the program; you can do this directly to the current directory by using the wget command and the download link. If you use the -O option shown below, it will rename the tar file to match what is used in the rest of the guide. $ wget http://vina.scripps.edu/download/autodock_vina_1_1_2_linux_x86.tgz -O autodock_vina.tar.gz","title":"Tutorial Files"},{"location":"software_examples_for_osg/drug_discovery/tutorial-AutoDockVina/#files-need-to-submit-the-job","text":"The file vina_job.submit is the job submission file and contains the description of the job in HTCondor language. Specifically, it includes an \"executable\" (the script HTCondor will use in the job to run vina), a list of the files needed to run the job (shown in \"transfer_input_files\"), and indications of where to write logging information and what resources and requirements the job needs. Change needed: If your download software tar.gz file has a different name, change the name in the transfer_input_files line below. executable = vina_wrapper.bash transfer_input_files = data/, receptor_config.txt, autodock_vina.tar.gz should_transfer_files = Yes when_to_transfer_output = ON_EXIT output = job.$(Cluster).$(Process).out error = job.$(Cluster).$(Process).error log = job.$(Cluster).$(Process).log requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 512MB request_disk = 512MB queue 1 Next we see the execution wrapper vina_wrapper.bash . The execution wrapper and its inside content are executed on a worker node out in the Open Science Pool. The first two commands will unzip the file containing the AutoDock Vina program and make it accessible on the command line; we can then run a typical vina command. Change needed: If your download tar.gz file has a different name, change it in the script below, in the tar command. You will also need to change the name of the AUTODOCK_FOLDER to match whatever folder appears when you un-tar the downloaded tar.gz file. 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash # set TMPDIR variable export TMPDIR = $_CONDOR_SCRATCH_DIR # Unzip autodock vina software tar -xzf autodock_vina.tar.gz export PATH = $PWD /AUTODOCK_FOLDER/bin: $PATH # Run vina vina --config receptor_config.txt \\ --ligand ligand.pdbqt --out receptor-ligand.pdbqt \\ --log receptor-ligand.log","title":"Files Need to Submit the Job"},{"location":"software_examples_for_osg/drug_discovery/tutorial-AutoDockVina/#running-the-simulation","text":"We submit the job using condor_submit command as follows $ condor_submit vina_job.submit Now you have submitted the AutoDock Vina job on the OSG. The present job should be finished quickly (less than 10 mins). You can check the status of the submitted job by using condor_q command as follows: $ condor_q username After job completion, you will see the output files receptor-ligand_output.pdbqt and receptor-ligand.log in your work directory.","title":"Running the simulation"},{"location":"software_examples_for_osg/drug_discovery/tutorial-AutoDockVina/#next-steps","text":"After running this example, you may want to scale up to testing multiple molecules or ligands. Some good starting points for this include: Decide how many docking runs you want to try per job. If one molecule can be tested in a few seconds, you can probably run a few hundred in a job that runs in about an hour. How should you divide up the input data in this case? Do you need individual input files for each molecule, or can you use one to share? Should the molecule files all get copied to every job or just the jobs where they're needed? You can separate groups of files by putting them in separate directories or tar.gz files to help with this. Look at this guide to see different ways that you can use HTCondor to submit multiple jobs at once. If you want to use a different (or additional) docking programs, you can include them in the same job by downloading and including those software files in your job submission.","title":"Next Steps"},{"location":"software_examples_for_osg/drug_discovery/tutorial-AutoDockVina/#getting-help","text":"For assistance or questions, please email the OSG User Support team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Getting Help"},{"location":"software_examples_for_osg/freesurfer/Introduction/","text":"Introduction to FreeSurfer on the OSPool \u00b6 Overview \u00b6 FreeSurfer is a software package to analyze MRI scans of human brains. The OSG used to have a service called Fsurf, which is now discontinued. Instead we have community supported FreeSurfer container image and workflow. Please see: https://github.com/pegasus-isi/freesurfer-osg-workflow - scroll down to see the documentaion on this page. Container image: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:latest and defined at https://github.com/opensciencegrid/osgvo-freesurfer Prerequisites \u00b6 To use the FreeSurfer on OSG, you need: Your own FreeSurfer license file (see: https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#License ) A regular OSG Connect account. Privacy and Confidentiality of Subjects \u00b6 In order to protect the privacy of your participants\u2019 scans, we request that you submit only defaced and fully deidentified scans for processing. Getting Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums . Acknowledging the OSG Consortium \u00b6 We gratefully request your acknowledgement of the OSG in publications benefiting from this service as described here .","title":"Introduction to FreeSurfer on the OSPool "},{"location":"software_examples_for_osg/freesurfer/Introduction/#introduction-to-freesurfer-on-the-ospool","text":"","title":"Introduction to FreeSurfer on the OSPool"},{"location":"software_examples_for_osg/freesurfer/Introduction/#overview","text":"FreeSurfer is a software package to analyze MRI scans of human brains. The OSG used to have a service called Fsurf, which is now discontinued. Instead we have community supported FreeSurfer container image and workflow. Please see: https://github.com/pegasus-isi/freesurfer-osg-workflow - scroll down to see the documentaion on this page. Container image: /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-freesurfer:latest and defined at https://github.com/opensciencegrid/osgvo-freesurfer","title":"Overview"},{"location":"software_examples_for_osg/freesurfer/Introduction/#prerequisites","text":"To use the FreeSurfer on OSG, you need: Your own FreeSurfer license file (see: https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#License ) A regular OSG Connect account.","title":"Prerequisites"},{"location":"software_examples_for_osg/freesurfer/Introduction/#privacy-and-confidentiality-of-subjects","text":"In order to protect the privacy of your participants\u2019 scans, we request that you submit only defaced and fully deidentified scans for processing.","title":"Privacy and Confidentiality of Subjects"},{"location":"software_examples_for_osg/freesurfer/Introduction/#getting-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Getting Help"},{"location":"software_examples_for_osg/freesurfer/Introduction/#acknowledging-the-osg-consortium","text":"We gratefully request your acknowledgement of the OSG in publications benefiting from this service as described here .","title":"Acknowledging the OSG Consortium"},{"location":"software_examples_for_osg/machine_learning/tutorial-tensorflow-containers/","text":"Overview Defining container images Adding a container to the OSG CVMFS distribution mechanism Testing the container on the submit host Running a CPU job Running a GPU job Overview \u00b6 In this tutorial, we explore GPUs and containers on OSG, using the popular Tensorflow sofware package. Tensorflow is a good example here as the software is too complex to bundle up and ship with your job. Containers solve this problem by defining a full OS image, containing not only the complex software package, but dependencies and environment configuration as well. https://www.tensorflow.org/ desribes TensorFlow as: TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well. Defining container images \u00b6 Defining containers is fully described in the Docker and Singularity Containers section. Here we will just provide an overview of how you could take something like an existing Tensorflow image provided by OSG staff, and extend it by adding your own modules to it. Let's assume you like Tensorflow version 2.3. The definition of this image can be found in Github: Dockerfile . You don't really need to understand how an image was built in order to use it. As described in the containers documentation, make sure the HTCondor submit file has: Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3\" If you want to extend an existing image, you can just inherit from the parent image available on DockerHub here . For example, if you just need some additional Python packages, your new Dockerfile could look like: FROM opensciencegrid/tensorflow:2.3 RUN python3 -m pip install some_package_name You can then docker build and docker push it so that your new image is available on DockerHub. Note that OSG does not provide any infrastructure for these steps. You will have to complete them on your own computer or using the DockerHub build infrastructure. Adding a container to the OSG CVMFS distribution mechanism \u00b6 How to add a container image to the OSG CVMFS distribution mechanism is also described in Docker and Singularity Containers , but a quick scan of the cvmfs-singularity-sync and specifically the docker_images.txt file show us that the tensorflow images are listed as: opensciencegrid/tensorflow:* opensciencegrid/tensorflow-gpu:* Those two lines means that all tags from those two DockerHub repositories should be mapped to /cvmfs/singularity.opensciencegrid.org/ . On the login node, try running: ls /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3/ This is the image in its expanded form - something we can execute with Singularity! Testing the container on the submit host \u00b6 First, download the files contained in this tutorial to the login node using the tutorial command and cd into the tutorial directory that is created: tutorial tensorflow-containers cd tutorial-tensorflow-containers Before submitting jobs to the OSG, it is always a good idea to test your code so that you understand runtime requirements. The containers can be tested on the OSGConnect submit hosts with singularity shell , which will drop you into a container and let you exlore it interactively. To explore the Tensorflow 2.3 image, run: singularity shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3/ Note how the command line prompt changes, providing you an indicator that you are inside the image. You can exit any time by running exit . Another important thing to note is that your $HOME directory is automatically mounted inside the interactive container - allowing you to access your codes and test it out. First, start with a simple python3 import test to make sure tensorflow is available: $ python3 Python 3.6.9 (default, Jul 17 2020, 12:50:27) [GCC 8.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import tensorflow 2021-01-15 17:32:33.901607: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory 2021-01-15 17:32:33.901735: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. >>> Tensorflow will warn you that no GPUs where found. This is expected as we do not have GPUs attached to our login nodes, and it is fine as Tensorflow works fine with regular CPUs (slower of course). Exit out of Python3 with CTRL+D and then we can run a Tensorflow testcode which can be found in this tutorial: $ python3 test.py 2021-01-15 17:37:43.152892: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory 2021-01-15 17:37:43.153021: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 2021-01-15 17:37:44.899967: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory 2021-01-15 17:37:44.900063: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303) 2021-01-15 17:37:44.900130: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (login05.osgconnect.net): /proc/driver/nvidia/version does not exist 2021-01-15 17:37:44.900821: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-01-15 17:37:44.912483: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2700000000 Hz 2021-01-15 17:37:44.915548: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fa0bf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2021-01-15 17:37:44.915645: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2021-01-15 17:37:44.921895: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0 tf.Tensor( [[22. 28.] [49. 64.]], shape=(2, 2), dtype=float32) We will again see a bunch of warnings regarding GPUs not being available, but as we can see by the /job:localhost/replica:0/task:0/device:CPU:0 line, the code ran on one of the CPUs. When testing your own code like this, take note of how much memory, disk and runtime is required - it is needed in the next step. Once you are done with testing, use CTRL+D or run exit to exit out of the container. Note that you can not submit jobs from within the container. Running a CPU job \u00b6 If Tensorflow can run on GPUs, you might be wondering why we might want to run it on slower CPUs? One reason is that CPUs are plentiful while GPUs are still somewhat scarce. If you have a lot of shorter Tensorflow jobs, they might complete faster on available CPUs, rather than wait in the queue for the faster, less available, GPUs. The good news is that Tensorflow code should work in both enviroments automatically, so if your code runs too slow on CPUs, moving to GPUs should be easy. To submit our job, we need a submit file and a job wrapper script. The submit file is a basic OSGConnect flavored HTCondor file, specifying that we want the job to run in a container. cpu-job.submit contains: universe = vanilla # Job requirements - ensure we are running on a Singularity enabled # node and have enough resources to execute our code # Tensorflow also requires AVX instruction set and a newer host kernel Requirements = HAS_SINGULARITY == True && HAS_AVX2 == True && OSG_HOST_KERNEL_VERSION >= 31000 request_cpus = 1 request_gpus = 0 request_memory = 1 GB request_disk = 1 GB # Container image to run the job in +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3\" # Executable is the program your job will run It's often useful # to create a shell script to \"wrap\" your actual work. Executable = job-wrapper.sh Arguments = # Inputs/outputs - in this case we just need our python code. # If you leave out transfer_output_files, all generated files comes back transfer_input_files = test.py #transfer_output_files = # Error and output are the error and output channels from your job # that HTCondor returns from the remote host. Error = $(Cluster).$(Process).error Output = $(Cluster).$(Process).output # The LOG file is where HTCondor places information about your # job's status, success, and resource consumption. Log = $(Cluster).log # Send the job to Held state on failure. #on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # Periodically retry the jobs every 1 hour, up to a maximum of 5 retries. #periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 60*60) # queue is the \"start button\" - it launches any jobs that have been # specified thus far. queue 1 And job-wrapper.sh: 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash set -e # set TMPDIR variable export TMPDIR = $_CONDOR_SCRATCH_DIR echo echo \"I'm running on\" $( hostname -f ) echo \"OSG site: $OSG_SITE_NAME \" echo python3 test.py 2 > & 1 The job can now be submitted with condor_submit cpu-job.submit . Once the job is done, check the files named after the job id for the outputs. Running a GPU job \u00b6 When moving the job to be run on a GPU, all we have to do is update two lines in the submit file: set request_gpus to 1 and specify a GPU enabled container image for +SingularityImage . The updated submit file can be found in gpu-job.submit with the contents: universe = vanilla # Job requirements - ensure we are running on a Singularity enabled # node and have enough resources to execute our code # Tensorflow also requires AVX instruction set and a newer host kernel Requirements = HAS_SINGULARITY == True && HAS_AVX2 == True && OSG_HOST_KERNEL_VERSION >= 31000 request_cpus = 1 request_gpus = 1 request_memory = 1 GB request_disk = 1 GB # Container image to run the job in +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3\" # Executable is the program your job will run It's often useful # to create a shell script to \"wrap\" your actual work. Executable = job-wrapper.sh Arguments = # Inputs/outputs - in this case we just need our python code. # If you leave out transfer_output_files, all generated files comes back transfer_input_files = test.py #transfer_output_files = # Error and output are the error and output channels from your job # that HTCondor returns from the remote host. Error = $(Cluster).$(Process).error Output = $(Cluster).$(Process).output # The LOG file is where HTCondor places information about your # job's status, success, and resource consumption. Log = $(Cluster).log # Send the job to Held state on failure. #on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # Periodically retry the jobs every 1 hour, up to a maximum of 5 retries. #periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 60*60) # queue is the \"start button\" - it launches any jobs that have been # specified thus far. queue 1 Submit a job with condor_submit gpu-job.submit . Once the job is complete, check the .out file for a line stating the code was run under a GPU. Something similar to: 2021-02-02 23:25:19.022467: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0 The GPU:0 parts shows that a GPU was found and used for the computation.","title":"Working with Tensorflow, GPUs, and containers"},{"location":"software_examples_for_osg/machine_learning/tutorial-tensorflow-containers/#overview","text":"In this tutorial, we explore GPUs and containers on OSG, using the popular Tensorflow sofware package. Tensorflow is a good example here as the software is too complex to bundle up and ship with your job. Containers solve this problem by defining a full OS image, containing not only the complex software package, but dependencies and environment configuration as well. https://www.tensorflow.org/ desribes TensorFlow as: TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.","title":"Overview"},{"location":"software_examples_for_osg/machine_learning/tutorial-tensorflow-containers/#defining-container-images","text":"Defining containers is fully described in the Docker and Singularity Containers section. Here we will just provide an overview of how you could take something like an existing Tensorflow image provided by OSG staff, and extend it by adding your own modules to it. Let's assume you like Tensorflow version 2.3. The definition of this image can be found in Github: Dockerfile . You don't really need to understand how an image was built in order to use it. As described in the containers documentation, make sure the HTCondor submit file has: Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3\" If you want to extend an existing image, you can just inherit from the parent image available on DockerHub here . For example, if you just need some additional Python packages, your new Dockerfile could look like: FROM opensciencegrid/tensorflow:2.3 RUN python3 -m pip install some_package_name You can then docker build and docker push it so that your new image is available on DockerHub. Note that OSG does not provide any infrastructure for these steps. You will have to complete them on your own computer or using the DockerHub build infrastructure.","title":"Defining container images"},{"location":"software_examples_for_osg/machine_learning/tutorial-tensorflow-containers/#adding-a-container-to-the-osg-cvmfs-distribution-mechanism","text":"How to add a container image to the OSG CVMFS distribution mechanism is also described in Docker and Singularity Containers , but a quick scan of the cvmfs-singularity-sync and specifically the docker_images.txt file show us that the tensorflow images are listed as: opensciencegrid/tensorflow:* opensciencegrid/tensorflow-gpu:* Those two lines means that all tags from those two DockerHub repositories should be mapped to /cvmfs/singularity.opensciencegrid.org/ . On the login node, try running: ls /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3/ This is the image in its expanded form - something we can execute with Singularity!","title":"Adding a container to the OSG CVMFS distribution mechanism"},{"location":"software_examples_for_osg/machine_learning/tutorial-tensorflow-containers/#testing-the-container-on-the-submit-host","text":"First, download the files contained in this tutorial to the login node using the tutorial command and cd into the tutorial directory that is created: tutorial tensorflow-containers cd tutorial-tensorflow-containers Before submitting jobs to the OSG, it is always a good idea to test your code so that you understand runtime requirements. The containers can be tested on the OSGConnect submit hosts with singularity shell , which will drop you into a container and let you exlore it interactively. To explore the Tensorflow 2.3 image, run: singularity shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3/ Note how the command line prompt changes, providing you an indicator that you are inside the image. You can exit any time by running exit . Another important thing to note is that your $HOME directory is automatically mounted inside the interactive container - allowing you to access your codes and test it out. First, start with a simple python3 import test to make sure tensorflow is available: $ python3 Python 3.6.9 (default, Jul 17 2020, 12:50:27) [GCC 8.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import tensorflow 2021-01-15 17:32:33.901607: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory 2021-01-15 17:32:33.901735: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. >>> Tensorflow will warn you that no GPUs where found. This is expected as we do not have GPUs attached to our login nodes, and it is fine as Tensorflow works fine with regular CPUs (slower of course). Exit out of Python3 with CTRL+D and then we can run a Tensorflow testcode which can be found in this tutorial: $ python3 test.py 2021-01-15 17:37:43.152892: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory 2021-01-15 17:37:43.153021: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 2021-01-15 17:37:44.899967: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory 2021-01-15 17:37:44.900063: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303) 2021-01-15 17:37:44.900130: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (login05.osgconnect.net): /proc/driver/nvidia/version does not exist 2021-01-15 17:37:44.900821: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2021-01-15 17:37:44.912483: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2700000000 Hz 2021-01-15 17:37:44.915548: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fa0bf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2021-01-15 17:37:44.915645: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2021-01-15 17:37:44.921895: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0 tf.Tensor( [[22. 28.] [49. 64.]], shape=(2, 2), dtype=float32) We will again see a bunch of warnings regarding GPUs not being available, but as we can see by the /job:localhost/replica:0/task:0/device:CPU:0 line, the code ran on one of the CPUs. When testing your own code like this, take note of how much memory, disk and runtime is required - it is needed in the next step. Once you are done with testing, use CTRL+D or run exit to exit out of the container. Note that you can not submit jobs from within the container.","title":"Testing the container on the submit host"},{"location":"software_examples_for_osg/machine_learning/tutorial-tensorflow-containers/#running-a-cpu-job","text":"If Tensorflow can run on GPUs, you might be wondering why we might want to run it on slower CPUs? One reason is that CPUs are plentiful while GPUs are still somewhat scarce. If you have a lot of shorter Tensorflow jobs, they might complete faster on available CPUs, rather than wait in the queue for the faster, less available, GPUs. The good news is that Tensorflow code should work in both enviroments automatically, so if your code runs too slow on CPUs, moving to GPUs should be easy. To submit our job, we need a submit file and a job wrapper script. The submit file is a basic OSGConnect flavored HTCondor file, specifying that we want the job to run in a container. cpu-job.submit contains: universe = vanilla # Job requirements - ensure we are running on a Singularity enabled # node and have enough resources to execute our code # Tensorflow also requires AVX instruction set and a newer host kernel Requirements = HAS_SINGULARITY == True && HAS_AVX2 == True && OSG_HOST_KERNEL_VERSION >= 31000 request_cpus = 1 request_gpus = 0 request_memory = 1 GB request_disk = 1 GB # Container image to run the job in +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:2.3\" # Executable is the program your job will run It's often useful # to create a shell script to \"wrap\" your actual work. Executable = job-wrapper.sh Arguments = # Inputs/outputs - in this case we just need our python code. # If you leave out transfer_output_files, all generated files comes back transfer_input_files = test.py #transfer_output_files = # Error and output are the error and output channels from your job # that HTCondor returns from the remote host. Error = $(Cluster).$(Process).error Output = $(Cluster).$(Process).output # The LOG file is where HTCondor places information about your # job's status, success, and resource consumption. Log = $(Cluster).log # Send the job to Held state on failure. #on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # Periodically retry the jobs every 1 hour, up to a maximum of 5 retries. #periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 60*60) # queue is the \"start button\" - it launches any jobs that have been # specified thus far. queue 1 And job-wrapper.sh: 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash set -e # set TMPDIR variable export TMPDIR = $_CONDOR_SCRATCH_DIR echo echo \"I'm running on\" $( hostname -f ) echo \"OSG site: $OSG_SITE_NAME \" echo python3 test.py 2 > & 1 The job can now be submitted with condor_submit cpu-job.submit . Once the job is done, check the files named after the job id for the outputs.","title":"Running a CPU job"},{"location":"software_examples_for_osg/machine_learning/tutorial-tensorflow-containers/#running-a-gpu-job","text":"When moving the job to be run on a GPU, all we have to do is update two lines in the submit file: set request_gpus to 1 and specify a GPU enabled container image for +SingularityImage . The updated submit file can be found in gpu-job.submit with the contents: universe = vanilla # Job requirements - ensure we are running on a Singularity enabled # node and have enough resources to execute our code # Tensorflow also requires AVX instruction set and a newer host kernel Requirements = HAS_SINGULARITY == True && HAS_AVX2 == True && OSG_HOST_KERNEL_VERSION >= 31000 request_cpus = 1 request_gpus = 1 request_memory = 1 GB request_disk = 1 GB # Container image to run the job in +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.3\" # Executable is the program your job will run It's often useful # to create a shell script to \"wrap\" your actual work. Executable = job-wrapper.sh Arguments = # Inputs/outputs - in this case we just need our python code. # If you leave out transfer_output_files, all generated files comes back transfer_input_files = test.py #transfer_output_files = # Error and output are the error and output channels from your job # that HTCondor returns from the remote host. Error = $(Cluster).$(Process).error Output = $(Cluster).$(Process).output # The LOG file is where HTCondor places information about your # job's status, success, and resource consumption. Log = $(Cluster).log # Send the job to Held state on failure. #on_exit_hold = (ExitBySignal == True) || (ExitCode != 0) # Periodically retry the jobs every 1 hour, up to a maximum of 5 retries. #periodic_release = (NumJobStarts < 5) && ((CurrentTime - EnteredCurrentStatus) > 60*60) # queue is the \"start button\" - it launches any jobs that have been # specified thus far. queue 1 Submit a job with condor_submit gpu-job.submit . Once the job is complete, check the .out file for a line stating the code was run under a GPU. Something similar to: 2021-02-02 23:25:19.022467: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0 The GPU:0 parts shows that a GPU was found and used for the computation.","title":"Running a GPU job"},{"location":"software_examples_for_osg/matlab_runtime/tutorial-matlab-HelloWorld/","text":"Overview MATLAB script: hello_world.m Compilation Running standalone binary applications on OSG Tutorial files Executing the MATLAB application binary Job execution and submission files Job submmision Job outputs What's next? Getting help Overview \u00b6 MATLAB\u00ae is a licensed high level language and modeling toolkit. The MATLAB Compiler\u2122 lets you share MATLAB programs as standalone applications. MATLAB Compiler is invoked with mcc . The compiler supports most toolboxes and user-developed interfaces. For more details, check the list of supported toolboxes and ineligible programs . All applications created with MATLAB Compiler use MATLAB Compiler Runtime\u2122 (MCR) , which enables royalty-free deployment and use. We assume you have access to a server that has MATLAB compiler because the compiler is not available on OSG Connect. MATLAB Runtime is available on OSG Connect. Although the compiled binaries are portable, they need to have a compatible, OS-specific matlab runtime to interpret the binary. We recommend the compilation of your matlab program against matlab versions that match the OSG modules (or containers , with the compilation executed on a server with Scientific Linux version 7 so that the compiled binaries are portable on OSG machines. In this tutorial, we learn the basics of compiling MATLAB programs on a licensed linux machine and running the compiled binaries using a matlab compiled runtime (MCR) in the OSG modules or containers. MATLAB script: hello_world.m \u00b6 Lets start with a simple MATLAB script hello_world.m that prints Hello World! to standard output. function helloworld fprintf('\\n=============') fprintf('\\nHello, World!\\n') fprintf('=============\\n') end Compilation \u00b6 OSG connect does not have a license to use the MATLAB compiler . On a Linux server with a MATLAB license, invoke the compiler mcc . We turn off all graphical options ( -nodisplay ), disable Java ( -nojvm ), and instruct MATLAB to run this application as a single-threaded application ( -singleCompThread ): mcc -m -R -singleCompThread -R -nodisplay -R -nojvm hello_world.m The flag -m means C language translation during compilation, and the flag -R indicates runtime options. The compilation would produce the files: `hello_world, run_hello_world.sh, mccExcludedFiles.log` and `readme.txt` The file hello_world is the standalone executable. The file run_hello_world.sh is MATLAB generated shell script. mccExcludedFiles.log is the log file and readme.txt contains the information about the compilation process. We just need the standalone binary file hello_world . Running standalone binary applications on OSG \u00b6 To see which releases are available on OSG: $ ssh username@login.osgconnect.net # login on OSG connect login node $ module spider matlab ------------------------------------------------------- matlab: matlab/R2018b ------------------------------------------------------- This module can be loaded directly: module load matlab/R2018b Tutorial files \u00b6 Let us say you have created the standalone binary hello_world . Transfer the file hello_world to login.osgconnect.net. Alternatively, you may also use the readily available files by invoking the tutorial command: $ tutorial matlab-HelloWorld # Copies input and script files to the directory tutorial-matlab-HelloWorld. This will create a directory tutorial-matlab-HelloWorld . Inside the directory, you will see the following files hello_world # compiled executable binary of hello_world.m hello_world.m # matlab program hello_world.submit # condor job description file hello_world.sh # execution script Executing the MATLAB application binary \u00b6 The compilation and execution environment need to the same. The file hello_world is a standalone binary of the matlab program hello_world.m which was compiled using MATLAB 2018b on a Linux platform. The login node and many of the worker nodes on OSG are based on Linux platform. In addition to the platform requirement, we also need to have the same MATLAB Runtime version. Load the MATLAB runtime for 2018b version via module command. On the terminal prompt, type $ module load matlab/R2018b The above command sets up the environment to run the matlab/2018b runtime applications. Now execute the binary $ ./hello_world (would produce the following output) ============= Hello, World! ============= If you get the above output, the binary execution is successful. Next, we see how to submit the job on a remote execute point using HTcondor. Job execution and submission files \u00b6 Let us take a look at hello_world.submit file: Universe = vanilla # One OSG Connect vanilla, the preffered job universe is \"vanilla\" Executable = hello_world.sh # Job execution file which is transfered to execute point transfer_input_files = hello_world # list of file(s) need be transffered to the remote execute point Output = Log/job.$(Process).out\u22c5 # standard output Error = Log/job.$(Process).err # standard error Log = Log/job.$(Process).log # log information about job execution requirements = OSGVO_OS_STRING == \"RHEL 7\" && Arch == \"X86_64\" && HAS_MODULES == True queue 10 # Submit 10 jobs The wrapper script hello_world.sh 1 2 3 4 5 #!/bin/bash set -e module load matlab/R2018b chmod +x hello_world ./hello_world loads the correct matlab module and executes the binary. If you are using an OSG-supported Matlab container , you don't need the module load command. Before we submit the job, make sure that the directory Log exists on the current working directory. Because HTcondor looks for Log directory to copy the standard output, error and log files as specified in the job description file. From your work directory, type $ mkdir -p Log Absence of Log directory would send the jobs to held state. Job submmision \u00b6 We submit the job using the condor_submit command as follows $ condor_submit hello_world.submit //Submit the condor job description file \"hello_world.submit\" Now you have submitted an ensemble of 10 MATLAB jobs. Each job prints hello world on the standard output. Check the status of the submitted job, $ condor_q username # The status of the job is printed on the screen. Here, username is your login name. Job outputs \u00b6 The hello_world.m script sends the output to standard output. In the condor job description file, we expressed that the standard output is written on the Log/job.$(ProcessID).out . After job completion, ten output files are produced with the hello world message under the directory Log . What's next? \u00b6 Sure, it is not very exciting to print the same message on 10 output files. In the subsequent MATLAB examples, we see how to scale up MATLAB computation on HTC environment. Getting help \u00b6 For assistance or questions, please email the OSG User Support team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Basics of compiled MATLAB applications - Hello World example"},{"location":"software_examples_for_osg/matlab_runtime/tutorial-matlab-HelloWorld/#overview","text":"MATLAB\u00ae is a licensed high level language and modeling toolkit. The MATLAB Compiler\u2122 lets you share MATLAB programs as standalone applications. MATLAB Compiler is invoked with mcc . The compiler supports most toolboxes and user-developed interfaces. For more details, check the list of supported toolboxes and ineligible programs . All applications created with MATLAB Compiler use MATLAB Compiler Runtime\u2122 (MCR) , which enables royalty-free deployment and use. We assume you have access to a server that has MATLAB compiler because the compiler is not available on OSG Connect. MATLAB Runtime is available on OSG Connect. Although the compiled binaries are portable, they need to have a compatible, OS-specific matlab runtime to interpret the binary. We recommend the compilation of your matlab program against matlab versions that match the OSG modules (or containers , with the compilation executed on a server with Scientific Linux version 7 so that the compiled binaries are portable on OSG machines. In this tutorial, we learn the basics of compiling MATLAB programs on a licensed linux machine and running the compiled binaries using a matlab compiled runtime (MCR) in the OSG modules or containers.","title":"Overview"},{"location":"software_examples_for_osg/matlab_runtime/tutorial-matlab-HelloWorld/#matlab-script-hello_worldm","text":"Lets start with a simple MATLAB script hello_world.m that prints Hello World! to standard output. function helloworld fprintf('\\n=============') fprintf('\\nHello, World!\\n') fprintf('=============\\n') end","title":"MATLAB script: hello_world.m"},{"location":"software_examples_for_osg/matlab_runtime/tutorial-matlab-HelloWorld/#compilation","text":"OSG connect does not have a license to use the MATLAB compiler . On a Linux server with a MATLAB license, invoke the compiler mcc . We turn off all graphical options ( -nodisplay ), disable Java ( -nojvm ), and instruct MATLAB to run this application as a single-threaded application ( -singleCompThread ): mcc -m -R -singleCompThread -R -nodisplay -R -nojvm hello_world.m The flag -m means C language translation during compilation, and the flag -R indicates runtime options. The compilation would produce the files: `hello_world, run_hello_world.sh, mccExcludedFiles.log` and `readme.txt` The file hello_world is the standalone executable. The file run_hello_world.sh is MATLAB generated shell script. mccExcludedFiles.log is the log file and readme.txt contains the information about the compilation process. We just need the standalone binary file hello_world .","title":"Compilation"},{"location":"software_examples_for_osg/matlab_runtime/tutorial-matlab-HelloWorld/#running-standalone-binary-applications-on-osg","text":"To see which releases are available on OSG: $ ssh username@login.osgconnect.net # login on OSG connect login node $ module spider matlab ------------------------------------------------------- matlab: matlab/R2018b ------------------------------------------------------- This module can be loaded directly: module load matlab/R2018b","title":"Running standalone binary applications on OSG"},{"location":"software_examples_for_osg/matlab_runtime/tutorial-matlab-HelloWorld/#tutorial-files","text":"Let us say you have created the standalone binary hello_world . Transfer the file hello_world to login.osgconnect.net. Alternatively, you may also use the readily available files by invoking the tutorial command: $ tutorial matlab-HelloWorld # Copies input and script files to the directory tutorial-matlab-HelloWorld. This will create a directory tutorial-matlab-HelloWorld . Inside the directory, you will see the following files hello_world # compiled executable binary of hello_world.m hello_world.m # matlab program hello_world.submit # condor job description file hello_world.sh # execution script","title":"Tutorial files"},{"location":"software_examples_for_osg/matlab_runtime/tutorial-matlab-HelloWorld/#executing-the-matlab-application-binary","text":"The compilation and execution environment need to the same. The file hello_world is a standalone binary of the matlab program hello_world.m which was compiled using MATLAB 2018b on a Linux platform. The login node and many of the worker nodes on OSG are based on Linux platform. In addition to the platform requirement, we also need to have the same MATLAB Runtime version. Load the MATLAB runtime for 2018b version via module command. On the terminal prompt, type $ module load matlab/R2018b The above command sets up the environment to run the matlab/2018b runtime applications. Now execute the binary $ ./hello_world (would produce the following output) ============= Hello, World! ============= If you get the above output, the binary execution is successful. Next, we see how to submit the job on a remote execute point using HTcondor.","title":"Executing the MATLAB application binary"},{"location":"software_examples_for_osg/matlab_runtime/tutorial-matlab-HelloWorld/#job-execution-and-submission-files","text":"Let us take a look at hello_world.submit file: Universe = vanilla # One OSG Connect vanilla, the preffered job universe is \"vanilla\" Executable = hello_world.sh # Job execution file which is transfered to execute point transfer_input_files = hello_world # list of file(s) need be transffered to the remote execute point Output = Log/job.$(Process).out\u22c5 # standard output Error = Log/job.$(Process).err # standard error Log = Log/job.$(Process).log # log information about job execution requirements = OSGVO_OS_STRING == \"RHEL 7\" && Arch == \"X86_64\" && HAS_MODULES == True queue 10 # Submit 10 jobs The wrapper script hello_world.sh 1 2 3 4 5 #!/bin/bash set -e module load matlab/R2018b chmod +x hello_world ./hello_world loads the correct matlab module and executes the binary. If you are using an OSG-supported Matlab container , you don't need the module load command. Before we submit the job, make sure that the directory Log exists on the current working directory. Because HTcondor looks for Log directory to copy the standard output, error and log files as specified in the job description file. From your work directory, type $ mkdir -p Log Absence of Log directory would send the jobs to held state.","title":"Job execution and submission files"},{"location":"software_examples_for_osg/matlab_runtime/tutorial-matlab-HelloWorld/#job-submmision","text":"We submit the job using the condor_submit command as follows $ condor_submit hello_world.submit //Submit the condor job description file \"hello_world.submit\" Now you have submitted an ensemble of 10 MATLAB jobs. Each job prints hello world on the standard output. Check the status of the submitted job, $ condor_q username # The status of the job is printed on the screen. Here, username is your login name.","title":"Job submmision"},{"location":"software_examples_for_osg/matlab_runtime/tutorial-matlab-HelloWorld/#job-outputs","text":"The hello_world.m script sends the output to standard output. In the condor job description file, we expressed that the standard output is written on the Log/job.$(ProcessID).out . After job completion, ten output files are produced with the hello world message under the directory Log .","title":"Job outputs"},{"location":"software_examples_for_osg/matlab_runtime/tutorial-matlab-HelloWorld/#whats-next","text":"Sure, it is not very exciting to print the same message on 10 output files. In the subsequent MATLAB examples, we see how to scale up MATLAB computation on HTC environment.","title":"What's next?"},{"location":"software_examples_for_osg/matlab_runtime/tutorial-matlab-HelloWorld/#getting-help","text":"For assistance or questions, please email the OSG User Support team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Getting help"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/","text":"Using conda to Run Python on the OSPool \u00b6 The Anaconda/Miniconda distribution of Python is a common tool for installing and managing Python-based software and other tools. Overview \u00b6 When should you use Miniconda as an installation method in OSG? * Your software has specific conda-centric installation instructions. * The above is true and the software has a lot of dependencies. * You mainly use Python to do your work. Notes on terminology: conda is a Python package manager and package ecosystem that exists in parallel with pip and PyPI . Miniconda is a slim Python distribution, containing the minimum amount of packages necessary for a Python installation that can use conda. Anaconda is a pre-built scientific Python distribution based on Miniconda that has many useful scientific packages pre-installed. To create the smallest, most portable Python installation possible, we recommend starting with Miniconda and installing only the packages you actually require. To use a Miniconda installation on OSG, create your installation environment on the submit server and send a zipped version to your jobs. Pre-Install Miniconda and Transfer to Jobs This guide also discusses how to \u201cpin\u201d your conda environment to create a more consistent and reproducible environment with specified versions of packages. Install Miniconda and Package for Jobs \u00b6 In this approach, we will create an entire software installation inside Miniconda and then use a tool called conda pack to package it up for running jobs. 1. Create a Miniconda Installation \u00b6 On the submit server, download the latest Linux miniconda installer and run it. [alice@login05]$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh [alice@login05]$ sh Miniconda3-latest-Linux-x86_64.sh Accept the license agreement and default options. At the end, you can choose whether or not to \u201cinitialize Miniconda3 by running conda init?\u201d The default is no; you would then run the eval command listed by the installer to \u201cactivate\u201d Miniconda. If you choose \u201cno\u201d you\u2019ll want to save this command so that you can reactivate the Miniconda installation when needed in the future. 2. Create a conda \"Environment\" With Your Packages \u00b6 (If you are using an environment.yml file as described later , you should instead create the environment from your environment.yml file. If you don\u2019t have an environment.yml file to work with, follow the install instructions in this section. We recommend switching to the environment.yml method of creating environments once you understand the \u201cmanual\u201d method presented here.) Make sure that you\u2019ve activated the base Miniconda environment if you haven\u2019t already. Your prompt should look like this: (base)[alice@login05]$ To create an environment, use the conda create command and then activate the environment: (base)[alice@login05]$ conda create -n env-name (base)[alice@login05]$ conda activate env-name Then, run the conda install command to install the different packages and software you want to include in the installation. How this should look is often listed in the installation examples for software (e.g. Qiime2 , Pytorch ). (env-name)[alice@login05]$ conda install pkg1 pkg2 Some Conda packages are only available via specific Conda channels which serve as repositories for hosting and managing packages. If Conda is unable to locate the requested packages using the example above, you may need to have Conda search other channels. More detail are available at https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/channels.html. Packages may also be installed via pip , but you should only do this when there is no conda package available. Once everything is installed, deactivate the environment to go back to the Miniconda \u201cbase\u201d environment. (env-name)[alice@login05]$ conda deactivate For example, if you wanted to create an installation with pandas and matplotlib and call the environment py-data-sci , you would use this sequence of commands: (base)[alice@login05]$ conda create -n py-data-sci (base)[alice@login05]$ conda activate py-data-sci (py-data-sci)[alice@login05]$ conda install pandas matplotlib (py-data-sci)[alice@login05]$ conda deactivate (base)[alice@login05]$ More About Miniconda \u00b6 See the official conda documentation for more information on creating and managing environments with conda . 3. Create Software Package \u00b6 Make sure that your job\u2019s Miniconda environment is created, but deactivated, so that you\u2019re in the \u201cbase\u201d Miniconda environment: (base)[alice@login05]$ Then, run this command to install the conda pack tool: (base)[alice@login05]$ conda install -c conda-forge conda-pack Enter y when it asks you to install. Finally, use conda pack to create a zipped tar.gz file of your environment (substitute the name of your conda environment where you see env-name ), set the proper permissions for this file using chmod , and check the size of the final tarball: (base)[alice@login05]$ conda pack -n env-name (base)[alice@login05]$ chmod 644 env-name.tar.gz (base)[alice@login05]$ ls -sh env-name.tar.gz When this step finishes, you should see a file in your current directory named env-name.tar.gz . 4. Check Size of Conda Environment Tar Archive \u00b6 The tar archive, env-name.tar.gz , created in the previous step will be used as input for subsequent job submission. As with all job input files, you should check the size of this Conda environment file. If >100MB in size, you should NOT transfer the tar ball using transfer_input_files from your home directory . Instead, you should plan to use OSG Connect's /public folder, and a stash:/// link, as described in this guide . Please contact a research computing facilitator at support@opensciencegrid.org if you have questions about the best option for your jobs. More information is available at File Availability with Squid Web Proxy and Managing Large Data in HTC Jobs . 5. Create a Job Executable \u00b6 The job will need to go through a few steps to use this \u201cpacked\u201d conda environment; first, setting the PATH , then unzipping the environment, then activating it, and finally running whatever program you like. The script below is an example of what is needed (customize as indicated to match your choices above). #!/bin/bash # have job exit if any command returns with non-zero exit status (aka failure) set -e # replace env-name on the right hand side of this line with the name of your conda environment ENVNAME=env-name # if you need the environment directory to be named something other than the environment name, change this line ENVDIR=$ENVNAME # these lines handle setting up the environment; you shouldn't have to modify them export PATH mkdir $ENVDIR tar -xzf $ENVNAME.tar.gz -C $ENVDIR . $ENVDIR/bin/activate # modify this line to run your desired Python script and any other work you need to do python3 hello.py 6. Submit Jobs \u00b6 In your submit file, make sure to have the following: Your executable should be the the bash script you created in step 5 . Remember to transfer your Python script and the environment tar.gz file to the job. If the tar.gz file is larger than 100MB, please use the stash:/// file delivery mechanism as described above. Specificying Exact Dependency Versions \u00b6 An important part of improving reproducibility and consistency between runs is to ensure that you use the correct/expected versions of your dependencies. When you run a command like conda install numpy conda tries to install the most recent version of numpy For example, numpy version 1.22.3 was released on Mar 7, 2022. To install exactly this version of numpy, you would run conda install numpy=1.22.3 (the same works for pip if you replace = with == ). We recommend installing with an explicit version to make sure you have exactly the version of a package that you want. This is often called \u201cpinning\u201d or \u201clocking\u201d the version of the package. If you want a record of what is installed in your environment, or want to reproduce your environment on another computer, conda can create a file, usually called environment.yml , that describes the exact versions of all of the packages you have installed in an environment. This file can be re-used by a different conda command to recreate that exact environment on another computer. To create an environment.yml file from your currently-activated environment, run [alice@login05]$ conda env export > environment.yml This environment.yml will pin the exact version of every dependency in your environment. This can sometimes be problematic if you are moving between platforms because a package version may not be available on some other platform, causing an \u201cunsatisfiable dependency\u201d or \u201cinconsistent environment\u201d error. A much less strict pinning is [alice@login05]$ conda env export --from-history > environment.yml which only lists packages that you installed manually, and does not pin their versions unless you yourself pinned them during installation . If you need an intermediate solution, it is also possible to manually edit environment.yml files; see the conda environment documentation for more details about the format and what is possible. In general, exact environment specifications are simply not guaranteed to be transferable between platforms (e.g., between Windows and Linux). We strongly recommend using the strictest possible pinning available to you . To create an environment from an environment.yml file, run [alice@login05]$ conda env create -f environment.yml By default, the name of the environment will be whatever the name of the source environment was; you can change the name by adding a -n \\<name> option to the conda env create command. If you use a source control system like git , we recommend checking your environment.yml file into source control and making sure to recreate it when you make changes to your environment. Putting your environment under source control gives you a way to track how it changes along with your own code. If you are developing software on your local computer for eventual use on the Open Science pool, your workflow might look like this: Set up a conda environment for local development and install packages as desired (e.g., conda create -n science; conda activate science; conda install numpy ). Once you are ready to run on the Open Science pool, create an environment.yml file from your local environment (e.g., conda env export > environment.yml ). Move your environment.yml file from your local computer to the submit machine and create an environment from it (e.g., conda env create -f environment.yml ), then pack it for use in your jobs, as per Create Software Package . More information on conda environments can be found in their documentation .","title":"Using conda to Run Python on the OSPool "},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#using-conda-to-run-python-on-the-ospool","text":"The Anaconda/Miniconda distribution of Python is a common tool for installing and managing Python-based software and other tools.","title":"Using conda to Run Python on the OSPool"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#overview","text":"When should you use Miniconda as an installation method in OSG? * Your software has specific conda-centric installation instructions. * The above is true and the software has a lot of dependencies. * You mainly use Python to do your work. Notes on terminology: conda is a Python package manager and package ecosystem that exists in parallel with pip and PyPI . Miniconda is a slim Python distribution, containing the minimum amount of packages necessary for a Python installation that can use conda. Anaconda is a pre-built scientific Python distribution based on Miniconda that has many useful scientific packages pre-installed. To create the smallest, most portable Python installation possible, we recommend starting with Miniconda and installing only the packages you actually require. To use a Miniconda installation on OSG, create your installation environment on the submit server and send a zipped version to your jobs. Pre-Install Miniconda and Transfer to Jobs This guide also discusses how to \u201cpin\u201d your conda environment to create a more consistent and reproducible environment with specified versions of packages.","title":"Overview"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#install-miniconda-and-package-for-jobs","text":"In this approach, we will create an entire software installation inside Miniconda and then use a tool called conda pack to package it up for running jobs.","title":"Install Miniconda and Package for Jobs"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#1-create-a-miniconda-installation","text":"On the submit server, download the latest Linux miniconda installer and run it. [alice@login05]$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh [alice@login05]$ sh Miniconda3-latest-Linux-x86_64.sh Accept the license agreement and default options. At the end, you can choose whether or not to \u201cinitialize Miniconda3 by running conda init?\u201d The default is no; you would then run the eval command listed by the installer to \u201cactivate\u201d Miniconda. If you choose \u201cno\u201d you\u2019ll want to save this command so that you can reactivate the Miniconda installation when needed in the future.","title":"1. Create a Miniconda Installation"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#2-create-a-conda-environment-with-your-packages","text":"(If you are using an environment.yml file as described later , you should instead create the environment from your environment.yml file. If you don\u2019t have an environment.yml file to work with, follow the install instructions in this section. We recommend switching to the environment.yml method of creating environments once you understand the \u201cmanual\u201d method presented here.) Make sure that you\u2019ve activated the base Miniconda environment if you haven\u2019t already. Your prompt should look like this: (base)[alice@login05]$ To create an environment, use the conda create command and then activate the environment: (base)[alice@login05]$ conda create -n env-name (base)[alice@login05]$ conda activate env-name Then, run the conda install command to install the different packages and software you want to include in the installation. How this should look is often listed in the installation examples for software (e.g. Qiime2 , Pytorch ). (env-name)[alice@login05]$ conda install pkg1 pkg2 Some Conda packages are only available via specific Conda channels which serve as repositories for hosting and managing packages. If Conda is unable to locate the requested packages using the example above, you may need to have Conda search other channels. More detail are available at https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/channels.html. Packages may also be installed via pip , but you should only do this when there is no conda package available. Once everything is installed, deactivate the environment to go back to the Miniconda \u201cbase\u201d environment. (env-name)[alice@login05]$ conda deactivate For example, if you wanted to create an installation with pandas and matplotlib and call the environment py-data-sci , you would use this sequence of commands: (base)[alice@login05]$ conda create -n py-data-sci (base)[alice@login05]$ conda activate py-data-sci (py-data-sci)[alice@login05]$ conda install pandas matplotlib (py-data-sci)[alice@login05]$ conda deactivate (base)[alice@login05]$","title":"2. Create a conda \"Environment\" With Your Packages"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#more-about-miniconda","text":"See the official conda documentation for more information on creating and managing environments with conda .","title":"More About Miniconda"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#3-create-software-package","text":"Make sure that your job\u2019s Miniconda environment is created, but deactivated, so that you\u2019re in the \u201cbase\u201d Miniconda environment: (base)[alice@login05]$ Then, run this command to install the conda pack tool: (base)[alice@login05]$ conda install -c conda-forge conda-pack Enter y when it asks you to install. Finally, use conda pack to create a zipped tar.gz file of your environment (substitute the name of your conda environment where you see env-name ), set the proper permissions for this file using chmod , and check the size of the final tarball: (base)[alice@login05]$ conda pack -n env-name (base)[alice@login05]$ chmod 644 env-name.tar.gz (base)[alice@login05]$ ls -sh env-name.tar.gz When this step finishes, you should see a file in your current directory named env-name.tar.gz .","title":"3. Create Software Package"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#4-check-size-of-conda-environment-tar-archive","text":"The tar archive, env-name.tar.gz , created in the previous step will be used as input for subsequent job submission. As with all job input files, you should check the size of this Conda environment file. If >100MB in size, you should NOT transfer the tar ball using transfer_input_files from your home directory . Instead, you should plan to use OSG Connect's /public folder, and a stash:/// link, as described in this guide . Please contact a research computing facilitator at support@opensciencegrid.org if you have questions about the best option for your jobs. More information is available at File Availability with Squid Web Proxy and Managing Large Data in HTC Jobs .","title":"4. Check Size of Conda Environment Tar Archive"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#5-create-a-job-executable","text":"The job will need to go through a few steps to use this \u201cpacked\u201d conda environment; first, setting the PATH , then unzipping the environment, then activating it, and finally running whatever program you like. The script below is an example of what is needed (customize as indicated to match your choices above). #!/bin/bash # have job exit if any command returns with non-zero exit status (aka failure) set -e # replace env-name on the right hand side of this line with the name of your conda environment ENVNAME=env-name # if you need the environment directory to be named something other than the environment name, change this line ENVDIR=$ENVNAME # these lines handle setting up the environment; you shouldn't have to modify them export PATH mkdir $ENVDIR tar -xzf $ENVNAME.tar.gz -C $ENVDIR . $ENVDIR/bin/activate # modify this line to run your desired Python script and any other work you need to do python3 hello.py","title":"5. Create a Job Executable"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#6-submit-jobs","text":"In your submit file, make sure to have the following: Your executable should be the the bash script you created in step 5 . Remember to transfer your Python script and the environment tar.gz file to the job. If the tar.gz file is larger than 100MB, please use the stash:/// file delivery mechanism as described above.","title":"6. Submit Jobs"},{"location":"software_examples_for_osg/other_languages_tools/conda-on-osg/#specificying-exact-dependency-versions","text":"An important part of improving reproducibility and consistency between runs is to ensure that you use the correct/expected versions of your dependencies. When you run a command like conda install numpy conda tries to install the most recent version of numpy For example, numpy version 1.22.3 was released on Mar 7, 2022. To install exactly this version of numpy, you would run conda install numpy=1.22.3 (the same works for pip if you replace = with == ). We recommend installing with an explicit version to make sure you have exactly the version of a package that you want. This is often called \u201cpinning\u201d or \u201clocking\u201d the version of the package. If you want a record of what is installed in your environment, or want to reproduce your environment on another computer, conda can create a file, usually called environment.yml , that describes the exact versions of all of the packages you have installed in an environment. This file can be re-used by a different conda command to recreate that exact environment on another computer. To create an environment.yml file from your currently-activated environment, run [alice@login05]$ conda env export > environment.yml This environment.yml will pin the exact version of every dependency in your environment. This can sometimes be problematic if you are moving between platforms because a package version may not be available on some other platform, causing an \u201cunsatisfiable dependency\u201d or \u201cinconsistent environment\u201d error. A much less strict pinning is [alice@login05]$ conda env export --from-history > environment.yml which only lists packages that you installed manually, and does not pin their versions unless you yourself pinned them during installation . If you need an intermediate solution, it is also possible to manually edit environment.yml files; see the conda environment documentation for more details about the format and what is possible. In general, exact environment specifications are simply not guaranteed to be transferable between platforms (e.g., between Windows and Linux). We strongly recommend using the strictest possible pinning available to you . To create an environment from an environment.yml file, run [alice@login05]$ conda env create -f environment.yml By default, the name of the environment will be whatever the name of the source environment was; you can change the name by adding a -n \\<name> option to the conda env create command. If you use a source control system like git , we recommend checking your environment.yml file into source control and making sure to recreate it when you make changes to your environment. Putting your environment under source control gives you a way to track how it changes along with your own code. If you are developing software on your local computer for eventual use on the Open Science pool, your workflow might look like this: Set up a conda environment for local development and install packages as desired (e.g., conda create -n science; conda activate science; conda install numpy ). Once you are ready to run on the Open Science pool, create an environment.yml file from your local environment (e.g., conda env export > environment.yml ). Move your environment.yml file from your local computer to the submit machine and create an environment from it (e.g., conda env create -f environment.yml ), then pack it for use in your jobs, as per Create Software Package . More information on conda environments can be found in their documentation .","title":"Specificying Exact Dependency Versions"},{"location":"software_examples_for_osg/other_languages_tools/java-on-osg/","text":"Using Java in Jobs \u00b6 Overview \u00b6 If your code uses Java via a .jar file, it is easy to bring along your own copy of the Java Development Kit (JDK) which allows you to run your .jar file anywhere on the Open Science Pool. Steps to Use Java in Jobs \u00b6 Get a copy of Java/JDK. You can access the the Java Development Kit (JDK) from the JDK website . First select the link to the JDK that is listed as \"Ready for Use\" and then download the Linux/x64 version of the tar.gz file using a Unix command such as wget from your /home directory. For example, $ wget https://download.java.net/java/GA/jdk17.0.1/2a2082e5a09d4267845be086888add4f/12/GPL/openjdk-17.0.1_linux-x64_bin.tar.gz The downloaded file should end up in your home directory on the OSG Connect access point. Include Java in Input Files. Add the downloaded tar file to the transfer_input_files line of your submit file, along with the .jar file and any other input files the job needs: transfer_input_files = openjdk-17.0.1_linux-x64_bin.tar.gz, program.jar, other_input Setup Java inside the job. Write a script that unpacks the JDK tar file, sets the environment to find the java software, and then runs your program. This script will be your job\\'s executable. See this example for what the script should look like: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash # unzip the JDK tar -xzf openjdk-17.0.1_linux-x64_bin.tar.gz # Add the unzipped JDK folder to the environment export PATH = $PWD /jdk-17.0.1/bin: $PATH export JAVA_HOME = $PWD /jdk-17.0.1 # run your .jar file java -jar program.jar Note that the exact name of the unzipped JDK folder and the JDK tar.gz file will vary depending on the version you downloaded. You should unzip the JDK tar.gz file in your home directory to find out the correct directory name to add to the script.","title":"Using Java in Jobs "},{"location":"software_examples_for_osg/other_languages_tools/java-on-osg/#using-java-in-jobs","text":"","title":"Using Java in Jobs"},{"location":"software_examples_for_osg/other_languages_tools/java-on-osg/#overview","text":"If your code uses Java via a .jar file, it is easy to bring along your own copy of the Java Development Kit (JDK) which allows you to run your .jar file anywhere on the Open Science Pool.","title":"Overview"},{"location":"software_examples_for_osg/other_languages_tools/java-on-osg/#steps-to-use-java-in-jobs","text":"Get a copy of Java/JDK. You can access the the Java Development Kit (JDK) from the JDK website . First select the link to the JDK that is listed as \"Ready for Use\" and then download the Linux/x64 version of the tar.gz file using a Unix command such as wget from your /home directory. For example, $ wget https://download.java.net/java/GA/jdk17.0.1/2a2082e5a09d4267845be086888add4f/12/GPL/openjdk-17.0.1_linux-x64_bin.tar.gz The downloaded file should end up in your home directory on the OSG Connect access point. Include Java in Input Files. Add the downloaded tar file to the transfer_input_files line of your submit file, along with the .jar file and any other input files the job needs: transfer_input_files = openjdk-17.0.1_linux-x64_bin.tar.gz, program.jar, other_input Setup Java inside the job. Write a script that unpacks the JDK tar file, sets the environment to find the java software, and then runs your program. This script will be your job\\'s executable. See this example for what the script should look like: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash # unzip the JDK tar -xzf openjdk-17.0.1_linux-x64_bin.tar.gz # Add the unzipped JDK folder to the environment export PATH = $PWD /jdk-17.0.1/bin: $PATH export JAVA_HOME = $PWD /jdk-17.0.1 # run your .jar file java -jar program.jar Note that the exact name of the unzipped JDK folder and the JDK tar.gz file will vary depending on the version you downloaded. You should unzip the JDK tar.gz file in your home directory to find out the correct directory name to add to the script.","title":"Steps to Use Java in Jobs"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/","text":"Using Julia on the OSPool \u00b6 Overview \u00b6 This guide provides an introduction to running Julia code on the Open Science Pool. The Quickstart Instructions provide an outline of job submission. The following sections provide more details about installing Julia packages ( Install Julia Packages ) and creating a complete job submission ( Submit Julia Jobs ). This guide assumes that you have a script written in Julia and can identify the additional Julia packages needed to run the script. If you are using many Julia packages or have other software dependencies as part of your job, you may want to manage your software via a container instead of using the tar.gz file method described in this guide. The OSG Connect team maintains a Julia container that can be used as a starting point for creating a customized container with added packages. See our Docker and Singularity Guide for more details. Quickstart Instructions \u00b6 Download the precompiled Julia software from https://julialang.org/downloads/ . You will need the 64-bit, tarball compiled for general use on a Linux x86 system. The file name will resemble something like julia-#.#.#-linux-x86_64.tar.gz . Tip: use wget to download directly to your /home directory on the login node, OR use transfer_input_files = url in your HTCondor submit files. Install your Julia packages on the login node, else skip to the next step. For more details, see the section on installing Julia packages below: Installing Julia Packages Submit a job that executes a Julia script using the Julia precompiled binary with base Julia and Standard Library, via a shell script like the following as the job's executable: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash # extract Julia tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz # add Julia binary to PATH export PATH = $_CONDOR_SCRATCH_DIR /julia-#-#-#/bin: $PATH # run Julia script julia my-script.jl For more details on the job submission, see the section below: Submit Julia Jobs Install Julia Packages \u00b6 If your work requires additional Julia packages, you will need to peform a one-time installation of these packages within a Julia project. A copy of the project can then be saved for use in subsequent job submissions. For more details, please see Julia's documentation at Julia Pkg.jl . Download Julia and set up a \"project\" \u00b6 If you have not already downloaded a copy of Julia, download the precompiled Julia software from https://julialang.org/downloads/ . You will need the 64-bit, tarball compiled for general use on a Linux x86 system. The file name will resemble something like julia-#.#.#-linux-x86_64.tar.gz . We will need a copy of the original tar.gz file for running jobs, but to install packages, we also need an unpacked version of the software. Run the following commands to extract the Julia software and add Julia to your PATH : $ tar -xzf julia-#.#.#-linux-x86_64.tar.gz $ export PATH=$PWD/julia-#.#.#/bin:$PATH After these steps, you should be able to run Julia from the command line, e.g. $ julia --version Now create a project directory to install your packages (we've called it my-project/ below) and tell Julia its name: $ mkdir my-project $ export JULIA_DEPOT_PATH=$PWD/my-project If you already have a directory with Julia packages on the login node, you can add to it by skipping the mkdir step above and going straight to setting the JULIA_DEPOT_PATH variable. You can choose whatever name to use for this directory -- if you have different projects that you use for different jobs, you could use a more descriptive name than \"my-project\". Install Packages \u00b6 We will now use Julia to install any needed packages to the project directory we created in the previous step. Open Julia with the --project option set to the project directory: $ julia --project=my-project Once you've started up the Julia REPL (interpreter), start the Pkg REPL, used to install packages, by typing ] . Then install and test packages by using Julia's add Package syntax. _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type \"?\" for help, \"]?\" for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.0.5 (2019-09-09) _/ |\\__'_|_|_|\\__'_| | Official https://julialang.org/ release |__/ | julia> ] (my-project) pkg> add Package (my-project) pkg> test Package If you have multiple packages to install they can be combined into a single command, e.g. (my-project) pkg> add Package1 Package2 Package3 . If you encounter issues getting packages to install successfully, please contact us at support@opensciencegrid.org Once you are done, you can exit the Pkg REPL by typing the DELETE key and then typing exit() (my-project) pkg> julia> exit() Your packages will have been installed to the my_project directory; we want to compress this folder so that it is easier to copy to jobs. $ tar -czf my-project.tar.gz my-project/ Submit Julia Jobs \u00b6 To submit a job that runs a Julia script, create a bash script and HTCondor submit file following the examples in this section. These example assume that you have downloaded a copy of Julia for Linux as a tar.gz file and if using packages, you have gone through the steps above to install them and create an additional tar.gz file of the installed packages. Create Executable Bash Script \u00b6 Your job will use a bash script as the HTCondor executable . This script will contain all the steps needed to unpack the Julia binaries and execute your Julia script ( script.jl below). What follows are two example bash scripts, one which can be used to execute a script with base Julia only, and one that will use packages you installed to a project directory (see Install Julia Packages ). Example Bash Script For Base Julia Only \u00b6 If your Julia script can run without additional packages (other than base Julia and the Julia Standard library) use the example script directly below. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash # julia-job.sh # extract Julia tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz # add Julia binary to PATH export PATH = $_CONDOR_SCRATCH_DIR /julia-#.#.#/bin: $PATH # run Julia script julia script.jl Example Bash Script For Julia With Installed Packages \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash # julia-job.sh # extract Julia tar.gz file and project tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz tar -xzf my-project.tar.gz # add Julia binary to PATH export PATH = $_CONDOR_SCRATCH_DIR /julia-#.#.#/bin: $PATH # add Julia packages to DEPOT variable export JULIA_DEPOT_PATH = $_CONDOR_SCRATCH_DIR /my-project # run Julia script julia --project = my-project script.jl Create HTCondor Submit File \u00b6 After creating a bash script to run Julia, then create a submit file to submit the job. More details about setting up a submit file, including a submit file template, can be found in our quickstart guide: Quickstart Tutorial # julia-job.sub executable = julia-job.sh transfer_input_files = julia-#.#.#-linux-x86_64.tar.gz, script.jl should_transfer_files = Yes when_to_transfer_output = ON_EXIT output = job.$(Cluster).$(Process).out error = job.$(Cluster).$(Process).error log = job.$(Cluster).$(Process).log requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 If your Julia script needs to use packages installed for a project, be sure to include my-project.tar.gz as an input file in julia-job.sub . For project tarballs that are <100MB, you can follow the below example: transfer_input_files = julia-#.#.#-linux-x86_64.tar.gz, script.jl, my-project.tar.gz Modify the CPU/memory request lines to match what is needed by the job. Test a few jobs for disk space/memory usage in order to make sure your requests for a large batch are accurate! Disk space and memory usage can be found in the log file after the job completes.","title":"Using Julia on the OSPool "},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#using-julia-on-the-ospool","text":"","title":"Using Julia on the OSPool"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#overview","text":"This guide provides an introduction to running Julia code on the Open Science Pool. The Quickstart Instructions provide an outline of job submission. The following sections provide more details about installing Julia packages ( Install Julia Packages ) and creating a complete job submission ( Submit Julia Jobs ). This guide assumes that you have a script written in Julia and can identify the additional Julia packages needed to run the script. If you are using many Julia packages or have other software dependencies as part of your job, you may want to manage your software via a container instead of using the tar.gz file method described in this guide. The OSG Connect team maintains a Julia container that can be used as a starting point for creating a customized container with added packages. See our Docker and Singularity Guide for more details.","title":"Overview"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#quickstart-instructions","text":"Download the precompiled Julia software from https://julialang.org/downloads/ . You will need the 64-bit, tarball compiled for general use on a Linux x86 system. The file name will resemble something like julia-#.#.#-linux-x86_64.tar.gz . Tip: use wget to download directly to your /home directory on the login node, OR use transfer_input_files = url in your HTCondor submit files. Install your Julia packages on the login node, else skip to the next step. For more details, see the section on installing Julia packages below: Installing Julia Packages Submit a job that executes a Julia script using the Julia precompiled binary with base Julia and Standard Library, via a shell script like the following as the job's executable: 1 2 3 4 5 6 7 8 9 10 #!/bin/bash # extract Julia tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz # add Julia binary to PATH export PATH = $_CONDOR_SCRATCH_DIR /julia-#-#-#/bin: $PATH # run Julia script julia my-script.jl For more details on the job submission, see the section below: Submit Julia Jobs","title":"Quickstart Instructions"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#install-julia-packages","text":"If your work requires additional Julia packages, you will need to peform a one-time installation of these packages within a Julia project. A copy of the project can then be saved for use in subsequent job submissions. For more details, please see Julia's documentation at Julia Pkg.jl .","title":"Install Julia Packages"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#download-julia-and-set-up-a-project","text":"If you have not already downloaded a copy of Julia, download the precompiled Julia software from https://julialang.org/downloads/ . You will need the 64-bit, tarball compiled for general use on a Linux x86 system. The file name will resemble something like julia-#.#.#-linux-x86_64.tar.gz . We will need a copy of the original tar.gz file for running jobs, but to install packages, we also need an unpacked version of the software. Run the following commands to extract the Julia software and add Julia to your PATH : $ tar -xzf julia-#.#.#-linux-x86_64.tar.gz $ export PATH=$PWD/julia-#.#.#/bin:$PATH After these steps, you should be able to run Julia from the command line, e.g. $ julia --version Now create a project directory to install your packages (we've called it my-project/ below) and tell Julia its name: $ mkdir my-project $ export JULIA_DEPOT_PATH=$PWD/my-project If you already have a directory with Julia packages on the login node, you can add to it by skipping the mkdir step above and going straight to setting the JULIA_DEPOT_PATH variable. You can choose whatever name to use for this directory -- if you have different projects that you use for different jobs, you could use a more descriptive name than \"my-project\".","title":"Download Julia and set up a \"project\""},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#install-packages","text":"We will now use Julia to install any needed packages to the project directory we created in the previous step. Open Julia with the --project option set to the project directory: $ julia --project=my-project Once you've started up the Julia REPL (interpreter), start the Pkg REPL, used to install packages, by typing ] . Then install and test packages by using Julia's add Package syntax. _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type \"?\" for help, \"]?\" for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.0.5 (2019-09-09) _/ |\\__'_|_|_|\\__'_| | Official https://julialang.org/ release |__/ | julia> ] (my-project) pkg> add Package (my-project) pkg> test Package If you have multiple packages to install they can be combined into a single command, e.g. (my-project) pkg> add Package1 Package2 Package3 . If you encounter issues getting packages to install successfully, please contact us at support@opensciencegrid.org Once you are done, you can exit the Pkg REPL by typing the DELETE key and then typing exit() (my-project) pkg> julia> exit() Your packages will have been installed to the my_project directory; we want to compress this folder so that it is easier to copy to jobs. $ tar -czf my-project.tar.gz my-project/","title":"Install Packages"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#submit-julia-jobs","text":"To submit a job that runs a Julia script, create a bash script and HTCondor submit file following the examples in this section. These example assume that you have downloaded a copy of Julia for Linux as a tar.gz file and if using packages, you have gone through the steps above to install them and create an additional tar.gz file of the installed packages.","title":"Submit Julia Jobs"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#create-executable-bash-script","text":"Your job will use a bash script as the HTCondor executable . This script will contain all the steps needed to unpack the Julia binaries and execute your Julia script ( script.jl below). What follows are two example bash scripts, one which can be used to execute a script with base Julia only, and one that will use packages you installed to a project directory (see Install Julia Packages ).","title":"Create Executable Bash Script"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#example-bash-script-for-base-julia-only","text":"If your Julia script can run without additional packages (other than base Julia and the Julia Standard library) use the example script directly below. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash # julia-job.sh # extract Julia tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz # add Julia binary to PATH export PATH = $_CONDOR_SCRATCH_DIR /julia-#.#.#/bin: $PATH # run Julia script julia script.jl","title":"Example Bash Script For Base Julia Only"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#example-bash-script-for-julia-with-installed-packages","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash # julia-job.sh # extract Julia tar.gz file and project tar.gz file tar -xzf julia-#.#.#-linux-x86_64.tar.gz tar -xzf my-project.tar.gz # add Julia binary to PATH export PATH = $_CONDOR_SCRATCH_DIR /julia-#.#.#/bin: $PATH # add Julia packages to DEPOT variable export JULIA_DEPOT_PATH = $_CONDOR_SCRATCH_DIR /my-project # run Julia script julia --project = my-project script.jl","title":"Example Bash Script For Julia With Installed Packages"},{"location":"software_examples_for_osg/other_languages_tools/julia-on-osg/#create-htcondor-submit-file","text":"After creating a bash script to run Julia, then create a submit file to submit the job. More details about setting up a submit file, including a submit file template, can be found in our quickstart guide: Quickstart Tutorial # julia-job.sub executable = julia-job.sh transfer_input_files = julia-#.#.#-linux-x86_64.tar.gz, script.jl should_transfer_files = Yes when_to_transfer_output = ON_EXIT output = job.$(Cluster).$(Process).out error = job.$(Cluster).$(Process).error log = job.$(Cluster).$(Process).log requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 If your Julia script needs to use packages installed for a project, be sure to include my-project.tar.gz as an input file in julia-job.sub . For project tarballs that are <100MB, you can follow the below example: transfer_input_files = julia-#.#.#-linux-x86_64.tar.gz, script.jl, my-project.tar.gz Modify the CPU/memory request lines to match what is needed by the job. Test a few jobs for disk space/memory usage in order to make sure your requests for a large batch are accurate! Disk space and memory usage can be found in the log file after the job completes.","title":"Create HTCondor Submit File"},{"location":"software_examples_for_osg/python/manage-python-packages/","text":"Run Python Scripts on the OSPool \u00b6 Overview \u00b6 This guide will show you two examples of how to run jobs that use Python in the Open Science Pool. The first example will demonstrate how to submit a job that uses base Python. The second example will demonstrate the workflow for jobs that use specific Python packages, including how to install a custom set of Python packages to your home directory and how to add them to a Python job submission. Before getting started, you should know which Python packages you need to run your job. Running Base Python on the Open Science Pool \u00b6 Create a bash script to run Python \u00b6 To submit jobs that use a module to run base Python, first create a bash executable - for this example we'll call it run_py.sh - which will run our Python script called myscript.py . For example, run_py.sh : 1 2 3 4 #!/bin/bash # Run the Python script python3 myscript.py If you need to use Python 2, replace the python3 above with python2 . Create an HTCondor submit file \u00b6 In order to submit run_py.sh as part of a job, we need to create an HTCondor submit file. This should include the following: run_py.sh specified as the executable use transfer_input_files to bring our Python script myscript.py to wherever the job runs include a standard OSG Connect Singularity image that has Python installed. All together, the submit file will look something like this: universe = vanilla +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest\" executable = run_py.sh transfer_input_files = myscript.py log = job.log output = job.out error = job.error request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 Once everything is set up, the job can be submitted in the usual way, by running the condor_submit command with the name of the submit file. Running Python Jobs That Use Additional Packages \u00b6 It's likely that you'll need additional Python packages that are not present in the base Python installations. This portion of the guide describes how to install your packages to a custom directory and then include them as part of your jobs. Install Python packages \u00b6 While connected to your login node, start the base Singularity container that has a copy of Python inside: $ singularity shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest Next, create a directory for your files and set the PYTHONPATH Singularity> mkdir my_env Singularity> export PYTHONPATH=$PWD/my_env You can swap out my_env for a more descriptive name like scipy or word-analysis . Now we can use pip to install Python packages. Singularity> pip3 install --target=$PWD/my_env numpy ......some download message... Installing collected packages: numpy Installing collected packages: numpy Successfully installed numpy-1.16.3 Install each package that you need for your job using the pip install command. If you would like to test the package installation, you can run the python3 command and then try importing the packages you just installed. To exit the Python console, type \"quit()\" Once you are done, you can leave the virtual environment: Singularity> exit All of the packages that were just installed should be contained in a sub-directory of the my_env directory. To use these packages in a job, the entire my_env directory will be transfered as a tar.gz file. So our final step is to compress the directory, as follows: $ tar -czf my_env.tar.gz my_env Create executable script to use installed packages \u00b6 In addition to loading the appropriate Python module, we will need to add a few steps to our bash executable to set-up the virtual environment we just created. That will look something like this: 1 2 3 4 5 6 7 8 #!/bin/bash # Unpack your envvironment (with your packages), and activate it tar -xzf my_env.tar.gz export PYTHONPATH = $PWD /my_env # Run the Python script python3 myscript.py Modify the HTCondor submit file to transfer Python packages \u00b6 The submit file for this job will be similar to the base Python job submit file shown above with one addition - we need to include my_env.tar.gz in the list of files specified by transfer_input_files . As an example: universe = vanilla +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest\" executable = run_py.sh transfer_input_files = myscript.py, my_env.tar.gz log = job.log output = job.out error = job.error request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 Other Considerations \u00b6 This guide mainly focuses on the nuts and bolts of running Python, but it's important to remember that additional files needed for your jobs (input data, setting files, etc.) need to be transferred with the job as well. See our Introduction to Data Management on OSG for details on the different ways to deliver inputs to your jobs. When you've prepared a real job submission, make sure to run a test job and then check the log file for disk and memory usage; if you're using significantly more or less than what you requested, make sure you adjust your requests. Getting Help \u00b6 For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Run Python Scripts on the OSPool "},{"location":"software_examples_for_osg/python/manage-python-packages/#run-python-scripts-on-the-ospool","text":"","title":"Run Python Scripts on the OSPool"},{"location":"software_examples_for_osg/python/manage-python-packages/#overview","text":"This guide will show you two examples of how to run jobs that use Python in the Open Science Pool. The first example will demonstrate how to submit a job that uses base Python. The second example will demonstrate the workflow for jobs that use specific Python packages, including how to install a custom set of Python packages to your home directory and how to add them to a Python job submission. Before getting started, you should know which Python packages you need to run your job.","title":"Overview"},{"location":"software_examples_for_osg/python/manage-python-packages/#running-base-python-on-the-open-science-pool","text":"","title":"Running Base Python on the Open Science Pool"},{"location":"software_examples_for_osg/python/manage-python-packages/#create-a-bash-script-to-run-python","text":"To submit jobs that use a module to run base Python, first create a bash executable - for this example we'll call it run_py.sh - which will run our Python script called myscript.py . For example, run_py.sh : 1 2 3 4 #!/bin/bash # Run the Python script python3 myscript.py If you need to use Python 2, replace the python3 above with python2 .","title":"Create a bash script to run Python"},{"location":"software_examples_for_osg/python/manage-python-packages/#create-an-htcondor-submit-file","text":"In order to submit run_py.sh as part of a job, we need to create an HTCondor submit file. This should include the following: run_py.sh specified as the executable use transfer_input_files to bring our Python script myscript.py to wherever the job runs include a standard OSG Connect Singularity image that has Python installed. All together, the submit file will look something like this: universe = vanilla +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest\" executable = run_py.sh transfer_input_files = myscript.py log = job.log output = job.out error = job.error request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1 Once everything is set up, the job can be submitted in the usual way, by running the condor_submit command with the name of the submit file.","title":"Create an HTCondor submit file"},{"location":"software_examples_for_osg/python/manage-python-packages/#running-python-jobs-that-use-additional-packages","text":"It's likely that you'll need additional Python packages that are not present in the base Python installations. This portion of the guide describes how to install your packages to a custom directory and then include them as part of your jobs.","title":"Running Python Jobs That Use Additional Packages"},{"location":"software_examples_for_osg/python/manage-python-packages/#install-python-packages","text":"While connected to your login node, start the base Singularity container that has a copy of Python inside: $ singularity shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest Next, create a directory for your files and set the PYTHONPATH Singularity> mkdir my_env Singularity> export PYTHONPATH=$PWD/my_env You can swap out my_env for a more descriptive name like scipy or word-analysis . Now we can use pip to install Python packages. Singularity> pip3 install --target=$PWD/my_env numpy ......some download message... Installing collected packages: numpy Installing collected packages: numpy Successfully installed numpy-1.16.3 Install each package that you need for your job using the pip install command. If you would like to test the package installation, you can run the python3 command and then try importing the packages you just installed. To exit the Python console, type \"quit()\" Once you are done, you can leave the virtual environment: Singularity> exit All of the packages that were just installed should be contained in a sub-directory of the my_env directory. To use these packages in a job, the entire my_env directory will be transfered as a tar.gz file. So our final step is to compress the directory, as follows: $ tar -czf my_env.tar.gz my_env","title":"Install Python packages"},{"location":"software_examples_for_osg/python/manage-python-packages/#create-executable-script-to-use-installed-packages","text":"In addition to loading the appropriate Python module, we will need to add a few steps to our bash executable to set-up the virtual environment we just created. That will look something like this: 1 2 3 4 5 6 7 8 #!/bin/bash # Unpack your envvironment (with your packages), and activate it tar -xzf my_env.tar.gz export PYTHONPATH = $PWD /my_env # Run the Python script python3 myscript.py","title":"Create executable script to use installed packages"},{"location":"software_examples_for_osg/python/manage-python-packages/#modify-the-htcondor-submit-file-to-transfer-python-packages","text":"The submit file for this job will be similar to the base Python job submit file shown above with one addition - we need to include my_env.tar.gz in the list of files specified by transfer_input_files . As an example: universe = vanilla +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-20.04:latest\" executable = run_py.sh transfer_input_files = myscript.py, my_env.tar.gz log = job.log output = job.out error = job.error request_cpus = 1 request_memory = 2GB request_disk = 2GB queue 1","title":"Modify the HTCondor submit file to transfer Python packages"},{"location":"software_examples_for_osg/python/manage-python-packages/#other-considerations","text":"This guide mainly focuses on the nuts and bolts of running Python, but it's important to remember that additional files needed for your jobs (input data, setting files, etc.) need to be transferred with the job as well. See our Introduction to Data Management on OSG for details on the different ways to deliver inputs to your jobs. When you've prepared a real job submission, make sure to run a test job and then check the log file for disk and memory usage; if you're using significantly more or less than what you requested, make sure you adjust your requests.","title":"Other Considerations"},{"location":"software_examples_for_osg/python/manage-python-packages/#getting-help","text":"For assistance or questions, please email the OSG Research Facilitation team at support@opensciencegrid.org or visit the help desk and community forums .","title":"Getting Help"},{"location":"software_examples_for_osg/python/tutorial-ScalingUp-Python/","text":"Overview Python script and the optimization function Execution Script Submitting Jobs Concurrently Providing Different Inputs to Jobs Another Example of Different Inputs Overview \u00b6 Many large scale computations require the ability to process multiple jobs concurrently. Consider the extensive sampling done for a multi-dimensional Monte Carlo integration, parameter sweep for a given model or molecular dynamics simulation with several initial conditions. These calculations require submitting many jobs. About a million CPU hours per day are available to OSG users on an opportunistic basis. Learning how to scale up and control large numbers of jobs is essential to realize the full potential of distributed high throughput computing on the OSG. The HTCondor's queue command can run multiple jobs from a single job description file. In this tutorial, we will see how to scale up the calculations for a simple python example using the HTCondor\u2019s queue command. Once we understand the basic HTCondor script to run a single job, it is easy to scale up. Obtain the example files via the tutorial command, $ tutorial ScalingUp-Python $ cd tutorial-ScalingUp-Python Inside the tutorial-ScalingUp-python directory, all the required files are available. This includes the sample python program, job description file and executable files. Python script and the optimization function \u00b6 Let us take a look at our objective function that we are trying to optimize. f = (1 - x)**2 + (y - x**2)**2 This a two dimensional Rosenbrock function. Clearly, the minimum is located at (1,1). The Rosenbrock function is one of the test functions used to test the robustness of an optimization method. Here, we are going to use the brute force optimization approach to evaluate the two dimensional Rosenbrock function on grids of points. The boundary values for the grid points are randomly assigned inside the python script. However, these default values may be replaced by user supplied values. To run the calculations with the random boundary values, the script is executed without any argument: module load py-scipy/1.1.0-py3.7 python rosen_brock_brute_opt.py To run the calculations with the user supplied values, the script is executed with input arguments: python rosen_brock_brute_opt.py x_low x_high y_low y_high where x_low and x_high are low and high values along x direction, and y_low and y_high are the low and high values along the y direction. For example, the boundary of x direction is (-3, 3) and the boundary of y direction is (-2, 3). python rosen_brock_brute_opt.py -3 3 -2 2 sets the boundary of x direction to (-3, 3) and the boundary of y direction to (-2, 3). The directory Example1 runs the python script with the default random values. The directories Example2 , and Example3 deal with supplying the boundary values as input arguments. Execution Script \u00b6 Let us take a look at the execution script, scalingup-python-wrapper.sh 1 2 3 4 5 6 7 #!/bin/bash module load py-scipy/1.1.0-py3.7 # set TMPDIR variable export TMPDIR = $_CONDOR_SCRATCH_DIR python3 ./rosen_brock_brute_opt.py $1 $2 $3 $4 The wrapper loads the the relevant module and then executes the python script rosen_brock_brute_opt.py . The python script takes four argument but they are optional. If we don't supply these optional arguments, the values are internally assigned. Submitting Jobs Concurrently \u00b6 Now let us take a look at job description file. cd Example1 cat ScalingUp-PythonCals.submit If we want to submit several jobs, we need to track log, out and error files for each job. An easy way to do this is to add the $(Cluster) and $(Process) variables to the file names. You can see this below in the names given to the standard output, standard error and HTCondor log files: executable = ../scalingup-python-wrapper.sh transfer_input_files = ../rosen_brock_brute_opt.py log = Log/job.$(Cluster).$(Process).log output = Log/job.$(Cluster).$(Process).out error = Log/job.$(Cluster).$(Process).err requirements = OSGVO_OS_STRING =?= \"RHEL 7\" && HAS_MODULES =?= True request_cpus = 1 request_memory = 1 GB request_disk = 1 GB queue 10 Note the queue 10 . This tells Condor to queue 10 copies of this job as one cluster. Let us submit the above job $ condor_submit ScalingUp-PythonCals.submit Submitting job(s).......... 10 job(s) submitted to cluster 329837. Apply your condor_q knowledge to see this job progress. After all jobs finished, execute the post_script.sh script to sort the results. ./post_script.sh Note that all ten jobs will have run with random arguments because we did not supply any from the submit file. What if we wanted to supply those arguments so that we could reproduce this analysis if needed? The next example shows how to do this. Providing Different Inputs to Jobs \u00b6 In the previous example, we did not pass any argument to the program and the program generated random boundary conditions. If we have some guess about what could be a better boundary condition, it is a good idea to supply the boundary condition as arguments. It is possible to use a single file to supply multiple arguments. We can take the job description file from the previous example, and modify it to include arguments. The modified job description file is available in the Example2 directory. Take a look at the job description file ScalingUp-PythonCals.submit . $ cd ../Example2 $ cat ScalingUp-PythonCals.submit executable = ../scalingup-python-wrapper.sh arguments = $(x_low) $(x_high) $(y_low) $(y_high) transfer_input_files = ../rosen_brock_brute_opt.py log = Log/job.$(Cluster).$(Process).log output = Log/job.$(Cluster).$(Process).out error = Log/job.$(Cluster).$(Process).err requirements = OSGVO_OS_STRING =?= \"RHEL 7\" && HAS_MODULES =?= True request_cpus = 1 request_memory = 1 GB request_disk = 1 GB queue x_low x_high y_low y_high from job_values.txt A major part of the job description file looks same as the previous example. The main difference is that the addition of arguments keyword, which looks like this: arguments = $(x_low) $(x_high) $(y_low) $(y_high) The given arguments $(x_low) , $(x_high) , etc. are actually variables that represent the values we want to use. These values are set in the queue command at the end of the file: queue x_low x_high y_low y_high from job_values.txt Take a look at job_values.txt: $ cat job_values.txt -9 9 -9 9 -8 8 -8 8 -7 7 -7 7 -6 6 -6 6 -5 5 -5 5 -4 4 -4 4 -3 3 -3 3 -2 2 -2 2 -1 1 -1 1 The submit file's queue statement will read in this file and assign each value in a row to the four variables shown in the queue statement. Each row corresponds to the submission of a unique job with those four values. Let us submit the above job to see this: $ condor_submit ScalingUp-PythonCals.submit Submitting job(s).......... 9 job(s) submitted to cluster 329840. Apply your condor_q knowledge to see this job progress. After all jobs finished, execute the post_script.sh script to sort the results. ./post_process.sh Another Example of Different Inputs \u00b6 In the previous example, we split the input information into four variables that were included in the arguments line. However, we could have set the arguments line directly, without intermediate values. This is shown in Example 3: $ cd ../Example3 $ cat ScalingUp-PythonCals.submit executable = ../scalingup-python-wrapper.sh transfer_input_files = ../rosen_brock_brute_opt.py log = Log/job.$(Cluster).$(Process).log output = Log/job.$(Cluster).$(Process).out error = Log/job.$(Cluster).$(Process).err requirements = OSGVO_OS_STRING =?= \"RHEL 7\" && HAS_MODULES =?= True request_cpus = 1 request_memory = 1 GB request_disk = 1 GB queue arguments from job_values.txt Here, arguments has disappeared from the top of the file because we've included it in the queue statement at the end. The job_values.txt file has the same values as before; in this syntax, HTCondor will submit a job for each row of values and the job's arguments will be those four values. Let us submit the above job $ condor_submit ScalingUp-PythonCals.submit Submitting job(s).......... 9 job(s) submitted to cluster 329839. Apply your condor_q and connect watch knowledge to see this job progress. After all jobs finished, execute the post_script.sh script to sort the results. ./post_process.sh","title":"Scaling Up With HTCondor\u2019s Queue Command"},{"location":"software_examples_for_osg/python/tutorial-ScalingUp-Python/#overview","text":"Many large scale computations require the ability to process multiple jobs concurrently. Consider the extensive sampling done for a multi-dimensional Monte Carlo integration, parameter sweep for a given model or molecular dynamics simulation with several initial conditions. These calculations require submitting many jobs. About a million CPU hours per day are available to OSG users on an opportunistic basis. Learning how to scale up and control large numbers of jobs is essential to realize the full potential of distributed high throughput computing on the OSG. The HTCondor's queue command can run multiple jobs from a single job description file. In this tutorial, we will see how to scale up the calculations for a simple python example using the HTCondor\u2019s queue command. Once we understand the basic HTCondor script to run a single job, it is easy to scale up. Obtain the example files via the tutorial command, $ tutorial ScalingUp-Python $ cd tutorial-ScalingUp-Python Inside the tutorial-ScalingUp-python directory, all the required files are available. This includes the sample python program, job description file and executable files.","title":"Overview"},{"location":"software_examples_for_osg/python/tutorial-ScalingUp-Python/#python-script-and-the-optimization-function","text":"Let us take a look at our objective function that we are trying to optimize. f = (1 - x)**2 + (y - x**2)**2 This a two dimensional Rosenbrock function. Clearly, the minimum is located at (1,1). The Rosenbrock function is one of the test functions used to test the robustness of an optimization method. Here, we are going to use the brute force optimization approach to evaluate the two dimensional Rosenbrock function on grids of points. The boundary values for the grid points are randomly assigned inside the python script. However, these default values may be replaced by user supplied values. To run the calculations with the random boundary values, the script is executed without any argument: module load py-scipy/1.1.0-py3.7 python rosen_brock_brute_opt.py To run the calculations with the user supplied values, the script is executed with input arguments: python rosen_brock_brute_opt.py x_low x_high y_low y_high where x_low and x_high are low and high values along x direction, and y_low and y_high are the low and high values along the y direction. For example, the boundary of x direction is (-3, 3) and the boundary of y direction is (-2, 3). python rosen_brock_brute_opt.py -3 3 -2 2 sets the boundary of x direction to (-3, 3) and the boundary of y direction to (-2, 3). The directory Example1 runs the python script with the default random values. The directories Example2 , and Example3 deal with supplying the boundary values as input arguments.","title":"Python script and the optimization function"},{"location":"software_examples_for_osg/python/tutorial-ScalingUp-Python/#execution-script","text":"Let us take a look at the execution script, scalingup-python-wrapper.sh 1 2 3 4 5 6 7 #!/bin/bash module load py-scipy/1.1.0-py3.7 # set TMPDIR variable export TMPDIR = $_CONDOR_SCRATCH_DIR python3 ./rosen_brock_brute_opt.py $1 $2 $3 $4 The wrapper loads the the relevant module and then executes the python script rosen_brock_brute_opt.py . The python script takes four argument but they are optional. If we don't supply these optional arguments, the values are internally assigned.","title":"Execution Script"},{"location":"software_examples_for_osg/python/tutorial-ScalingUp-Python/#submitting-jobs-concurrently","text":"Now let us take a look at job description file. cd Example1 cat ScalingUp-PythonCals.submit If we want to submit several jobs, we need to track log, out and error files for each job. An easy way to do this is to add the $(Cluster) and $(Process) variables to the file names. You can see this below in the names given to the standard output, standard error and HTCondor log files: executable = ../scalingup-python-wrapper.sh transfer_input_files = ../rosen_brock_brute_opt.py log = Log/job.$(Cluster).$(Process).log output = Log/job.$(Cluster).$(Process).out error = Log/job.$(Cluster).$(Process).err requirements = OSGVO_OS_STRING =?= \"RHEL 7\" && HAS_MODULES =?= True request_cpus = 1 request_memory = 1 GB request_disk = 1 GB queue 10 Note the queue 10 . This tells Condor to queue 10 copies of this job as one cluster. Let us submit the above job $ condor_submit ScalingUp-PythonCals.submit Submitting job(s).......... 10 job(s) submitted to cluster 329837. Apply your condor_q knowledge to see this job progress. After all jobs finished, execute the post_script.sh script to sort the results. ./post_script.sh Note that all ten jobs will have run with random arguments because we did not supply any from the submit file. What if we wanted to supply those arguments so that we could reproduce this analysis if needed? The next example shows how to do this.","title":"Submitting Jobs Concurrently"},{"location":"software_examples_for_osg/python/tutorial-ScalingUp-Python/#providing-different-inputs-to-jobs","text":"In the previous example, we did not pass any argument to the program and the program generated random boundary conditions. If we have some guess about what could be a better boundary condition, it is a good idea to supply the boundary condition as arguments. It is possible to use a single file to supply multiple arguments. We can take the job description file from the previous example, and modify it to include arguments. The modified job description file is available in the Example2 directory. Take a look at the job description file ScalingUp-PythonCals.submit . $ cd ../Example2 $ cat ScalingUp-PythonCals.submit executable = ../scalingup-python-wrapper.sh arguments = $(x_low) $(x_high) $(y_low) $(y_high) transfer_input_files = ../rosen_brock_brute_opt.py log = Log/job.$(Cluster).$(Process).log output = Log/job.$(Cluster).$(Process).out error = Log/job.$(Cluster).$(Process).err requirements = OSGVO_OS_STRING =?= \"RHEL 7\" && HAS_MODULES =?= True request_cpus = 1 request_memory = 1 GB request_disk = 1 GB queue x_low x_high y_low y_high from job_values.txt A major part of the job description file looks same as the previous example. The main difference is that the addition of arguments keyword, which looks like this: arguments = $(x_low) $(x_high) $(y_low) $(y_high) The given arguments $(x_low) , $(x_high) , etc. are actually variables that represent the values we want to use. These values are set in the queue command at the end of the file: queue x_low x_high y_low y_high from job_values.txt Take a look at job_values.txt: $ cat job_values.txt -9 9 -9 9 -8 8 -8 8 -7 7 -7 7 -6 6 -6 6 -5 5 -5 5 -4 4 -4 4 -3 3 -3 3 -2 2 -2 2 -1 1 -1 1 The submit file's queue statement will read in this file and assign each value in a row to the four variables shown in the queue statement. Each row corresponds to the submission of a unique job with those four values. Let us submit the above job to see this: $ condor_submit ScalingUp-PythonCals.submit Submitting job(s).......... 9 job(s) submitted to cluster 329840. Apply your condor_q knowledge to see this job progress. After all jobs finished, execute the post_script.sh script to sort the results. ./post_process.sh","title":"Providing Different Inputs to Jobs"},{"location":"software_examples_for_osg/python/tutorial-ScalingUp-Python/#another-example-of-different-inputs","text":"In the previous example, we split the input information into four variables that were included in the arguments line. However, we could have set the arguments line directly, without intermediate values. This is shown in Example 3: $ cd ../Example3 $ cat ScalingUp-PythonCals.submit executable = ../scalingup-python-wrapper.sh transfer_input_files = ../rosen_brock_brute_opt.py log = Log/job.$(Cluster).$(Process).log output = Log/job.$(Cluster).$(Process).out error = Log/job.$(Cluster).$(Process).err requirements = OSGVO_OS_STRING =?= \"RHEL 7\" && HAS_MODULES =?= True request_cpus = 1 request_memory = 1 GB request_disk = 1 GB queue arguments from job_values.txt Here, arguments has disappeared from the top of the file because we've included it in the queue statement at the end. The job_values.txt file has the same values as before; in this syntax, HTCondor will submit a job for each row of values and the job's arguments will be those four values. Let us submit the above job $ condor_submit ScalingUp-PythonCals.submit Submitting job(s).......... 9 job(s) submitted to cluster 329839. Apply your condor_q and connect watch knowledge to see this job progress. After all jobs finished, execute the post_script.sh script to sort the results. ./post_process.sh","title":"Another Example of Different Inputs"},{"location":"software_examples_for_osg/python/tutorial-wordfreq/","text":"Wordcount Tutorial for Submitting Multiple Jobs \u00b6 Imagine you have a collection of books, and you want to analyze how word usage varies from book to book or author to author. Analyzing One Book \u00b6 Test the Command \u00b6 We can analyze one book by running the wordcount.py script, with the name of the book we want to analyze: $ ./wordcount.py Alice_in_Wonderland.txt If you run the ls command, you should see a new file with the prefix counts which has the results of this python script. This is the output we want to produce within an HTCondor job. For now, remove the output: $ rm counts.Alice_in_Wonderland.tsv Create a Submit File \u00b6 To submit a single job that runs this command and analyzes the Alice's Adventures in Wonderland book, we need to translate this command into HTCondor submit file syntax. The two main components we care about are (1) the actual command and (2) the needed input files. The command gets turned into the submit file executable and arguments options: executable = wordcount.py arguments = Alice_in_Wonderland.txt The input file for this job is the Alice_in_Wonderland.txt text file. We include that in the following submit file option: transfer_input_files = Alice_in_Wonderland.txt There are other submit file options that control other aspects of the job, like where to save error and logging information, and how many resources to request per job. This tutorial has a sample submit file ( wordcount.sub ) with most of these submit file options filled in: $ cat wordcount.sub executable = arguments = transfer_input_files = should_transfer_files = Yes when_to_transfer_output = ON_EXIT output = logs/job.$(Cluster).$(Process).out error = logs/job.$(Cluster).$(Process).error log = logs/job.$(Cluster).$(Process).log requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 512MB request_disk = 512MB queue 1 Open (or create) this file with a terminal-based text editor (like vi or nano ) and add the executable, arguments, and input information described above. Submit and Monitor the Job \u00b6 After saving the submit file, submit the job: $ condor_submit wordcount.submit You can check the job's progress using condor_q . Once it finishes, you should see the same counts.Alice_in_Wonderland.tsv output. Analyzing Multiple Books \u00b6 Now suppose you wanted to analyze multiple books - more than one at a time. You could create a separate submit file for each book, and submit all of the files manually, but you'd have a lot of file lines to modify each time (in particular, the \"arguments\" and \"transfer_input_files\" line from the previous submit file). This would be overly verbose and tedious. HTCondor has options that make it easy to submit many jobs from one submit file. Make a List of Inputs \u00b6 First we want to make a list of inputs that we want to use for our jobs. This should be a list where each item on the list corresponds to a job. In this example, our inputs are the different text files for different books. We want each job to analyze a different book, so our list should just contain the names of these text files. We can easily create this list by using an ls command and sending the output to a file: $ ls *.txt > book.list Modify the Submit File \u00b6 Next, we will make changes to our submit file so that it submits a job for each book title in our list (seen in the book.list file). Create a copy of our existing submit file, that we can use for this job submission. $ cp wordcount.sub wordcount-many.sub Then, open the file with a text editor and go to the end. We want to tell the queue keyword to use our list of inputs to submit jobs. The default syntax looks like this: queue from Therefore, when we modify this syntax to fit our example, we get: queue book from book.list This statement works a little bit like a for loop. For every item in the book.list file, HTCondor will create a job. Each item can be referenced elsewhere in the submit file using the book variable name. Therefore, every time we used the name of the book in our submit file (in the previous example, everywhere you see \"Alice_in_Wonderland.txt\") should be replaced with a variable. HTCondor's variable syntax looks like this: $(variablename) So the following lines in the submit file should be changed to use the variable $(book) : arguments = $(book) transfer_input_files = $(book) Submit and Monitor the Job \u00b6 We're now ready to submit all of our jobs. $ condor_submit wordcount-many.submit This will now submit five jobs (one for each book on our list). Once all five have finished running, we should see \"counts\" files for each book in the directory.","title":"Wordcount Tutorial for Submitting Multiple Jobs"},{"location":"software_examples_for_osg/python/tutorial-wordfreq/#wordcount-tutorial-for-submitting-multiple-jobs","text":"Imagine you have a collection of books, and you want to analyze how word usage varies from book to book or author to author.","title":"Wordcount Tutorial for Submitting Multiple Jobs"},{"location":"software_examples_for_osg/python/tutorial-wordfreq/#analyzing-one-book","text":"","title":"Analyzing One Book"},{"location":"software_examples_for_osg/python/tutorial-wordfreq/#test-the-command","text":"We can analyze one book by running the wordcount.py script, with the name of the book we want to analyze: $ ./wordcount.py Alice_in_Wonderland.txt If you run the ls command, you should see a new file with the prefix counts which has the results of this python script. This is the output we want to produce within an HTCondor job. For now, remove the output: $ rm counts.Alice_in_Wonderland.tsv","title":"Test the Command"},{"location":"software_examples_for_osg/python/tutorial-wordfreq/#create-a-submit-file","text":"To submit a single job that runs this command and analyzes the Alice's Adventures in Wonderland book, we need to translate this command into HTCondor submit file syntax. The two main components we care about are (1) the actual command and (2) the needed input files. The command gets turned into the submit file executable and arguments options: executable = wordcount.py arguments = Alice_in_Wonderland.txt The input file for this job is the Alice_in_Wonderland.txt text file. We include that in the following submit file option: transfer_input_files = Alice_in_Wonderland.txt There are other submit file options that control other aspects of the job, like where to save error and logging information, and how many resources to request per job. This tutorial has a sample submit file ( wordcount.sub ) with most of these submit file options filled in: $ cat wordcount.sub executable = arguments = transfer_input_files = should_transfer_files = Yes when_to_transfer_output = ON_EXIT output = logs/job.$(Cluster).$(Process).out error = logs/job.$(Cluster).$(Process).error log = logs/job.$(Cluster).$(Process).log requirements = (OSGVO_OS_STRING == \"RHEL 7\") request_cpus = 1 request_memory = 512MB request_disk = 512MB queue 1 Open (or create) this file with a terminal-based text editor (like vi or nano ) and add the executable, arguments, and input information described above.","title":"Create a Submit File"},{"location":"software_examples_for_osg/python/tutorial-wordfreq/#submit-and-monitor-the-job","text":"After saving the submit file, submit the job: $ condor_submit wordcount.submit You can check the job's progress using condor_q . Once it finishes, you should see the same counts.Alice_in_Wonderland.tsv output.","title":"Submit and Monitor the Job"},{"location":"software_examples_for_osg/python/tutorial-wordfreq/#analyzing-multiple-books","text":"Now suppose you wanted to analyze multiple books - more than one at a time. You could create a separate submit file for each book, and submit all of the files manually, but you'd have a lot of file lines to modify each time (in particular, the \"arguments\" and \"transfer_input_files\" line from the previous submit file). This would be overly verbose and tedious. HTCondor has options that make it easy to submit many jobs from one submit file.","title":"Analyzing Multiple Books"},{"location":"software_examples_for_osg/python/tutorial-wordfreq/#make-a-list-of-inputs","text":"First we want to make a list of inputs that we want to use for our jobs. This should be a list where each item on the list corresponds to a job. In this example, our inputs are the different text files for different books. We want each job to analyze a different book, so our list should just contain the names of these text files. We can easily create this list by using an ls command and sending the output to a file: $ ls *.txt > book.list","title":"Make a List of Inputs"},{"location":"software_examples_for_osg/python/tutorial-wordfreq/#modify-the-submit-file","text":"Next, we will make changes to our submit file so that it submits a job for each book title in our list (seen in the book.list file). Create a copy of our existing submit file, that we can use for this job submission. $ cp wordcount.sub wordcount-many.sub Then, open the file with a text editor and go to the end. We want to tell the queue keyword to use our list of inputs to submit jobs. The default syntax looks like this: queue from Therefore, when we modify this syntax to fit our example, we get: queue book from book.list This statement works a little bit like a for loop. For every item in the book.list file, HTCondor will create a job. Each item can be referenced elsewhere in the submit file using the book variable name. Therefore, every time we used the name of the book in our submit file (in the previous example, everywhere you see \"Alice_in_Wonderland.txt\") should be replaced with a variable. HTCondor's variable syntax looks like this: $(variablename) So the following lines in the submit file should be changed to use the variable $(book) : arguments = $(book) transfer_input_files = $(book)","title":"Modify the Submit File"},{"location":"software_examples_for_osg/python/tutorial-wordfreq/#submit-and-monitor-the-job_1","text":"We're now ready to submit all of our jobs. $ condor_submit wordcount-many.submit This will now submit five jobs (one for each book on our list). Once all five have finished running, we should see \"counts\" files for each book in the directory.","title":"Submit and Monitor the Job"},{"location":"software_examples_for_osg/r/tutorial-R/","text":"Overview Run R scripts on OSG Access R on the submit host Other Supported R Versions Run R code Build the HTCondor job Submit and analyze the output Next Steps Getting Help Overview \u00b6 This tutorial describes how to run R scripts on the OSPool. We'll first run the program locally as a test. After that we'll create a submit file, submit it to the OSPool using OSG Connect, and look at the results when the jobs finish. Run R scripts on OSG \u00b6 Access R on the submit host \u00b6 First we'll need to create a working directory, you can either run $ tutorial R OR type the following: $ mkdir tutorial-R; cd tutorial-R R is run using containers on the OSG. To test it out on the submit node, we can run: $ singularity shell \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 Other Supported R Versions \u00b6 To see a list of all Singularity containers containing R, look at the list of OSPool Supported Containers The previous command sometimes takes a minute or so to start. Once it starts, you should see the following prompt: Singularity osgvo-r:3.5.0:~> Now, we can try to run R by typing R in our terminal: $ Singularity osgvo-r:3.5.0:~> R R version 3.5.1 (2018-07-02) -- \"Feather Spray\" Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > You can quit out with q() . > q() Save workspace image? [y/n/c]: n Singularity osgvo-r:3.5.0:~> Great! R works. We'll leave the container running for the next step. See below on how to exit from the container. Run R code \u00b6 Now that we can run R, let's create a small script. Create the file hello_world.R using a text editor like nano or vim that contains the following: print(\"Hello World!\") We will then exit the text editing program and run this script. To run this script, we will use the Rscript command (equivalent to R CMD BATCH ) which accepts the script as command line argument. This approach makes R much less verbose, and it's easier to parse the output later. The command in our script will look like this: Singularity osgvo-r:3.5.0:~> Rscript hello_world.R If this works, we will have [1] \"Hello World!\" printed to our terminal. Once we have this output, we'll exit the container for now with exit : Singularity osgvo-r:3.5.0:~> exit $ Build the HTCondor job \u00b6 To prepare our R job to run on the OSPool, we need to create a wrapper for our R environment, based on the setup we did in previous sections. Create the file R-wrapper.sh with this text inside the file: 1 2 3 4 5 6 7 #!/bin/bash # set TMPDIR variable mkdir rtmp export TMPDIR = $_CONDOR_SCRATCH_DIR /rtmp Rscript hello_world.R Once done, exit the file. We will now change the permissions on the wrapper script: $ chmod +x R-wrapper.sh Now that we've created a wrapper, let's build a HTCondor submit file around it. Using a text editor, create a file called R.submit with the following text inside it: universe = vanilla log = R.log.$(Cluster).$(Process) error = R.err.$(Cluster).$(Process) output = R.out.$(Cluster).$(Process) +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0\" executable = R-wrapper.sh transfer_input_files = hello_world.R request_cpus = 1 request_memory = 1GB request_disk = 1GB queue 1 The path you put in the +SingularityImage option should match whatever you used to test R above. The R.submit file may have included a few lines that you are unfamiliar with. For example, $(Cluster) and $(Process) are variables that will be replaced with the job's cluster and process id. This is useful when you have many jobs submitted in the same file. Any output and errors will be placed in a separate file for each job. Also, did you see the transfer_input_files line? This tells HTCondor what files to transfer with the job to the worker node. You don't have to tell it to transfer the executable, HTCondor is smart enough to know that the job will need that. But any extra files, such as our R script file, will need to be explicitly listed to be transferred with the job. You can use transfer_input_files for input data to the job, as shown in Transferring data with HTCondor . Submit and analyze the output \u00b6 Finally, submit the job! $ condor_submit R.submit Submitting job(s).......... 1 job(s) submitted to cluster 3796250. $ condor_q user $ condor_q -- Schedd: login03.osgconnect.net : <192.170.227.22:9618?... @ 05/13/19 09:51:04 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS user ID: 3796250 5/13 09:50 _ _ 1 1 3796250.0 ... You can follow the status of your job cluster with the connect watch command, which shows condor_q output that refreshes each 5 seconds. Press control-C to stop watching. Since our jobs prints to standard out, we can check the output files. Let's see what one looks like: $ cat R.out.3796250.0 [1] \"Hello World!\" Next Steps \u00b6 Use Custom Libraries with R Scale Up your R jobs We recommend you read about how to steer your jobs with HTCondor job requirements - this will allow you to select good resources for your workload. Please see this page Getting Help \u00b6 For assistance or questions, please email the OSG User Support team at support@opensciencegrid.org .","title":"Run R scripts on the OSPool"},{"location":"software_examples_for_osg/r/tutorial-R/#overview","text":"This tutorial describes how to run R scripts on the OSPool. We'll first run the program locally as a test. After that we'll create a submit file, submit it to the OSPool using OSG Connect, and look at the results when the jobs finish.","title":"Overview"},{"location":"software_examples_for_osg/r/tutorial-R/#run-r-scripts-on-osg","text":"","title":"Run R scripts on OSG"},{"location":"software_examples_for_osg/r/tutorial-R/#access-r-on-the-submit-host","text":"First we'll need to create a working directory, you can either run $ tutorial R OR type the following: $ mkdir tutorial-R; cd tutorial-R R is run using containers on the OSG. To test it out on the submit node, we can run: $ singularity shell \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0","title":"Access R on the submit host"},{"location":"software_examples_for_osg/r/tutorial-R/#other-supported-r-versions","text":"To see a list of all Singularity containers containing R, look at the list of OSPool Supported Containers The previous command sometimes takes a minute or so to start. Once it starts, you should see the following prompt: Singularity osgvo-r:3.5.0:~> Now, we can try to run R by typing R in our terminal: $ Singularity osgvo-r:3.5.0:~> R R version 3.5.1 (2018-07-02) -- \"Feather Spray\" Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > You can quit out with q() . > q() Save workspace image? [y/n/c]: n Singularity osgvo-r:3.5.0:~> Great! R works. We'll leave the container running for the next step. See below on how to exit from the container.","title":"Other Supported R Versions"},{"location":"software_examples_for_osg/r/tutorial-R/#run-r-code","text":"Now that we can run R, let's create a small script. Create the file hello_world.R using a text editor like nano or vim that contains the following: print(\"Hello World!\") We will then exit the text editing program and run this script. To run this script, we will use the Rscript command (equivalent to R CMD BATCH ) which accepts the script as command line argument. This approach makes R much less verbose, and it's easier to parse the output later. The command in our script will look like this: Singularity osgvo-r:3.5.0:~> Rscript hello_world.R If this works, we will have [1] \"Hello World!\" printed to our terminal. Once we have this output, we'll exit the container for now with exit : Singularity osgvo-r:3.5.0:~> exit $","title":"Run R code"},{"location":"software_examples_for_osg/r/tutorial-R/#build-the-htcondor-job","text":"To prepare our R job to run on the OSPool, we need to create a wrapper for our R environment, based on the setup we did in previous sections. Create the file R-wrapper.sh with this text inside the file: 1 2 3 4 5 6 7 #!/bin/bash # set TMPDIR variable mkdir rtmp export TMPDIR = $_CONDOR_SCRATCH_DIR /rtmp Rscript hello_world.R Once done, exit the file. We will now change the permissions on the wrapper script: $ chmod +x R-wrapper.sh Now that we've created a wrapper, let's build a HTCondor submit file around it. Using a text editor, create a file called R.submit with the following text inside it: universe = vanilla log = R.log.$(Cluster).$(Process) error = R.err.$(Cluster).$(Process) output = R.out.$(Cluster).$(Process) +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0\" executable = R-wrapper.sh transfer_input_files = hello_world.R request_cpus = 1 request_memory = 1GB request_disk = 1GB queue 1 The path you put in the +SingularityImage option should match whatever you used to test R above. The R.submit file may have included a few lines that you are unfamiliar with. For example, $(Cluster) and $(Process) are variables that will be replaced with the job's cluster and process id. This is useful when you have many jobs submitted in the same file. Any output and errors will be placed in a separate file for each job. Also, did you see the transfer_input_files line? This tells HTCondor what files to transfer with the job to the worker node. You don't have to tell it to transfer the executable, HTCondor is smart enough to know that the job will need that. But any extra files, such as our R script file, will need to be explicitly listed to be transferred with the job. You can use transfer_input_files for input data to the job, as shown in Transferring data with HTCondor .","title":"Build the HTCondor job"},{"location":"software_examples_for_osg/r/tutorial-R/#submit-and-analyze-the-output","text":"Finally, submit the job! $ condor_submit R.submit Submitting job(s).......... 1 job(s) submitted to cluster 3796250. $ condor_q user $ condor_q -- Schedd: login03.osgconnect.net : <192.170.227.22:9618?... @ 05/13/19 09:51:04 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS user ID: 3796250 5/13 09:50 _ _ 1 1 3796250.0 ... You can follow the status of your job cluster with the connect watch command, which shows condor_q output that refreshes each 5 seconds. Press control-C to stop watching. Since our jobs prints to standard out, we can check the output files. Let's see what one looks like: $ cat R.out.3796250.0 [1] \"Hello World!\"","title":"Submit and analyze the output"},{"location":"software_examples_for_osg/r/tutorial-R/#next-steps","text":"Use Custom Libraries with R Scale Up your R jobs We recommend you read about how to steer your jobs with HTCondor job requirements - this will allow you to select good resources for your workload. Please see this page","title":"Next Steps"},{"location":"software_examples_for_osg/r/tutorial-R/#getting-help","text":"For assistance or questions, please email the OSG User Support team at support@opensciencegrid.org .","title":"Getting Help"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/","text":"Overview Background Use custom R libraries on OSG Setup Workflow Files Create a Directory for Packages Start an R Container and Install Packages Other Supported R Versions Turn Package Directory Into a tar.gz File Use Packages in an R Script Define Packages in the Executable Include Packages in the Submit File Submit Jobs and Review Output Variations on This Process Install multiple packages at once Getting Help Overview \u00b6 This tutorial describes how to create custom R libraries for use in jobs on OSG Connect. Background \u00b6 The material in this tutorial builds upon the Run R Scripts on OSG tutorial. If you are not already familiar with how to run R jobs on OSG Connect, please see that tutorial first. Use custom R libraries on OSG \u00b6 Often we may need to add R external libraries that are not part of the base R installation. As a user, we could add the libraries in our home (or stash) directory and then compress the library to make them available on remote machines for job executions. Setup Workflow Files \u00b6 First we'll need to create a working directory, you can either run $ tutorial R-addlib or type the following: $ mkdir tutorial-R-addlib $ cd tutorial-R-addlib In the previous tutorial, recall that we created the script hello_world.R that contained the following: print(\"Hello World!\") We also created a wrapper script called R-wrapper.sh to execute our R script. The contents of that file is shown below: 1 2 3 4 5 6 7 #!/bin/bash # create a tmp directory mkdir rtmp export TMPDIR = $_CONDOR_SCRATCH_DIR /rtmp Rscript hello_world.R Finally, we had the R.submit submit script which we used to submit the job to OSG Connect: universe = vanilla log = R.log.$(Cluster).$(Process) error = R.err.$(Cluster).$(Process) output = R.out.$(Cluster).$(Process) +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0\" executable = R-wrapper.sh transfer_input_files = hello_world.R request_cpus = 1 request_memory = 1GB request_disk = 1GB queue 1 Create a Directory for Packages \u00b6 It is helpful to create a dedicated directory to install the package into. This will facilitate zipping the library so it can be transported with the job. Say, you decided to build the library in R-packages in the current folder. If it does not already exist, make the necessary directory by typing the following in your shell prompt: $ mkdir -p R-packages Start an R Container and Install Packages \u00b6 Start an R container by running: $ singularity shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 Other Supported R Versions \u00b6 To see a list of all Singularity containers containing R, look at the list of OSPool Supported Containers Before starting to run R, set the R_LIBS environment variable so R knows where to find our custom library directory: $ export R_LIBS=$PWD/R-packages We also need to set the TMPDIR variable so that R has a place to download any intermediate or temporary package files. $ export TMPDIR=$PWD Now we can run R and check that our library location is being used (here the > is the R-prompt): Singularity osgvo-r:3.5.0:~> R > .libPaths() [1] \"/home/alice/tutorial-R-addlib/R-packages\" [2] \"/usr/lib64/R/library\" [3] \"/usr/share/R/library\" We should be able to see our R-packages path in [1] . We can also check for available libraries within R. > library() Press q to close that display. To install packages within R, we use the command (where \u201cXYZ\u201d is the name of the target package): > install.packages(\"XYZ\", repos = \"http://cloud.r-project.org/\", dependencies = TRUE) For this tutorial, we are going to use the lubridate package. To install lubridate, enter this command: > install.packages(\"cowsay\", repos=\"http://cloud.r-project.org/\") Turn Package Directory Into a tar.gz File \u00b6 Proceeding with the cowsay package, the next step is create a tarball of the package so we can send the tarball along with the job. Exit from the R prompt by typing: > quit() or: >q() In either case, be sure to say n when prompted to Save workspace image? [y/n/c]: . And then exit out of the container by typing \"exit\": Singularity osgvo-r:3.5.0:~> exit $ To tar the package directory, type the following at the shell prompt: $ tar -czf R-packages.tar.gz R-packages/ Use Packages in an R Script \u00b6 Now, let's change the hello_world job to use the new package. First, modify the hello_world.R R script by adding and changing the following lines: library(cowsay) say(\"Hello World!\", \"cow\") Define Packages in the Executable \u00b6 R library locations are set upon launch and can be modified using the R_LIBS environmental variable. To set this correctly, we need to modify the wrapper script. Change the file R-wrapper.sh so it matches the following: 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash # Uncompress the tarball tar -xzf R-packages.tar.gz # Set the library location export R_LIBS = \" $PWD /R-packages\" # set TMPDIR variable export TMPDIR = $_CONDOR_SCRATCH_DIR # run the R program Rscript hello_world.R Include Packages in the Submit File \u00b6 Next, we need to modify the submit script so that the package tarball is transferred correctly with the job. Change the submit script R.submit so that transfer_input_files and arguments are set correctly. The completed file, which can bee seen in R.submit should look like below: universe = vanilla log = R.log.$(Cluster).$(Process) error = R.err.$(Cluster).$(Process) output = R.out.$(Cluster).$(Process) +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0\" executable = R-wrapper.sh transfer_input_files = R-packages.tar.gz, hello_world.R request_cpus = 1 request_memory = 1GB request_disk = 1GB queue 1 Submit Jobs and Review Output \u00b6 Now we are ready to submit the job: $ condor_submit R.submit and check the job status: $ condor_q Once the job finished running, check the output files as before. They should now look like this: $ cat R.out.0000.0 ----- Hello World! ------ \\ ^__^ \\ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || || Variations on This Process \u00b6 Install multiple packages at once \u00b6 If you have multiple packages to be added, it may be better to list each of the install.packages() commands within a separate R script and source the file to R. For example, if we needed to install ggplot2 , dplyr , and tidyr , we can list them to be installed in a script called setup_packages.R which would contain the following: install.packages(\"ggplot2\", repos=\"http://cloud.r-project.org/\", dependencies = TRUE) install.packages(\"dplyr\", repos=\"http://cloud.r-project.org/\", dependencies = TRUE) install.packages(\"tidyr\", repos=\"http://cloud.r-project.org/\", dependencies = TRUE) Then, install all of the packages by running the setup file within R: > source(`setup_packages.R`) Getting Help \u00b6 For assistance or questions, please email the OSG User Support team at support@opensciencegrid.org .","title":"Use External Packages in your R Jobs"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#overview","text":"This tutorial describes how to create custom R libraries for use in jobs on OSG Connect.","title":"Overview"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#background","text":"The material in this tutorial builds upon the Run R Scripts on OSG tutorial. If you are not already familiar with how to run R jobs on OSG Connect, please see that tutorial first.","title":"Background"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#use-custom-r-libraries-on-osg","text":"Often we may need to add R external libraries that are not part of the base R installation. As a user, we could add the libraries in our home (or stash) directory and then compress the library to make them available on remote machines for job executions.","title":"Use custom R libraries on OSG"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#setup-workflow-files","text":"First we'll need to create a working directory, you can either run $ tutorial R-addlib or type the following: $ mkdir tutorial-R-addlib $ cd tutorial-R-addlib In the previous tutorial, recall that we created the script hello_world.R that contained the following: print(\"Hello World!\") We also created a wrapper script called R-wrapper.sh to execute our R script. The contents of that file is shown below: 1 2 3 4 5 6 7 #!/bin/bash # create a tmp directory mkdir rtmp export TMPDIR = $_CONDOR_SCRATCH_DIR /rtmp Rscript hello_world.R Finally, we had the R.submit submit script which we used to submit the job to OSG Connect: universe = vanilla log = R.log.$(Cluster).$(Process) error = R.err.$(Cluster).$(Process) output = R.out.$(Cluster).$(Process) +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0\" executable = R-wrapper.sh transfer_input_files = hello_world.R request_cpus = 1 request_memory = 1GB request_disk = 1GB queue 1","title":"Setup Workflow Files"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#create-a-directory-for-packages","text":"It is helpful to create a dedicated directory to install the package into. This will facilitate zipping the library so it can be transported with the job. Say, you decided to build the library in R-packages in the current folder. If it does not already exist, make the necessary directory by typing the following in your shell prompt: $ mkdir -p R-packages","title":"Create a Directory for Packages"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#start-an-r-container-and-install-packages","text":"Start an R container by running: $ singularity shell /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0","title":"Start an R Container and Install Packages"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#other-supported-r-versions","text":"To see a list of all Singularity containers containing R, look at the list of OSPool Supported Containers Before starting to run R, set the R_LIBS environment variable so R knows where to find our custom library directory: $ export R_LIBS=$PWD/R-packages We also need to set the TMPDIR variable so that R has a place to download any intermediate or temporary package files. $ export TMPDIR=$PWD Now we can run R and check that our library location is being used (here the > is the R-prompt): Singularity osgvo-r:3.5.0:~> R > .libPaths() [1] \"/home/alice/tutorial-R-addlib/R-packages\" [2] \"/usr/lib64/R/library\" [3] \"/usr/share/R/library\" We should be able to see our R-packages path in [1] . We can also check for available libraries within R. > library() Press q to close that display. To install packages within R, we use the command (where \u201cXYZ\u201d is the name of the target package): > install.packages(\"XYZ\", repos = \"http://cloud.r-project.org/\", dependencies = TRUE) For this tutorial, we are going to use the lubridate package. To install lubridate, enter this command: > install.packages(\"cowsay\", repos=\"http://cloud.r-project.org/\")","title":"Other Supported R Versions"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#turn-package-directory-into-a-targz-file","text":"Proceeding with the cowsay package, the next step is create a tarball of the package so we can send the tarball along with the job. Exit from the R prompt by typing: > quit() or: >q() In either case, be sure to say n when prompted to Save workspace image? [y/n/c]: . And then exit out of the container by typing \"exit\": Singularity osgvo-r:3.5.0:~> exit $ To tar the package directory, type the following at the shell prompt: $ tar -czf R-packages.tar.gz R-packages/","title":"Turn Package Directory Into a tar.gz File"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#use-packages-in-an-r-script","text":"Now, let's change the hello_world job to use the new package. First, modify the hello_world.R R script by adding and changing the following lines: library(cowsay) say(\"Hello World!\", \"cow\")","title":"Use Packages in an R Script"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#define-packages-in-the-executable","text":"R library locations are set upon launch and can be modified using the R_LIBS environmental variable. To set this correctly, we need to modify the wrapper script. Change the file R-wrapper.sh so it matches the following: 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash # Uncompress the tarball tar -xzf R-packages.tar.gz # Set the library location export R_LIBS = \" $PWD /R-packages\" # set TMPDIR variable export TMPDIR = $_CONDOR_SCRATCH_DIR # run the R program Rscript hello_world.R","title":"Define Packages in the Executable"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#include-packages-in-the-submit-file","text":"Next, we need to modify the submit script so that the package tarball is transferred correctly with the job. Change the submit script R.submit so that transfer_input_files and arguments are set correctly. The completed file, which can bee seen in R.submit should look like below: universe = vanilla log = R.log.$(Cluster).$(Process) error = R.err.$(Cluster).$(Process) output = R.out.$(Cluster).$(Process) +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0\" executable = R-wrapper.sh transfer_input_files = R-packages.tar.gz, hello_world.R request_cpus = 1 request_memory = 1GB request_disk = 1GB queue 1","title":"Include Packages in the Submit File"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#submit-jobs-and-review-output","text":"Now we are ready to submit the job: $ condor_submit R.submit and check the job status: $ condor_q Once the job finished running, check the output files as before. They should now look like this: $ cat R.out.0000.0 ----- Hello World! ------ \\ ^__^ \\ (oo)\\ ________ (__)\\ )\\ /\\ ||------w| || ||","title":"Submit Jobs and Review Output"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#variations-on-this-process","text":"","title":"Variations on This Process"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#install-multiple-packages-at-once","text":"If you have multiple packages to be added, it may be better to list each of the install.packages() commands within a separate R script and source the file to R. For example, if we needed to install ggplot2 , dplyr , and tidyr , we can list them to be installed in a script called setup_packages.R which would contain the following: install.packages(\"ggplot2\", repos=\"http://cloud.r-project.org/\", dependencies = TRUE) install.packages(\"dplyr\", repos=\"http://cloud.r-project.org/\", dependencies = TRUE) install.packages(\"tidyr\", repos=\"http://cloud.r-project.org/\", dependencies = TRUE) Then, install all of the packages by running the setup file within R: > source(`setup_packages.R`)","title":"Install multiple packages at once"},{"location":"software_examples_for_osg/r/tutorial-R-addlibSNA/#getting-help","text":"For assistance or questions, please email the OSG User Support team at support@opensciencegrid.org .","title":"Getting Help"},{"location":"software_examples_for_osg/r/tutorial-ScalingUp-R/","text":"Overview Background Set up an R Job Create and test an R Script Create an Executable Create a Submit File and Log Directory Submit the Jobs Post Process\u22c5 Key Points Getting Help Overview \u00b6 Scaling up the computational resources is a big advantage for doing certain large scale calculations on OSG. Consider the extensive sampling for a multi-dimensional Monte Carlo integration or molecular dynamics simulation with several initial conditions. These type of calculations require submitting lot of jobs. In the previous example, we submitted the job to a single worker machine. About a million CPU hours per day are available to OSG users on an opportunistic basis. Learning how to scale up and control large numbers of jobs to realizing the full potential of distributed high throughput computing on the OSG. In this section, we will see how to scale up the calculations with simple example. Once we understand the basic HTCondor script, it is easy to scale up. Background \u00b6 For this example, we will use computational methods to estimate pi. First, we will define a square inscribed by a unit circle from which we will randomly sample points. The ratio of the points outside the circle to the points in the circle is calculated which approaches pi/4. This method converges extremely slowly, which makes it great for a CPU-intensive exercise (but bad for a real estimation!). Set up an R Job \u00b6 First, we'll need to create a working directory, you can either run $ tutorial ScalingUp-R or type the following: $ mkdir tutorial-ScalingUp-R $ cd tutorial-ScalingUp-R Create and test an R Script \u00b6 Create an R script by typing the following into a file called mcpi.R : args = commandArgs(trailingOnly = TRUE) iternum = as.numeric(args[[1]]) + 100 montecarloPi <- function(trials) { count = 0 for(i in 1:trials) { if((runif(1,0,1)^2 + runif(1,0,1)^2)<1) { count = count + 1 } } return((count*4)/trials) } montecarloPi(iternum) If you want to test the script, start an R container, and then run the script using Rscript : $ singularity shell \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 Singularity osgvo-r:3.5.0:~> Rscript mcpi.R 10 [1] 3.14 Singularity osgvo-r:3.5.0:~> exit $ If we were running a more intensive script, we would want to test our pipeline with a shortened, test script first. Create an Executable \u00b6 As discussed in the Run R Jobs tutorial , we need to prepare the job execution and the job submission scripts. First, make a wrapper script called R-wrapper.sh . 1 2 3 4 5 6 #!/bin/bash # set TMPDIR variable export TMPDIR = $_CONDOR_SCRATCH_DIR Rscript mcpi.R This script will set the location for temporary files and execute our R script. Test the wrapper script to ensure it works: Create a Submit File and Log Directory \u00b6 Now that we have both our R script and wrapper script written and tested, we can begin building the submit file for our job. If we want to submit several jobs, we need to track log, out and error files for each job. An easy way to do this is to use the Cluster and Process ID values to create unique files for each process in our job. Create a submit file named R.submit : +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0\" executable = R-wrapper.sh arguments = $(Process) transfer_input_files = mcpi.R log = log/job.log.$(Cluster).$(Process) error = log/job.error.$(Cluster).$(Process) output = log/mcpi.out.$(Cluster).$(Process) queue 100 Note the queue 100 . This tells Condor to enqueue 100 copies of this job as one cluster. Also, notice the use of $(Cluster) and $(Process) to specify unique output files. HTCondor will replace these with the Cluster and Process ID numbers for each individual process within the cluster. Let's make the log directory that will hold these files for us. $ mkdir log Submit the Jobs \u00b6 Now it is time to submit our job! You'll see something like the following upon submission: $ condor_submit R.submit Submitting job(s)......................... 100 job(s) submitted to cluster 837. Apply your condor_q knowledge to see this job progress. Check your log folder to see the individual output files. Post Process\u22c5 \u00b6 Once the jobs are completed, you can use the information in the output files to calculate an average of all of our computed estimates of Pi. To see this, we can use the command: $ cat log/mcpi*.out* | awk '{ sum += $2; print $2\" \"NR} END { print \"---------------\\n Grand Average = \" sum/NR }' Key Points \u00b6 Scaling up the computational resources on OSG is crucial to taking full advantage of grid computing. Changing the value of Queue allows the user to scale up the resources. Arguments allows you to pass parameters to a job script. $(Cluster) and $(Process) can be used to name log files uniquely. Getting Help \u00b6 For assistance or questions, please email the OSG User Support team at support@opensciencegrid.org .","title":"Scaling up compute resources"},{"location":"software_examples_for_osg/r/tutorial-ScalingUp-R/#overview","text":"Scaling up the computational resources is a big advantage for doing certain large scale calculations on OSG. Consider the extensive sampling for a multi-dimensional Monte Carlo integration or molecular dynamics simulation with several initial conditions. These type of calculations require submitting lot of jobs. In the previous example, we submitted the job to a single worker machine. About a million CPU hours per day are available to OSG users on an opportunistic basis. Learning how to scale up and control large numbers of jobs to realizing the full potential of distributed high throughput computing on the OSG. In this section, we will see how to scale up the calculations with simple example. Once we understand the basic HTCondor script, it is easy to scale up.","title":"Overview"},{"location":"software_examples_for_osg/r/tutorial-ScalingUp-R/#background","text":"For this example, we will use computational methods to estimate pi. First, we will define a square inscribed by a unit circle from which we will randomly sample points. The ratio of the points outside the circle to the points in the circle is calculated which approaches pi/4. This method converges extremely slowly, which makes it great for a CPU-intensive exercise (but bad for a real estimation!).","title":"Background"},{"location":"software_examples_for_osg/r/tutorial-ScalingUp-R/#set-up-an-r-job","text":"First, we'll need to create a working directory, you can either run $ tutorial ScalingUp-R or type the following: $ mkdir tutorial-ScalingUp-R $ cd tutorial-ScalingUp-R","title":"Set up an R Job"},{"location":"software_examples_for_osg/r/tutorial-ScalingUp-R/#create-and-test-an-r-script","text":"Create an R script by typing the following into a file called mcpi.R : args = commandArgs(trailingOnly = TRUE) iternum = as.numeric(args[[1]]) + 100 montecarloPi <- function(trials) { count = 0 for(i in 1:trials) { if((runif(1,0,1)^2 + runif(1,0,1)^2)<1) { count = count + 1 } } return((count*4)/trials) } montecarloPi(iternum) If you want to test the script, start an R container, and then run the script using Rscript : $ singularity shell \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0 Singularity osgvo-r:3.5.0:~> Rscript mcpi.R 10 [1] 3.14 Singularity osgvo-r:3.5.0:~> exit $ If we were running a more intensive script, we would want to test our pipeline with a shortened, test script first.","title":"Create and test an R Script"},{"location":"software_examples_for_osg/r/tutorial-ScalingUp-R/#create-an-executable","text":"As discussed in the Run R Jobs tutorial , we need to prepare the job execution and the job submission scripts. First, make a wrapper script called R-wrapper.sh . 1 2 3 4 5 6 #!/bin/bash # set TMPDIR variable export TMPDIR = $_CONDOR_SCRATCH_DIR Rscript mcpi.R This script will set the location for temporary files and execute our R script. Test the wrapper script to ensure it works:","title":"Create an Executable"},{"location":"software_examples_for_osg/r/tutorial-ScalingUp-R/#create-a-submit-file-and-log-directory","text":"Now that we have both our R script and wrapper script written and tested, we can begin building the submit file for our job. If we want to submit several jobs, we need to track log, out and error files for each job. An easy way to do this is to use the Cluster and Process ID values to create unique files for each process in our job. Create a submit file named R.submit : +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-r:3.5.0\" executable = R-wrapper.sh arguments = $(Process) transfer_input_files = mcpi.R log = log/job.log.$(Cluster).$(Process) error = log/job.error.$(Cluster).$(Process) output = log/mcpi.out.$(Cluster).$(Process) queue 100 Note the queue 100 . This tells Condor to enqueue 100 copies of this job as one cluster. Also, notice the use of $(Cluster) and $(Process) to specify unique output files. HTCondor will replace these with the Cluster and Process ID numbers for each individual process within the cluster. Let's make the log directory that will hold these files for us. $ mkdir log","title":"Create a Submit File and Log Directory"},{"location":"software_examples_for_osg/r/tutorial-ScalingUp-R/#submit-the-jobs","text":"Now it is time to submit our job! You'll see something like the following upon submission: $ condor_submit R.submit Submitting job(s)......................... 100 job(s) submitted to cluster 837. Apply your condor_q knowledge to see this job progress. Check your log folder to see the individual output files.","title":"Submit the Jobs"},{"location":"software_examples_for_osg/r/tutorial-ScalingUp-R/#post-process","text":"Once the jobs are completed, you can use the information in the output files to calculate an average of all of our computed estimates of Pi. To see this, we can use the command: $ cat log/mcpi*.out* | awk '{ sum += $2; print $2\" \"NR} END { print \"---------------\\n Grand Average = \" sum/NR }'","title":"Post Process\u22c5"},{"location":"software_examples_for_osg/r/tutorial-ScalingUp-R/#key-points","text":"Scaling up the computational resources on OSG is crucial to taking full advantage of grid computing. Changing the value of Queue allows the user to scale up the resources. Arguments allows you to pass parameters to a job script. $(Cluster) and $(Process) can be used to name log files uniquely.","title":"Key Points"},{"location":"software_examples_for_osg/r/tutorial-ScalingUp-R/#getting-help","text":"For assistance or questions, please email the OSG User Support team at support@opensciencegrid.org .","title":"Getting Help"},{"location":"support_and_training_resources/education_and_training/osg-user-school/","text":"Annual, Week-Long OSG User School \u00b6 Overview \u00b6 During this week-long training event held at the University of Wisconsin-Madison every summer, students learn to use high-throughput computing (HTC) systems \u2014 at their own campus or using the OSG \u2014 to run large-scale computing applications that are at the heart of today\u2019s cutting-edge science. Through lectures, discussions, and lots of hands-on activities with experienced OSG staff, students will learn how HTC systems work, how to run and manage lots of jobs and huge datasets, to implement a scientific computing workflow, and where to turn for more information and help. The School is ideal for graduate students in any science or research domain where large-scale computing is a vital part of the research process, plus we will consider applications from advanced undergraduates, post-doctoral students, faculty, and staff. Students accepted to this program will receive financial support for basic travel and local costs associated with the School. Open Materials and Recordings \u00b6 The OSG User School want virtual in 2020 and 2021, which means that we were able to record lectures to complement lecture and exercise materials! OSG Virtual School Pilot, August 2021 OSG Virtual School Pilot, July 2020 Past OSG User Schools \u00b6 OSG User School, July 15-19, 2019 OSG User School, July 9-13, 2018 OSG User School, July 17-21, 2017 OSG User School, July 25-29, 2016 OSG User School, July 27-31, 2015 OSG User School, July 7-10, 2014","title":"Annual, Week-Long OSG User School "},{"location":"support_and_training_resources/education_and_training/osg-user-school/#annual-week-long-osg-user-school","text":"","title":"Annual, Week-Long OSG User School"},{"location":"support_and_training_resources/education_and_training/osg-user-school/#overview","text":"During this week-long training event held at the University of Wisconsin-Madison every summer, students learn to use high-throughput computing (HTC) systems \u2014 at their own campus or using the OSG \u2014 to run large-scale computing applications that are at the heart of today\u2019s cutting-edge science. Through lectures, discussions, and lots of hands-on activities with experienced OSG staff, students will learn how HTC systems work, how to run and manage lots of jobs and huge datasets, to implement a scientific computing workflow, and where to turn for more information and help. The School is ideal for graduate students in any science or research domain where large-scale computing is a vital part of the research process, plus we will consider applications from advanced undergraduates, post-doctoral students, faculty, and staff. Students accepted to this program will receive financial support for basic travel and local costs associated with the School.","title":"Overview"},{"location":"support_and_training_resources/education_and_training/osg-user-school/#open-materials-and-recordings","text":"The OSG User School want virtual in 2020 and 2021, which means that we were able to record lectures to complement lecture and exercise materials! OSG Virtual School Pilot, August 2021 OSG Virtual School Pilot, July 2020","title":"Open Materials and Recordings"},{"location":"support_and_training_resources/education_and_training/osg-user-school/#past-osg-user-schools","text":"OSG User School, July 15-19, 2019 OSG User School, July 9-13, 2018 OSG User School, July 17-21, 2017 OSG User School, July 25-29, 2016 OSG User School, July 27-31, 2015 OSG User School, July 7-10, 2014","title":"Past OSG User Schools"},{"location":"support_and_training_resources/education_and_training/osgusertraining/","text":"OSG User Training (regular/monthly) \u00b6 Sign Up for Upcoming Trainings! \u00b6 All User Training sessions are offered from 2:30-4pm ET (and usually on Tuesdays). New User Training is offered monthly, generally on the first Tuesday of the month, and training on various additional topics happens on the third Tuesday of the month. It's best to already have an active account on an OSG Connect login node (or other access point that submits to the Open Science Pool) to follow along with hands-on examples, but anyone can listen in by registering. Tuesday, July 5: New User Training, Register here Tuesday, July 19: Organizing and Submitting HTC Workloads, Register here Tuesday, August 2: New User Training, Register here Materials \u00b6 All of our training materials are public, and with recent video recordings available: New User Training (monthly) \u00b6 The most recent version of our New User Training materials are here: Slides , Video Wordcount Frequency Tutorial Special Topics (at least one per month) \u00b6 As we introduce new training topics, we will add materials to this page. Organizing and Submitting HTC Workloads \u00b6 The most recent version of these training materials are here: Slides Wordcount Frequency Tutorial Using Containerized Software on the Open Science Pool \u00b6 The most recent version of these training materials are here: Slides Software Portability on the Open Science Pool \u00b6 The most recent version of these training materials are here: Slides , List of Commands Tutorials (used in part) Using Julia on the OSPool High Throughput BWA Read Mapping","title":"Bi-Monthly OSG User Training (registration+materials)"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#osg-user-training-regularmonthly","text":"","title":"OSG User Training (regular/monthly)"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#sign-up-for-upcoming-trainings","text":"All User Training sessions are offered from 2:30-4pm ET (and usually on Tuesdays). New User Training is offered monthly, generally on the first Tuesday of the month, and training on various additional topics happens on the third Tuesday of the month. It's best to already have an active account on an OSG Connect login node (or other access point that submits to the Open Science Pool) to follow along with hands-on examples, but anyone can listen in by registering. Tuesday, July 5: New User Training, Register here Tuesday, July 19: Organizing and Submitting HTC Workloads, Register here Tuesday, August 2: New User Training, Register here","title":"Sign Up for Upcoming Trainings!"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#materials","text":"All of our training materials are public, and with recent video recordings available:","title":"Materials"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#new-user-training-monthly","text":"The most recent version of our New User Training materials are here: Slides , Video Wordcount Frequency Tutorial","title":"New User Training (monthly)"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#special-topics-at-least-one-per-month","text":"As we introduce new training topics, we will add materials to this page.","title":"Special Topics (at least one per month)"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#organizing-and-submitting-htc-workloads","text":"The most recent version of these training materials are here: Slides Wordcount Frequency Tutorial","title":"Organizing and Submitting HTC Workloads"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#using-containerized-software-on-the-open-science-pool","text":"The most recent version of these training materials are here: Slides","title":"Using Containerized Software on the Open Science Pool"},{"location":"support_and_training_resources/education_and_training/osgusertraining/#software-portability-on-the-open-science-pool","text":"The most recent version of these training materials are here: Slides , List of Commands Tutorials (used in part) Using Julia on the OSPool High Throughput BWA Read Mapping","title":"Software Portability on the Open Science Pool"},{"location":"support_and_training_resources/education_and_training/previous-training-events/","text":"Other Past Training Events \u00b6 Overview \u00b6 We offer on-site training and tutorials on a periodic basis, usually at conferences (including the annual OSG All Hands Meeting) where many researchers and/or research computing staff are gathered. Below are some trainings for which the materials were public. (Apologies if any links/materials aren't accessible anymore, as some of these are external to our own web location. Feel free to let us know via support@opensciencegrid.org, in case we can fix/remove them.) Workshops/Tutorials \u00b6 Empowering Research Computing at Your Organization Through the OSG (PEARC 21) Organizing and Submitting HTC Workloads (OSG User Training pilot, June 2021) Empower Research Computing at your Organization Through the OSG (RMACC 2021) dHTC Campus Workshop (February 2021) Empowering Research Computing at Your Campus Through the OSG (PEARC 20) Deploy jobs on the Open Science Grid (Gateways/eScience 2019) High Throughput Computation on the Open Science Grid (Internet2 2018 Technology Exchange) Open Science Grid Workshop (The Quilt 2018) High Throughput Computation on the Open Science Grid (RMACC 18) Tutorials at Recent OSG All-Hands Meetings \u00b6 The below were offered on-site at OSG All-Hands Meetings. Note that the last on-site AHM in 2020 was canceled due to the pandemic, though we've linked to the materials. User/Facilitator Training at the OSG All Hands Meeting, University of Oklahoma (OU), March 2020 User Training at the OSG All Hands Meeting, Thomas Jefferson National Accelerator Facility (JLAB), March 2019 User Training at the OSG All Hands Meeting, University of Utah, March 2018","title":"Previous training events"},{"location":"support_and_training_resources/education_and_training/previous-training-events/#other-past-training-events","text":"","title":"Other Past Training Events"},{"location":"support_and_training_resources/education_and_training/previous-training-events/#overview","text":"We offer on-site training and tutorials on a periodic basis, usually at conferences (including the annual OSG All Hands Meeting) where many researchers and/or research computing staff are gathered. Below are some trainings for which the materials were public. (Apologies if any links/materials aren't accessible anymore, as some of these are external to our own web location. Feel free to let us know via support@opensciencegrid.org, in case we can fix/remove them.)","title":"Overview"},{"location":"support_and_training_resources/education_and_training/previous-training-events/#workshopstutorials","text":"Empowering Research Computing at Your Organization Through the OSG (PEARC 21) Organizing and Submitting HTC Workloads (OSG User Training pilot, June 2021) Empower Research Computing at your Organization Through the OSG (RMACC 2021) dHTC Campus Workshop (February 2021) Empowering Research Computing at Your Campus Through the OSG (PEARC 20) Deploy jobs on the Open Science Grid (Gateways/eScience 2019) High Throughput Computation on the Open Science Grid (Internet2 2018 Technology Exchange) Open Science Grid Workshop (The Quilt 2018) High Throughput Computation on the Open Science Grid (RMACC 18)","title":"Workshops/Tutorials"},{"location":"support_and_training_resources/education_and_training/previous-training-events/#tutorials-at-recent-osg-all-hands-meetings","text":"The below were offered on-site at OSG All-Hands Meetings. Note that the last on-site AHM in 2020 was canceled due to the pandemic, though we've linked to the materials. User/Facilitator Training at the OSG All Hands Meeting, University of Oklahoma (OU), March 2020 User Training at the OSG All Hands Meeting, Thomas Jefferson National Accelerator Facility (JLAB), March 2019 User Training at the OSG All Hands Meeting, University of Utah, March 2018","title":"Tutorials at Recent OSG All-Hands Meetings"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/","text":"Email, Office Hours, and 1-1 Meetings \u00b6 There are multiple ways to get help from OSG\u2019s Research Computing Facilitators. Get in touch anytime! Research Computing Facilitators \u00b6 To help researchers effectively utilize computing resources, our Research Computing Facilitators (RCFs) not only assist you in implementing your computational work on OSG compute resources, but can also point you to other services related to research computing and data needs. For example, RCFs can: Assist with planning your computational approach for a research problem Teach you to submit jobs via the OSG Connect service Help you with troubleshooting on OSG systems Connect you with other researchers using similar software or methods Point to learning materials for programming and software development Help you identify applicable non-OSG data storage options Find someone who knows the answer to your question, even if the RCF doesn\u2019t \u2026 and other helpful activities to facilitate your use of cyberinfrastructure We don\u2019t expect that you should be able to address all of your questions by consulting our documentation , user training , or web searches. RCFs are here to help! Get an Account \u00b6 If you don\u2019t have an account yet, please Sign Up , and we\u2019ll follow up quickly to set up a meeting time and create accounts. If you don\u2019t have an account but just have general questions, feel free to send an email to support@opensciencegrid.org (see below). Help via Email \u00b6 We provide ongoing support via email to support@opensciencegrid.org, and it\u2019s never a bad idea to start by sending questions or issues via email. You can typically expect a first response within a few business hours. Virtual Office Hours \u00b6 Drop-in for live help, starting January 11, 2022! Tuesdays, 4-5:30pm ET / 1-2:30pm PT Thursdays, 11:30am-1pm ET / 8:30-10am PT You can find the URL to the Virtual Office Hours meeting room when you log into an OSG Connect login node, or in the signature of a support email from an RCF. Click here to sign-in for office hours, once you arrive in the room. Cancellations will be announced via email. As always, if the times above don\u2019t work for you, please email us at our usual support address to schedule a separate meeting. Make an Appointment \u00b6 We are happy to arrange meetings outside of designated Office Hours, per your preference. Simply email us at support@opensciencegrid.org, and we will set up a time to meet! Training Opportunities \u00b6 The RCF team runs regular new user training on the first Tuesday of the month. See upcoming training dates, registration information, and materials on the OSG Training page .","title":"Email, Office Hours, and 1-1 Meetings "},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#email-office-hours-and-1-1-meetings","text":"There are multiple ways to get help from OSG\u2019s Research Computing Facilitators. Get in touch anytime!","title":"Email, Office Hours, and 1-1 Meetings"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#research-computing-facilitators","text":"To help researchers effectively utilize computing resources, our Research Computing Facilitators (RCFs) not only assist you in implementing your computational work on OSG compute resources, but can also point you to other services related to research computing and data needs. For example, RCFs can: Assist with planning your computational approach for a research problem Teach you to submit jobs via the OSG Connect service Help you with troubleshooting on OSG systems Connect you with other researchers using similar software or methods Point to learning materials for programming and software development Help you identify applicable non-OSG data storage options Find someone who knows the answer to your question, even if the RCF doesn\u2019t \u2026 and other helpful activities to facilitate your use of cyberinfrastructure We don\u2019t expect that you should be able to address all of your questions by consulting our documentation , user training , or web searches. RCFs are here to help!","title":"Research Computing Facilitators"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#get-an-account","text":"If you don\u2019t have an account yet, please Sign Up , and we\u2019ll follow up quickly to set up a meeting time and create accounts. If you don\u2019t have an account but just have general questions, feel free to send an email to support@opensciencegrid.org (see below).","title":"Get an Account"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#help-via-email","text":"We provide ongoing support via email to support@opensciencegrid.org, and it\u2019s never a bad idea to start by sending questions or issues via email. You can typically expect a first response within a few business hours.","title":"Help via Email"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#virtual-office-hours","text":"Drop-in for live help, starting January 11, 2022! Tuesdays, 4-5:30pm ET / 1-2:30pm PT Thursdays, 11:30am-1pm ET / 8:30-10am PT You can find the URL to the Virtual Office Hours meeting room when you log into an OSG Connect login node, or in the signature of a support email from an RCF. Click here to sign-in for office hours, once you arrive in the room. Cancellations will be announced via email. As always, if the times above don\u2019t work for you, please email us at our usual support address to schedule a separate meeting.","title":"Virtual Office Hours"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#make-an-appointment","text":"We are happy to arrange meetings outside of designated Office Hours, per your preference. Simply email us at support@opensciencegrid.org, and we will set up a time to meet!","title":"Make an Appointment"},{"location":"support_and_training_resources/get_help%21/getting-help-from-RCFs/#training-opportunities","text":"The RCF team runs regular new user training on the first Tuesday of the month. See upcoming training dates, registration information, and materials on the OSG Training page .","title":"Training Opportunities"}]}